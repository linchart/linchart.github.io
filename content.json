[{"title":"softmax","date":"2017-09-03T07:35:43.317Z","path":"posts/2017/09/03/softmax.html","text":"&emsp;&emsp;上一篇讲了逻辑回归的由来，在分类问题上，逻辑回归是一个二分类模型，而实际的项目中有可能需要处理一些多分类的问题（比如经典的MNIST），这时候如果使用二分类模型去处理多分类问题，会相对麻烦，这里介绍一种处理多分类问题的简单模型-softmax。softmax相当于做了一个归一化的工作，让强者相对更强，弱者相对更弱。 模型推导&emsp;&emsp;softmax模型是逻辑回归模型的推广，逻辑回归模型是softmax模型的一个特例，这两者都算是广义线性模型(Generalized Linear Model,GLM)。广义线性模型假设在给定属性$x$和参数$\\theta$之后，类别$y$的条件概率$p(y|x;\\theta)$服从指数分布族，它是长这样子的：$$P(y;\\eta)=b(y)exp(\\eta^TT(y) - a(\\eta))$$下面就是根据类别的联合分布概率密度搞成上面公式的样子。假设现在有$k$种分类$y \\in (1,2,…,k)$的数据，对于每一条观测（样本）都有一个对应的类别，假设每种分类对应的概率是$(p_1,p_2,…,p_k)$，那么所有类别的概率之和就是： 则第$k$类的概率也可以写成： 对于多分类，可以把类别写成编码向量的形式，向量的第$i$个位置为1，表示第$i$个类别，向量的其他位置为0。比如有5个分类，类别1可以表示成$(1,0,0,0,0)$，类别2可以表示成$(0,1,0,0,0)$。因为第$k$类可以用前$k-1$类表示，所以可以用$k-1$维向量$T(y)$表示类别。 用函数$\\mu(y=i)=1$表示第$y=i$为真，当$y=i$为假时，有$\\mu(y=i)=0$。这样就可以将所有了类别的概率整合起来：$$P(y;p)=p_1^{\\mu(y=1)}p_2^{\\mu(y=2)}…p_k^{\\mu(y=k)}$$因为第$k$类可以用前$k-1$类表示，所以上面的式子可以写成： 当$y=1$时，$P(y;\\eta)=p_1$，当$y=2$时，$P(y;\\eta)=p_2$，以此类推。再用向量$T(y)$表示，就变成了： 将上面式子的右边，写成下面这种形式： 也就是先取对数，再将结果作为$e$的指数（整个推导过程中，最关键的技巧就是在这里了），然后稍微做下推导，就变成这样子： 然后，另：$b(y)=1$，以及$a(\\eta)=-lnp_k$，还有$\\eta$： 上面的公式就可以写成（终于凑成了指数分布族的形式）： 由于 因为 所以 所以： 再根据$p_i=p_ke^{\\eta_i}$，可以得到: 再令$\\eta = w^Tx+b$，所以对于类别$i$，其概率可以表示成： &emsp;&emsp;softmax模型推导到这里已经算是结束了。&emsp;&emsp;我们再来看看，为什么说softmax相当于做了一个归一化的工作，让强者相对更强，弱者相对更弱。先看归一化，很简单，分母其实就是将分子从$1$到$k$累加起来，所以对于任意的$p_i$都有$p_i \\in [0,1]$。对于后者，softmax相当于做了一个拉大差距的工作，假设已经算好$w^Tx+b$就是以下几个值$[5,4,3,2,1]$，根据公式计算出来的对应的softmax值为$[0.636, 0.234, 0.086, 0.032, 0.012]$，可以看到，各个数之间的相对差距拉的更远了，所以softmax能够很好地凸显较大的数值。 似然函数&emsp;&emsp;像上篇逻辑回归一样，继续使用对数似然函数去估计参数。对于单个样本，当然是命中样本实际类别的概率越大越好。 这里$x^{(i)}$表示第$i$个样本，相对应的,$y^(i)$表示第$i$个样本对应的类别。同理，对于所有的样本，也是希望命中样本实际类别的概率越大越好，然后再取个对数（这里为了方便书写公式，将$w^Tx+b$写成$\\theta^T x$），这样就有： 最优化&emsp;&emsp;搞定了代价函数，剩下的工作就是求解参数了，一般可以使用梯度下降（上升）或者是牛顿迭代法来求解，这里以梯度下降（转化成求极小值）为例（梯度上升的话就是直接求解上面公式的极大值）。&emsp;&emsp;因为公式会写的比较长，这里为了方便，将$w^Tx+b$写成$\\theta ^Tx$的形式（都毕业了，还要把微积分捡回来，也是蛋疼）。&emsp;&emsp;首先是求导，在求导之前，先放两个需要用到的公式，这样整个求导过程会更加清晰。对于对数和分数的求导有下面两个公式： 对第$l$类的参数$\\theta _l$进行求导，所以就会有： 然后通过梯度上升更新参数： 这里的$\\alpha$为学习率。 后记哈哈","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"监督学习","slug":"监督学习","permalink":"http://yoursite.com/tags/监督学习/"},{"name":"多分类模型","slug":"多分类模型","permalink":"http://yoursite.com/tags/多分类模型/"},{"name":"对数线性模型","slug":"对数线性模型","permalink":"http://yoursite.com/tags/对数线性模型/"},{"name":"广义线性模型","slug":"广义线性模型","permalink":"http://yoursite.com/tags/广义线性模型/"}]},{"title":"逻辑回归","date":"2017-09-02T02:11:02.897Z","path":"posts/2017/09/02/logistic.html","text":"&emsp;&emsp;逻辑回归（Logistic Regression）是一个非常经典的回归模型，在神经网络里作为激活函数时也称为sigmoid函数。逻辑回归模型既可以处理线性分类问题也可以处理非线性分类问题，但其本质上是一个线性模型。作为机器学习入门级模型，该模型既简单又强大，在现实中有着广泛应用，比如国外成熟的信用卡评分模型就是基于逻辑回归模型建立的。 模型推导&emsp;&emsp;线性模型试图通过属性间的线性组合来预测目标值，它的模型形式是这样的：$$y=w_1x_1+w_2x_2+…+w_dx_d+b$$其中$x_i, i\\in (1,2,…,d)$表示$x$在第$i$个属性上的取值。上式表示成向量的形式就是：$$y=w^Tx+b$$其中$w=(w_1;w_2;…;w_3)$。&emsp;&emsp;在二维平面上，线性回归模型，其实就是一条直线，再来看下另外一个函数$y=e^x$。不难看出对数函数和线性函数都是单调的，那么肯定可以找到一个映射函数使得对于直线上的任意一个点$y=w^Tx+b$，都能在函数$y=e^x$上找到唯一的一个点与之相对应。对于函数$y=e^x$不妨令：$$x \\leftarrow w^Tx+b$$则有$y=e^{w^Tx+b}$，两边取对数之后，就有$lny=w^Tx+b$。 给定数据集$D= \\{ (x_1,y_1),(x_2,y_2),…,(x_m,y_m) \\} $，其中 $y_i \\in \\{0,1\\}$。我们可以用概率$p,p \\in [0,1]$来表示样本$x$被判为$y=1$时的可能性，$p$值越大，代表$x$被归为正样本$y=1$的可能性越大，也就是$p(y=1|x)$的概率为$p$，那么对应的$p(y=0|x)$的概率就是$1-p$，这两者的比值$$\\frac{p}{1-p}$$称为“几率”也有些家伙会称它为“优势比”。这个时候，我们再回到取对数之后的等式：$lny=w^Tx+b$，令$y=\\frac{p}{1-p}$，可以将等式转化为：$$ln\\frac{p}{1-p}=w^Tx+b$$ 同时将等式两边的值作为指数，以$e$为底，可以得到：$$\\frac{p}{1-p} = e^{w^Tx+b}$$ 稍微处理一下就可以推导出经典的逻辑回归模型:也就是说，对于$y=1$，我们有$$p(y=1|x)=\\frac {e^{w^Tx+b}}{1+e^{w^Tx+b}}$$对于$y=0$，我们有$$p(y=0|x)=\\frac {1}{1+e^{w^Tx+b}}$$为了看的更舒服一点，令$z=w^Tx+b$，分子分母再同时除以$e^z$，则模型写成以下形式：$$f(z)=\\frac {1}{1+e^{-z}}$$上面的等式其实是一个单调“S”形函数，$f(z)$的取值在$(0,1)$之间，当$z$取值越大时，$f(z)$越接近于1，反之，当$z$取值越小时，$f(z)$越接近于0，当$z=0$时，$f(z)=0.5$。其工作方式是，当$z&gt;0$时，对应的样本被判为正类$y=1$，当$z&lt;0$时，对应的样本被判为负类$y=0$，当$z=0$时，可视情况将样本归为任意类别。 线性and非线性为什么逻辑回归模型，既可以解决线性分类，也可以解决部分非线性分类问题呢？这里举两个例子。 线性分类假设有$y \\in \\{0,1\\}$两类样本，如下图，存在一条直线$f(x_1,x_2)=\\frac{1}{2}x_1-x_2+1$（我随便写的）能把这两类样本完全分割开，也就是对于所有的$y=1$有$f(x_1,x_2)&gt;0$，对于所有的$y=0$有$f(x_1,x_2)&lt;0$。 我们将$f(x_1,x_2)$代入逻辑回归模型，就会有：$$f(f(x_1,x_2))=\\frac {1}{1+e^{-f(x_1,x_2)}}$$显然，根据刚才提到的逻辑回归的工作方式，当$f(x_1,x_2)&gt;0$时，逻辑回归函数的取值$f(f(x_1,x_2))&gt;0.5$，也就是说，样本$(x_1,x_2)$被判为正类的可能性更大，相对应地，当$f(x_1,x_2)&lt;0$时，逻辑回归函数的取值$f(f(x_1,x_2))&lt;0.5$，样本$(x_1,x_2)$被判为负类的可能性更大。 非线性分类&emsp;&emsp;同样，假设有$y \\in \\{0,1\\}$两类样本，但是这两类样本在二维平面上不再线性可分，而是长成下面的样子。 这个时候继续使用直线一刀切地划分显然是不行的（当然，你可以将样本映射到高维空间，然后寻找划分超平面这两类样本分隔开）。但是我们可以找到这样的一个圆（如下图），能把正负样本完全分隔开，假设关于这个圆的函数是$(x1-4)^2+(x2-3)^2=0$（还是随便写的）。 函数$(x1-4)^2+(x2-3)^2&gt;0$时，代表样本点位于圆的外面，此时有$y=1$，把函数代入逻辑回归模型，会有$f(x)&gt;0.5$，此时样本$(x_1,x_2)$被判为正类的可能性更大。同样地，函数$(x1-4)^2+(x2-3)^2&lt;0$时，代表样本点位于圆的里面，此时有$y=0$，把函数代入逻辑回归模型，会有$f(x)&lt;0.5$，此时样本$(x_1,x_2)$被判为负类的可能性更大。&emsp;&emsp;因此，逻辑回归模型能够很好地处理线性分类以及部分非线性分类问题。 极大似然估计&emsp;&emsp;模型找出来了，但是参数$w$和$b$还没有求解，所以又到了寻找损失函数的时候了。在线性回归中，一般会使用均方误差（也叫最小二乘法）来衡量预测的好坏，该损失函数试图找到这样的一条直线，能够使得所有样本到直线上的距离之和最小，说白了就是用欧氏距离来衡量预测效果。 &emsp;&emsp;用均方误差求解线性回归参数，是因为该损失函数是一个凸函数，凸函数有一个比较好的性质就是，局部极小点就是全局最小点，在达到全局最优的过程中不会经历太多的波折，更不会走到半路就以为是终点。不妨也试一下，使用均方误差来衡量逻辑回归模型的拟合效果如何，把上面的$h_ \\theta(x)$换成逻辑回归的方程式可以得到：很遗憾的是，这个函数是非凸的，它可能是长成这个样子的：这样的函数，在进行最优化求解时，函数值很有可能一直逗留在某些奇怪的地方比如图中红色的点，而没办法达到全局最优。&emsp;&emsp;我们可以从概率的角度去思考，根据逻辑回归模型预测出来的概率值，肯定是离样本的真实标记越接近越好。比如对于正样本$x_1$，肯定是值$f(x_1)$越大越好，对于负样本$x_2$，值$f(x_2)$越小越好，因为值$f(x_2)$越小，样本$x_2$被判为负样本的可能性越大。对所有训练样本，我们希望最大化其似然函数： 当$y_i=1$时，$p$越大，$L(p)$越大。当$y_i=0$时，$p$越小，$L(p)$越大。对上式取对数之后，就得到对数似然函数：这里的$p(y_i=1|x_i)$对我们来说还是未知的，想要求解参数$w$，还需要稍微推导下。在模型推导中，我们知道：$$p(y=1|x)=\\frac {e^{w^Tx+b}}{1+e^{w^Tx+b}}$$把这个等式代入到损失函数（对数似然函数）中： 这样就可以使用最优化方法来求解参数$w$了。对$L(w)$求极大值，就可以得到$w$的估计值。当然，也可以将对数似然函数写成以下形式，就变成求极小值： 后记哈哈","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"分类模型","slug":"分类模型","permalink":"http://yoursite.com/tags/分类模型/"},{"name":"监督学习","slug":"监督学习","permalink":"http://yoursite.com/tags/监督学习/"},{"name":"对数线性模型","slug":"对数线性模型","permalink":"http://yoursite.com/tags/对数线性模型/"},{"name":"广义线性模型","slug":"广义线性模型","permalink":"http://yoursite.com/tags/广义线性模型/"}]},{"title":"支持向量机（三）","date":"2017-09-02T01:47:45.236Z","path":"posts/2017/09/02/SVM3.html","text":"&emsp;&emsp;硬间隔支持向量机要求训练数据是完全线性可分的，否则会宕机（无法工作）；软间隔支持向量机则允许很少量的样本线性不可分，模型的容错能力相对前者好一点；当训练数据出现不少（或者较多？）样本线性不可分的时候，软间隔支持向量不能很好地承担样本划分的工作，这时候可以引入Kernel（核函数）通过恰当的运算技巧（先计算映射到高维之后的内积）来达到非线性分类的目的。1、核函数的原理2、核函数的证明半正定矩阵的证明其实早在大学就学过了，很遗憾的是，作为学渣中的小弱弱，我不会。 知乎–https://www.zhihu.com/question/24627666在实用中，很多使用者都是盲目地试验各种核函数，并扫描其中的参数，选择效果最好的。至于什么样的核函数适用于什么样的问题，大多数人都不懂。很不幸，我也属于这大多数人，所以如果有人对这个问题有理论性的理解，还请指教。3、核函数的类型","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"分类模型","slug":"分类模型","permalink":"http://yoursite.com/tags/分类模型/"},{"name":"监督学习","slug":"监督学习","permalink":"http://yoursite.com/tags/监督学习/"}]},{"title":"支持向量机（二）","date":"2017-09-02T01:46:27.994Z","path":"posts/2017/09/02/SVM2.html","text":"&emsp;&emsp;上一篇对线性支持向量机的原理做了推导，这个推导的前提跟感知机模型一样，假设训练样本是线性可分的，也就是存在一个超平面能够将正负样本完全分隔开，但是这种假设在实际项目中存在的可能性跟国足挺进世界杯的可能性是一样一样的。 软间隔&emsp;&emsp;上一篇文章中提到的间隔，严格来说应该称之为“硬间隔”，因为该间隔存在的前提是所有训练样本都要满足约束条件$y_i(w^Tx_i+b) \\geq 1$，但是对于下图中的两类样本点，线性支持向量机并不适用，因为没办法在不映射到高维的情况下找到一条直线能把两个类别分隔开。 &emsp;&emsp;如果使用感知机的话，模型会无法收敛，并且找到的超平面有可能效果还可以也有可能很差。上图两个类别之间，混淆的样本点其实不多，总不能因为几颗芝麻就丢了西瓜吧。那么有没有办法使得支持向量机“容忍”这些不多的样本点出错，而保证绝大部分的样本点被正确分类呢，其中一个办法就是使用“软间隔”，允许某些样本不满足约束条件$y_i(w^Tx_i+b) \\geq 1$，也就是让某些样本点满足条件$y_i(w^Tx_i+b) &lt; 1$ 软间隔最大化&emsp;&emsp;为了获得最佳的划分超平面，还需要找出对应的损失函数来衡量划分超平面的效果。一方面，对于支持向量及以外的点，可以继续使用上一篇介绍的间隔最大化$\\frac{2}{||w||}$来评估，也就是$min \\frac{1}{2}||w||^2$；另一方面，对于支持向量内的点，没办法满足间隔大于等于1的约束条件，这种情况下，可以考虑引入松弛变量$\\xi _i$ （其中$ \\xi _i \\geq 0$），使得函数间隔在加上松弛变量之后大于等于1，也就是：$$y_i(w^Tx_i+b) + \\xi _i \\geq 1 $$将$\\xi _i$挪到右边：$$y_i(w^Tx_i+b) \\geq 1 -\\xi _i $$&emsp;&emsp;松弛变量可以理解为一种作用力，将误分类的点往正确分类的方向拉，拉回到对应的支持向量上。误分类的点与正确分类的边界越远，需要拉扯的力量越大，也就是$\\xi_i$越大，对于支持向量后方的点，$\\xi_i$值可以为0。&emsp;&emsp;然后，同时对于误分类的点，给它一个惩罚因子(cost)，用字母$C$表示（$C \\geq 0$），用来衡量误分类的代价，或者说，对离群点的重视程度（其尝试找出，在出现误分类的情况下，边界间隔最大的超平面以及保证数据点偏差量最小），这样损失函数就变成了： &emsp;&emsp;这里的损失函数相对上篇只是多了后半部分。显然，对于惩罚因子$C$，当$C$无穷大时，表明只要有一个样本点是特异点的话就需要付出无限的代价，上面式子趋向于无穷大，这时将会使得所有的样本点都满足式子$y_i(w^Tx_i+b) \\geq 1 $，这样的话软间隔最大化问题就变成硬间隔最大化问题（或者说是线性支持向量问题），但是由于给定的数据不是线性可分的，这样就会导致问题没有解；当$C$取有限值时，上式允许存在一些的样本不满足约束。&emsp;&emsp;加上约束条件，目标函数变成如下凸二次规划问题： 跟支持向量机（一）中，目标函数的求解方式一样，可以通过拉格朗日乘子法得到对应的拉格朗日函数： 其中$\\alpha _i \\geq 0 , \\mu _i \\geq 0$是拉格朗日乘子。一样的套路，求偏导，令$L(w,b,\\alpha _i,\\xi _i,\\mu _i)$对$w,b,\\xi _i$的偏导为0，可得： 将上面三个等式代入拉格朗日函数可以得到（公式推理写的有点宽，就不在这里展示了）： 与上一篇相似，上式同样需要满足KKT条件，也就是要求： &emsp;&emsp;这里的$f(x_i)$其实就是$w^Tx_i+b$，通过KKT条件，我们能更加深入地了解SVM的本质–不管是硬间隔支持向量机还是软间隔支持向量机，最终根据模型求得的分隔超平面仅与支持向量有关。&emsp;&emsp;对于上面KKT条件中的等式$\\alpha_i(y_i f(x_i)-1+\\xi _i)=0$，对于任意的训练样本，总会有$\\alpha_i = 0$或者$y_i f(x_i)-1+\\xi _i=0$，不妨把训练样本分为两种：支持向量以及支持向量以外的点，现在来分情况讨论下： $\\alpha_i=0$，这个时候$y_i f(x_i)-1+\\xi _i$等于任意值都是ok的，也就是$\\alpha_i=0$的时候，不会对样本有任何的影响。如上一篇提到的那样，在衡量目标损失的时候，对于非支持向量（也就是支持向量以外的点），为了满足损失函数最大化，$\\alpha_i$必定等于0。 $\\alpha_i&gt; 0$，为了满足KKT条件，必然有$y_i f(x_i)-1+\\xi _i=0$，也就意味着，样本$(x_i,y_i)$是支持向量 再结合偏导的等式$C=\\alpha_i + \\mu_i$ 和KKT条件$\\mu_i \\xi_i=0$、$\\mu_i \\geq 0$。由于$\\mu_i$是大于等于0的，所以$\\alpha_i$只能小于或等于$C$： $\\alpha_i&lt;C$，此时有$\\mu_i$大于0，因此$\\xi_i=0$，这个时候，则对应的样本$(x_i,y_i)$刚好落在最大间隔边界上面。 $\\alpha_i = C$，此时有$\\mu_i =0$，此时若$\\xi_i \\leq 1$，则对应的样本$(x_i,y_i)$落在最大间隔内部；若$\\xi_i &gt; 1$，则样本$(x_i,y_i)$分类错误。引出核函数后记&emsp;&emsp;软间隔支持向量机虽然比硬间隔支持向量机好那么一丢丢，对噪声的容忍程度比硬间隔支持向量机稍微强一点，但是用来处理现实复杂的数据，还是远远不够的。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"分类模型","slug":"分类模型","permalink":"http://yoursite.com/tags/分类模型/"},{"name":"监督学习","slug":"监督学习","permalink":"http://yoursite.com/tags/监督学习/"}]},{"title":"支持向量机（一）","date":"2017-08-10T14:15:11.873Z","path":"posts/2017/08/10/SVM.html","text":"&emsp;&emsp;支持向量机(Support Vector Machines, SVM)在实际应用中，算是一大分类神器了。原始的支持向量机是一种线性二分类模型，当支持向量机使用一些核技巧之后，可以从本质上变成非线性分类器。区别于感知机模型，线性可分支持向量机利用间隔（两个不同类的支持向量到超平面的距离）最大化求最优分离超平面，求得的解是唯一的。 感知机回顾&emsp;&emsp;从感知机模型我们知道，感知机的目标是只要能找到一个将训练数据集的正样本和负样本完全正确分割开的分离超平面就可以了，并不要求样本能被最大限度地划分，但是这样的话会导致模型的泛化能力不强，比如下左图，直线Y能把A,B两类样本完全分割开，是感知机模型的一个解，如果我们用Y去预测未知的数据，如下右图，$a1,a2$两个数据点实际为类别A，但却被直线Y误判为B类，说明该直线对未知数据的泛化能力并不强。 &emsp;&emsp;直观上，下图中的红色直线的划分效果比其他直线的划分效果都要好，因为该直线恰好在两个类的中间，尽量“公平”地远离了两个不同的类别。但是，如何才能找到这样的一个超平面使之能最好地区分正负样本，并且对未见的数据的泛化能力也是最强的呢？这就是支持向量机要解决的问题。 什么是支持向量？&emsp;&emsp;支持向量(Support Vector)的定义其实很简单，其实就是距离超平面最近的样本点，如下图中的$a1,a2,a3$三个点就称为支持向量。 为什么叫支持向量呢，因为所有的训练样本点其实都是以向量的形式表示的（尤其是当属性很多的时候），而上图的划分的超平面，仅仅由$a1,a2,a3$这三个点决定，与其他训练样本点一毛钱关系都没有（这也就是为什么SVM效率贼高的原因）！也就是说我们要找的划分超平面，其实是由这三个向量(vector)来支撑(support)的，因此我们称$a1,a2,a3$这三个点为Support Vector。so，只要找到支持向量，划分超平面也就找到了。 好了，知道了支持向量的概念，我们就可以按照模型→策略→算法 的步骤去求解SVM了。 模型&emsp;&emsp;上面的分析我们已经清楚地知道，我们的模型（目标函数）其实就是一个超平面，这个超平面可以通过如下线性方程来描述：$$w^Tx+b = 0$$ 策略&emsp;&emsp;有了模型，还需要有一个恰当的策略（损失函数）来评估模型的分类效果。前面有提到，我们要找的超平面，应该尽可能地离两个不同样本点都远，以保证泛化能力，而在空间中，衡量样本点到划分超平面的远近，很自然会想到用欧式距离来度量。样本空间中，任意一点$x$到超平面$w^Tx+b = 0$的距离可写为：$$d=\\frac{1}{||w||}|w^Tx+b|$$带上类别之后，上面的距离公式可以写成：$$d=\\frac{1}{||w||}y_i(w^Tx_i+b)$$ 对于下图类别A中的数据点$a1$，假设，在该点处，与分割超平面平行的超平面为：$w^Tx+b = c$，因为超平面的参数$w$和$b$同时放大或缩小$k(k\\neq 0)$倍时，超平面是一样的（比如$2x+4y=6$和参数同时除以2之后的$x+2y=3$）。那么，不妨对$w^Tx+b = c$做一个变换，令$$w \\leftarrow \\frac{w}{c}$$ $$b \\leftarrow \\frac{b}{c}$$ 所以$a1$点处的超平面可以写成$w^Tx+b = 1$，同理，$a2,a3$处的超平面可以写成：$w^Tx+b = -1$。显然，对于所有的正例（类别A），以及所有的负例（类别B），使得以下不等式成立： 这个时候，我们套上距离的公式，对于两个不同类别的支持向量$a1$和$a2,a3$到超平面的距离之和它可以写成：$$d = \\frac{2}{||w||} $$这里$\\frac{2}{||w||} $也称为间隔。 我们的目标是让间隔尽量地大，这样划分超平面对未见数据的预测能力会更强，也就是希望： 因为函数$\\frac{2}{||w||}$的单调性与$\\frac{1}{2} {||w||}^2$是相反的，所以，最大化$\\frac{2}{||w||}$时，相当于最小化$\\frac{1}{2} {||w||}^2$，这样，最优化问题可以转化为凸二次规划问题（目标函数是二次的，约束条件是线性的）： so，损失函数就这样被找出来了。 最优化&emsp;&emsp;在求取有约束条件的最优化问题时，为了更容易求解，我们可以使用拉格朗日乘子法将其转化为原问题的对偶问题进行求解。也就是，对于上面式子的每条约束添加拉格朗日乘子$\\alpha_i \\geq 0$，对应的拉格朗日函数可以写为:这里的$\\alpha=(\\alpha 1;\\alpha 2;…;\\alpha m)$，$\\alpha 1$代表第一个不等式的拉格朗日乘子。成功避开约束条件之后，我们就可以更加便利地去求解极值了。对于函数$L(w,b,\\alpha)$分别求参数$w$和$b$的偏导数，并令偏导数为0，可以得到：把等式$w$代入到函数$L(w,b,\\alpha)$:因为所以可以把等式中的划掉。最后得到：根据对偶问题的性质，也就是任何一个求极小值的线性规划问题都可以转化为求极大值的线性规划问题。所以：可以计算出：这样，最终的模型可以写成： 这里还需要注意一下的就是，$x_i^Tx$其实就是向量内积，可以用$&lt;·,·&gt;$表示，也就是： 之所以写成这样，是方便后面理解支持向量机是如何使用Kernel进行非线性分类的。上面这个式子可以说是支持向量机核函数的基本形式了。对于含有不等式约束的最优化，还必须满足KKT条件，也就是： 根据条件中的等式$\\alpha_i(y_if(x_i)-1)=0$，对于任意的训练样本，总会有$\\alpha_i = 0$或者$y_i f(x_i)-1=0$，若$\\alpha_i=0$，样本$(x_i,y_i)$不会对损失函数有任何的影响；若$\\alpha &gt;0$，则有$y_i f(x_i)-1 =0$，此时，样本$(x_i,y_i)$其实就是支持向量。 后记&emsp;&emsp;基于数据线性可分假设的SVM显然会对噪声很敏感。相对于硬间隔支持向量机，后续会介绍对噪声容忍度稍微强一点的软间隔支持向量机，以及装备核函数之后的强大SVM；总的来说，SVM的基本原理以及使用都是比较简单的，如果只是为了调包，了解基础原理已经可以了。如果是想深入了解，仅仅知道这些还是远远不够的。&emsp;&emsp;公式编辑已弃疗，复杂一点的公式还是截图显示吧，哎。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"分类模型","slug":"分类模型","permalink":"http://yoursite.com/tags/分类模型/"},{"name":"监督学习","slug":"监督学习","permalink":"http://yoursite.com/tags/监督学习/"}]},{"title":"梯度下降","date":"2017-08-07T08:19:01.063Z","path":"posts/2017/08/07/Gradient_descent.html","text":"&emsp;&emsp;李航在其《统计学习方法》中提到，机器学习的核心由三大块组成，分别为模型、策略、算法，其实说白了，机器学习就是一个模型 + 一个损失函数 + 一个最优化算法。损失函数是为了让模型能够更好地拟合或分类数据，最优化算法是用来优化损失函数的，而在机器学习中，最常用的最优化算法应该算梯度下降了，这篇文章就来讲讲梯度下降，探究下它是如何优化损失函数的。 梯度下降原理&emsp;&emsp;上文的感知机模型中有提到，梯度，其实就是求偏导，梯度下降就是沿着梯度的负方向移动。为了更直观地理解梯度下降，这里举一个网上的例子（实在想不出比这个更形象的解析了），比如我们现在站在山上的某个位置，如下图中的点A，目标是要最快速地下到山脚下，怎么一步一步走下去呢？我们可以选择往相对当前位置来说最陡峭的地方向下移动（也就是梯度的负方向），这样一直走到山脚。这个山脚有可能是整个大山的最低处（B点），也有可能是大山的局部低处（C或D点），这取决于我们移动的方向和幅度。这种情况下，梯度下降不一定能够找到全局最优解，有可能是一个局部最优解。 但如果是下面这个凸函数图，我们就一定可以移动到最低处，因为不管我们处于山上的哪一个位置，只要我们保持每次移动都是向下的，就一定能到达最低点。这种情况下，梯度下降法得到的解就一定是全局最优解。 梯度下降算法梯度下降算法如下（该算法摘抄自李航《统计学习方法》）：输入：目标函数$f(x)$，梯度函数$g(x) = \\nabla f(x)$，计算精度$\\epsilon $输出：$f(x)$的极小点$x^*$&emsp;&emsp;(1) 取初始值$x^{(0)} \\in R^n$，置$k=0$&emsp;&emsp;(2) 计算$f(x^{(k)})$&emsp;&emsp;(3) 计算梯度$g_k = g(x^{(k)})$，当$||g_k||&lt;\\epsilon$时，停止迭代，令$x^* = x^{(k)}$；否则，令$p_k = -g(x^{(k)})$，求$\\lambda_k $使得$$f(x^{(k)}+ \\lambda_k p_k) = min f(x^{(k)} + \\lambda p_k) $$&emsp;&emsp;(4) 置$x^{(k+1)} = x^{(k)} + \\lambda_k p_k $，计算$f(x^{(k+1)})$，当$||f(x^{(k+1)}) - f(x^{(k)})|| &lt; \\epsilon$ 或 $x^{(k+1)} - x^{(k)} &lt; \\epsilon$时，停止迭代，令$x^* = x^{(k+1)}$&emsp;&emsp;(5) 否则，令$k=k+1$，转$(3)$ &emsp;&emsp;这里的$p_k$是搜索方向，$\\lambda_k$是步长。假设有一个损失函数$y = x^2 - 4*x - 5 $，目标是找到$x$的值使得$y$取最小值，使用梯度下降求解如下： 梯度$$\\nabla x = -(2*x - 4)$$ 更新$x$的值$$x \\leftarrow x + \\eta \\nabla x $$ 设$x$的初始值为10，学习速率$\\eta$为0.2，则第一次迭代：梯度$$\\nabla x =-(2*10 - 4) = -16$$&emsp;&emsp;$x$的值相应更新为$$ x = 10 - 0.2 * 16 = 6.8$$&emsp;&emsp;对应的$y$值为$$y = 6.8*6.8 - 4*6.8 - 5 = 14.04$$第二次迭代：梯度$$\\nabla x =-(2*6.8 - 4) = -9.6$$&emsp;&emsp;$x$的值相应更新为$$ x = 6.8 + 0.2 * （-9.6） = 4.88$$&emsp;&emsp;对应的$y$值为$$y = 4.88*4.88 - 4*4.88 - 5 = -0.7056$$&emsp;&emsp;如此循环迭代，直到达到指定迭代次数或者$y$值最优。这里一直迭代到第38次时，求得极小值$ y =-9.0$。因此，梯度下降就是一个不断沿着梯度的负方向移动直到达到局部或全局最优点的一个过程。 梯度下降的三种形式梯度下降算法可以分为以下三种： 批量梯度下降法(Batch Gradient Descent) 随机梯度下降法(Stochastic Gradient Descent) 小批量梯度下降法(Mini-batch Gradient Descent) 损失函数&emsp;&emsp;为了更好地阐明这三种方式的本质与区别，这里以线性回归模型为例。对于任意一个线性方程，我们可以写成以下形式：$$h_\\theta (x)= \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + …+\\theta_n x_n$$ 这里的$(\\theta_0,\\theta_1,…,\\theta_n)$为参数，也称为权重。假设现在有一堆数据$X=(x^0,x^1,x^2,…,x^m)$以及对应的$y$值$Y=(y^0,y^1,y^2,…,y^m)$。我们的目标是要找到参数$(\\theta_0,\\theta_1,…,\\theta_n)$使得$y$值与theta（符号theta在这里硬是显示不出来，我也是醉了）值尽可能地接近。这里用h_theta(x)与y的平方误差作为损失函数来评估h_theta(x)与y值的接近程度。有了损失函数之后，我们就可以来聊一聊这三种梯度下降的形式了。 批量梯度下降法 批量梯度下降法每次迭代都需要用到全量的训练数据，优化过程比较耗时。 &emsp;&emsp;前面我们已经知道了梯度下降其实就是对参数求偏导，然后沿着梯度的负方向去更新参数。对于上面的损失函数$J(\\theta)$，我们随机初始化权重$\\theta$，然后重复执行以下更新：$$\\theta_j:=\\theta_j + (-\\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\theta))$$&emsp;&emsp;这里的$\\theta_j:$是指分别对$j=0,1,2,…,n$个参数求偏导。$\\alpha$指学习速率，代表每次向着$J(\\theta)$最陡峭的方向移动的步幅。从上面公式可以看到，为了更新参数$\\theta_j:$，式子右边的$\\frac{\\partial}{\\partial\\theta_j}J(\\theta))$我们还没有知道的。当我们只有一个数据点$(x,y)$的时候，$J$的偏导数： 因此，对于单个训练样本，其更新规则为：对于所有的训练样本，累加上述损失函数的偏导为：于是，每个参数的更新规则就变成为：&emsp;&emsp;从上面公式可以清楚看到，参数的每一次更新（迭代）都要用到全量训练数据（下图红色框位置）&emsp;&emsp;这种更新方式，在迭代过程中，参数的方差是最小的，收敛的过程也是最稳定的，但是如果训练数据量$m$很大，批量梯度下降将会是一个非常耗时的过程。 随机梯度下降法 随机梯度下降法的每次更新，是随机对一个样本求梯度并更新相应的参数。 &emsp;&emsp;从上面批量梯度下降法的求解过程可以看到，对于一个样本的损失函数，其对应参数的更新方式为：&emsp;&emsp;这种做法在面对大数据集时不会出现冗余，能够进行快速的迭代。因为每次仅迭代一个样本，随机梯度下降法求解极值的过程，并不是总是向着整体最优的方向迭代的，参数的方差变化很大，收敛很不稳定，相比批量梯度下降会更加曲折，因此准确率相对于批量梯度下降法会有所下降。 小批量梯度下降法 小批量梯度下降法每次更新参数，仅使用一小部分的训练数据进行迭代 &emsp;&emsp;使用批量梯度下降法准确率很高，但是效率低，随机梯度下降法效率高，但是准确率低，而小批量梯度下降法算是批量下降法和随机梯度下降法的一种折衷，其集合了前面两种下降方法的优势，具体操作过程如下： &emsp;&emsp;这里的$n$一般会选一个很小的数，比如10或者100，这样的话，在迭代过程中，参数值的方差不至于太大，收敛的过程会更加稳定。 后记&emsp;&emsp;在机器学习中，梯度下降法通常用于优化损失函数，优化的过程，是沿着梯度的负方向不断逼近极值。从第二点‘梯度下降算法’里的实例图可以看到，一开始算法的下降速度很快，但是在极值附近时，算法的收敛速度很慢，比如在第二点的例子，在第四次迭代时已经逼近极小值了，但是在第38次迭代才解出极小值。另外，步长$\\alpha$的选取很关键，步长过程可能会达不到极值点，甚至有可能发散，步长过短会导致收敛速度很慢。&emsp;&emsp;批量梯度下降法每次迭代都用到所有训练数据，准确度高同时复杂度也高；随机梯度下降法每次迭代随机选取一个数据样本进行更新，准确度不高，复杂度较低；小批量梯度下降法集合了前面两种下降方法的优势，训练复杂度较低，精确度也较高。&emsp;&emsp;最后，不得不吐槽下，markdown编辑公式真的是非常非常非常蛋疼。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"最优化","slug":"最优化","permalink":"http://yoursite.com/tags/最优化/"}]},{"title":"感知机模型","date":"2017-08-01T03:16:20.734Z","path":"posts/2017/08/01/perceptron_model.html","text":"&emsp;&emsp;刚从广发项目撤出来，未来几周应该都不会很忙，趁着闲暇时间整理一下机器学习方面的知识。&emsp;&emsp;先从最简单的感知机模型说起。感知机是一种二分类的线性模型，其假设训练数据集是线性可分的，目标是找到一个能够将训练数据集的正样本和负样本完全正确分割开的分离超平面。其实，模型只要找到该分离超平面，学习目的就达到了，并不要求样本能被最大限度地划分。 分类器&emsp;&emsp;分类器能够通过输入的特征向量映射到给定类别中的一个，也就是所谓的物以聚类人以群分。如下图一，蓝色直线将A和B完全分割开，那么该直线$y=ax+b$就是一个分类器。根据分类器是否线性可分的性质，分类器有线性分类器(图一)和非线性分类器（图二图三），从类别上，有二元分类器（图一）和多元分类器（两个类别以上，图二图三），而本章要讨论的感知机模型，就是一个二分类线性模型。 感知机模型&emsp;&emsp;感知机模型很简单，为什么说它简单呢，因为该模型只用了一个线性方程$y = ax + b$ 以及 一个符号函数$sign(x)$（初中的知识是不是？）&emsp;&emsp;下面来看下感知机模型的定义。&emsp;&emsp;假设有$N$维的特征输入$X = \\lbrace x_1,x_2,…,x_n \\rbrace $，输出的类别有两类$ Y = \\lbrace +1,-1\\rbrace $，则由特征输入映射到类别输出的函数$$f(x) = sign(w*x+b)$$称为感知机模型，其中，$w$ 和$b$是我们要求解的模型的参数，这里的$w$其实是一个跟特征输入维度一样$N$维的权值向量（也称为权重），而参数$b$为偏置（在二维平面上，$b$就是截距）。&emsp;&emsp;我们从公式上去解读下为什么说感知机模型是一个线性二分类模型。&emsp;&emsp;先看符号函数里面的线性方程$$y=w*x+b$$显然，方程$ y = w*x +b $在二维平面上是一条直线，在三维空间上是一个平面，在四维甚至更高维空间上就是一个超平面。（哈哈哈，这个实在是画不出来）也就是说，不管是在二维三维空间还是更高维空间，超平面都可以将空间一分为二，并且都可以表示成方程$ y = w*x +b $，所以感知机模型首先是一个线性模型。&emsp;&emsp;再看符号函数，这个更简单，符号函数的定义如下：$$sign(x)=\\begin{cases}+1,\\quad x\\geq 0\\\\-1, \\quad x&lt;0\\end{cases}$$ 如果$x&gt;=0$则$f(x)=+1$，否则$f(x)=-1$。那么对于函数$f(x) = sign(w*x+b)$，如果输入样本$w*x+b&gt;=0$，那么该样本会被判为+1类，否则判为-1类。举个例子，回到上文图一，对于直线上方有$w*x+b&gt;0$，所以所有的A样本都会归为+1类，对于直线下方有$w*x+b&lt;0$，这样所有的B样本都会归为-1类。因此，感知机模型是一个线性二分类模型，其任务就是，找到这样的一个超平面，能够把两个不同的分类完全分割开。 学习策略&emsp;&emsp;原理清楚了，那么现在的问题是，给定一个包含正负类的训练样本，我们应该如何找出这样的一个超平面，使之能够将正样例和负样例点完全分割开，也就是应该如何确定模型的参数$w$和$b$ &emsp;&emsp;如上图，如何找到一条直线，使之能够将蓝色实例和褐色实例分割开（这里只是举个例子哈，在实际构建模型过程中，涉及的数据动不动就成百上千万，你想拿支笔来，往两个样本点之间一画一条直线，这是妥妥的不行的，更何况实际处理的数据维度一般都有两位数以上）。直接求解好像无从下手，不妨先假设参数$w$和$b$已知，也就是说这个超平面$S$我们已经找到了，但是不知道这超平面对样例数据的分类效果怎样。如何去评估分类的效果呢，一个很自然的想法就是看有多少样例数据是误分类的，也就是将总误分类数作为模型的损失函数，但是这样的函数对于参数$w$和$b$来说不是连续可导的，难以优化。另一种方法就是可以通过衡量误分类点到超平面的总距离来评估效果。高中的时候我们就学过点到平面的距离的公式是这样子的。$$d=\\frac{|A*x_0+B*y_0+C*z_0+D|}{\\sqrt{A^2+B^2+C^2}}$$ 其中$(A,B,C)$是平面法向量，在这里是权值向量，上面距离公式也可以简写成$$d=\\frac{1}{||w||}|wx_0+b|$$ 其中$w=(A,B,C)$， 这里||w||也称为$w$的L2范数。依然以上文图一为例，对于任意一个样本点$(x_i,y_i)$: 如果该样本点是位于直线上方，并且刚好属于类别A的话，那么该样本点被直线正确分类，此时有$w*x_i+b&gt;0,y_i=+1$，显然$-y_i*(w*x_i+b)&lt;0$ 如果该样本点位于直线下方，并且刚好属于类别B的的话，那么该样本点被直线正确分类，此时有$w*x_i+b&lt;0,y_i=-1$，显然$-y_i*(w*x_i+b)&lt;0$ 如果该样本点位于直线上方，并且刚好属于类别B的的话，那么该样本点被直线错误分类，此时有$w*x_i+b&gt;0,y_i=-1$，显然$-y_i*(w*x_i+b)&gt;0$ 如果该样本点位于直线下方，并且刚好属于类别A的的话，那么该样本点被直线错误分类，此时有$w*x_i+b0$ &emsp;&emsp;所以，对于误分类的点$(x_i,y_i)$来说$-y_i*(w*x_i+b)&gt;0$成立。&emsp;&emsp;我们可以用$-y_i*(w*x_i+b)$（相当于$|w*x_i+b|$）去衡量误分类点的效果，$w*x_i+b$值越大，说明该点离分离超平面越远，误分类效果越差（虽然分错了就是分错了）。如下图中的红色直线对于点A的分类效果明显比点B的要差 有了单个误分类点离超平面的距离，那么对于所有的误分类点有：$$ -\\frac{1}{||w||}\\sum_i^m y_i(wx_i+b)$$ 其中$m$为误分类点数量。当不考虑$\\frac{1}{||w||}$时（$\\frac{1}{||w||}$恒大于0，去掉的话不影响评估效果），就得到感知机模型的损失函数$Loss Function$（用来度量模型预测的好坏）：$$ L(w,b) =-\\sum_i^m y_i(wx_i+b)$$既然误分类点离分离超平面越远，误分类效果越差，那么，误分类点离分离超平面越近，误分类效果越好，当$$-\\sum_i^m y_i(wx_i+b) = 0$$时，样本没有被误分类，所有样本都被超平面恰当地分割开。所以只要找到$w$和$b$使得$L(w,b)$取最小值，此时超平面的分类效果是最好的，这个时候我们可以将参数$w$和$b$的求解转化为最小化函数$L(w,b)$ $$minL(w,b) = -\\sum_i^m y_i(wx_i+b)$$ 算法实现&emsp;&emsp;最优化方法有很多，比如牛顿下降法、拉格朗日乘子法、共轭梯度下降法、梯度下降法，这里使用最简单也最常用的一样方法：随机梯度下降法（梯度下降法的一种方法）来优化上文提到的损失函数$L(w,b)$ $$L(w,b) = -\\sum_i^m y_i(wx_i+b)$$ 梯度其实就是求偏导，随机梯度下降其实就是一次随机选一个误分类点，使其函数值$L(w,b)$往着负梯度方向移动，不断逼近最小值的一个过程。函数$L(w,b)$关于参数$w$和$b$的梯度分别为：$$\\nabla_w L(w,b)= -\\sum_i^m y_i x_i $$ $$\\nabla_b L(w,b)= -\\sum_i^m y_i $$ 现在，我们随便找一个分割超平面，比如$w=0,b=0$（注意$w$是一个向量），然后随机选一个实例点，判断$-y_i*(w*x_i+b)$ 是否大于等于0，如果大于等于0则说明该点被误分类，这时对$(w,b)$进行如下更新（如果$-y_i*(w*x_i+b)$小于0则不更新$w$和$b$）：$$w \\leftarrow w+\\eta y_i x_i$$ $$b \\leftarrow b+\\eta y_i$$ 这里的$\\eta$$(0&lt;=\\eta&lt;=1)$为步长，也称为学习率，这样，通过不断选取误分类点，更新参数$(w,b)$，降低函数$L(w,b)$的值，直到$L(w,b)=0$时对应的$(w,b)$的取值就是要求解的值。算法很简单，分成4步，也就是： (1)选取初值$(w_0,b_0)$; (2)在训练数据集中随机选取一个数据$(x_i,y_i)$; (3)如果$-y_i*(w*x_i+b)&gt;=0$，则进行如下更新：$$w \\leftarrow w+\\eta y_i x_i$$ $$b \\leftarrow b+\\eta y_i$$ (4)转至(2)，直至数据集中没有误分类的点或者达到指定的迭代次数。 用python实现基于随机梯度下降的简单感知机模型。 import numpy as np import random import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets.samples_generator import make_blobs #随机生成特征维度为2，分别以[-1,-1],[1,1]为中心，类别方差为0.4,0.5的两个类 x,y = make_blobs(n_samples=100,n_features=2,centers=[[-1,-1],[1,1]],cluster_std=[0.4,0.5]) # 将 0 替换成-1 y[y==0] = -1 w = np.zeros(2)#初始权重赋值为0 b = 0 #初始偏置为0 k=200 #最大迭代次数 l_rate = 0.5 #学习率 i=0 while i &lt;= k: i = i+1 #生成随机数 random_num = random.randint(0,99) #损失函数 if sum(-y[random_num]*(w*x[random_num] + b)) &gt;= 0: #梯度更新权重 w = w + l_rate*y[random_num]*x[random_num] #梯度更新偏置 b = b + l_rate*y[random_num] #---------------------画图------------------------- x1 = -3.0 y1 = -(b + w[0] * x1) /w[1] x2 = 3.0 y2 = -(b + w[0] * x2) / w[1] plt.figure(figsize=(8, 6)) plt.plot([x1,x2],[y1,y2],'r') plt.scatter(x[:,0],x[:,1],marker='o',c=y,s=50) plt.xlabel('x1') plt.ylabel('x2') plt.show() &emsp;&emsp;因为这里的两个样本区分度太明显，所以经过几次迭代就已经收敛了。最终求得参数$w=[1.03972853 \\ 0.97360177] ,b=0$，根据参数$(w,b)$画出的直线如下图所示：&emsp;&emsp;选取不同的初始值$(w,b)$或者不同的误分类点，划分的超平面很有可能会不一样，这也就是为什么在线性可分数据集中，用感知机模型求出来的解有无穷多个。如下面直线$A,B,C$都可作为解。 后记&emsp;&emsp;感知机模型假设训练数据集是线性可分的，如果线性不可分，算法会一直震荡无法收敛，所以其无法处理一些复杂的数据，如在分类器中提到的图二和图三。考虑到实际业务的数据一般比较复杂，简单的感知机模型无法有效地处理如此复杂的数据，所以在建模中很少会使用该模型。但是感知机模型还是比较重要的，为什么这么说呢，因为只要稍微修改下模型的损失函数，感知机模型就可转变为广受欢迎的分类神器：支持向量机，而通过简单的堆叠可演变为神经网络模型。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"分类模型","slug":"分类模型","permalink":"http://yoursite.com/tags/分类模型/"},{"name":"监督学习","slug":"监督学习","permalink":"http://yoursite.com/tags/监督学习/"}]},{"title":"TF-IDF原理及应用","date":"2017-03-15T09:36:24.153Z","path":"posts/2017/03/15/TF-IDF.html","text":"你说广州塔，我知道是在广州，你说黄果树瀑布，我知道是在贵州，你说布达拉宫，我知道是在拉萨，你说公交车，我都不知道你在说哪个城市的公交车。这就是TF-IDF。 概念及原理TF-IDF全称Term Frequency and Inverse Document Frequency，直译过来就是’词频-逆向文件频率’，’TF’是指某一个给定的词语在该文件中出现的频率，’IDF’是指总文件数除以包含该词的文件数，再取对数。TF-IDF一般用来评估在一堆语料库或一堆文件集中，某个字词对于该语料库或该文件的重要程度。怎么理解呢，举个例子，假设现在手上有10篇文章，‘水果’这个词在某一篇文章出现的频率很高，但是在这10篇文章中的仅有2篇文章提到，那么‘水果’这个词的TF-IDF会很高，如果10篇文章中有8篇提到‘水果’这个词，那么这个词的‘TF-IDF’会相对偏低。主要思想就是，一个词越能将一篇文章与其他文章区分开来，那么这个词的权重越高。 计算公式TF计算： （markdown编辑数学公式还不怎么熟，先用mathtype搞好再截图吧）比如上面的例子，’水果’，’硬盘’在文章1（共有10个词）中出现的次数分别为2次，4次，那么:12TF(水果) = 2/10 = 0.2 TF(硬盘) = 4/10 = 0.4 IDF计算： 如果这10篇文章中，有2篇文章包含有’水果’这个词，有5篇包含’硬盘’这个词，那么：12IDF(水果) = log(10/2) = 1.6094 IDF(硬盘) = log(10/5) = 0.6931 TF-IDF计算算好TF和IDF之后，就可以计算’水果’和’硬盘’的TF-IDF了，只需要将TF和IDF相乘就ok。所以’水果’的TF-IDF为：10.2*1.6094 ‘硬盘’的TF-IDF为：10.4*0.6931 如果算’水果’和’硬盘’这两个词与文章1的相关性呢，很简单，只要将这两个词的TF-IDF加起来。10.2*1.6094 + 0.4*0.6931 python中计算TF-IDF使用的工具 jieba scikit-learn 切词其实切词只是计算TF-IDF的前期准备工作，在对中文文本进行TF-IDF计算的话，切词这一步应该是怎么也逃不过去了。平常工作中基本都是用jieba切词，这里也打算用jieba对文本进行处理。例如我现在有5个文本：1content = [['萨德系统核心装备X波段雷达'],['美韩当局部署萨德的步伐也在加速进行'],['纵观如今的手机处理器市场已经不是高通一家独大的局面'],['三星的Exynos处理器以及华为的海思麒麟芯片这些年风头正盛'],['魅族每年数以千万计的销量对于芯片厂商的贡献也是不可小看的']] 首先需要对文本进行切词，切词代码及结果如下：1234567891011def cut_words(text): results = [] for content in contents: seg_list = jieba.cut(content[0],cut_all=False) # 实际应用过程中，这里需要去除停用词 seg = ' '.join(seg_list) results.append(seg) return resultsresult = cut_words(contents)result = ['萨德 系统核心 装备 X 波段 雷达', '美韩 当局 部署 萨德 的 步伐 也 在 加速 进行', '纵观 如今 的 手机 处理器 市场 已经 不是 高通 一家独大 的 局面', '三星 的 Exynos 处理器 以及 华为 的 海思 麒麟 芯片 这些 年 风头 正 盛', '魅族 每年 数以千万计 的 销量 对于 芯片 厂商 的 贡献 也 是 不可 小看 的'] 准备工作做好之后，我们就可以进行TF-IDF计算了。 词语转矩阵词语转矩阵需要用到CountVectorizer这个函数，其作用是统计词汇的数量，并转为矩阵。1234#coding:utf-8from sklearn.feature_extraction.text import CountVectorizervectorizer = CountVectorizer()vector_location = vectorizer.fit_transform(result) 通过type(vector_location)可以看到，函数fit_transform把result二维数组表示成一个稀疏矩阵:123print(type(vector_location))#输出&lt;class 'scipy.sparse.csr.csr_matrix'&gt; 同时可以看下，vercot_location的输出结果：1234567891011121314151617181920212223242526272829303132333435363738394041424344print(vector_location)#输出#(0, 27) 1#(0, 23) 1#(0, 28) 1#(0, 21) 1#(0, 34) 1#(1, 27) 1#(1, 25) 1#(1, 16) 1#(1, 32) 1#(1, 19) 1#(1, 6) 1#(1, 31) 1#(2, 24) 1#(2, 10) 1#(2, 17) 1#(2, 9) 1#(2, 15) 1#(2, 14) 1#(2, 4) 1#(2, 36) 1#(2, 1) 1#(2, 13) 1#(3, 9) 1#(3, 2) 1#(3, 0) 1#(3, 5) 1#(3, 7) 1#(3, 22) 1#(3, 38) 1#(3, 26) 1#(3, 30) 1#(3, 35) 1#(4, 26) 1#(4, 37) 1#(4, 20) 1#(4, 18) 1#(4, 33) 1#(4, 11) 1#(4, 8) 1#(4, 29) 1#(4, 3) 1#(4, 12) 1 输出结果表示的是这个稀疏矩阵的第几行第几列有值，比如(0, 27) 1表示矩阵的第0行第27列有值。转成矩阵的形式之后，我们就可以很容易地算出每个词对应的TF-IDF了，这里使用TfidfTransformer函数进行计算。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from sklearn.feature_extraction.text import TfidfTransformertransformer = TfidfTransformer()tf_idf = transformer.fit_transform(vector_location)print(type(tf_idf))#输出，同样是稀疏矩阵的形式#&lt;class 'scipy.sparse.csr.csr_matrix'&gt;print(tf_idf)#输出#(0, 34) 0.463693222732#(0, 21) 0.463693222732#(0, 28) 0.463693222732#(0, 23) 0.463693222732#(0, 27) 0.37410477245#(1, 31) 0.387756660106#(1, 6) 0.387756660106#(1, 19) 0.387756660106#(1, 32) 0.387756660106#(1, 16) 0.387756660106#(1, 25) 0.387756660106#(1, 27) 0.312839631859#(2, 13) 0.321896111462#(2, 1) 0.321896111462#(2, 36) 0.321896111462#(2, 4) 0.321896111462#(2, 14) 0.321896111462#(2, 15) 0.321896111462#(2, 9) 0.259703755905#(2, 17) 0.321896111462#(2, 10) 0.321896111462#(2, 24) 0.321896111462#(3, 35) 0.327880622184#(3, 30) 0.327880622184#(3, 26) 0.264532021474#(3, 38) 0.327880622184#(3, 22) 0.327880622184#(3, 7) 0.327880622184#(3, 5) 0.327880622184#(3, 0) 0.327880622184#(3, 2) 0.327880622184#(3, 9) 0.264532021474#(4, 12) 0.321896111462#(4, 3) 0.321896111462#(4, 29) 0.321896111462#(4, 8) 0.321896111462#(4, 11) 0.321896111462#(4, 33) 0.321896111462#(4, 18) 0.321896111462#(4, 20) 0.321896111462#(4, 37) 0.321896111462#(4, 26) 0.259703755905 如果需要把稀疏矩阵转成平常用的行列形式的矩阵的话。这里可以使用todense()或者toarray()函数，前者是将稀疏矩阵转成matrix的形式，后者是将稀疏矩阵转成ndarray的形式1234567weight = tf_idf.toarray()#orweight1 = tf_idf.todense()print(weight)#输出#(5,39) 这里还有一个问题，就是我怎么知道每个权重对应的是哪个词呢？这里可以将词作为列名，将数组转成Dataframe进行查看。1234word=vectorizer.get_feature_names()df = pd.DataFrame(weight)df.columns = wordprint(df) 源代码最后照例附上本次分析的源代码123456789101112131415161718192021222324252627282930313233343536#coding:utf-8#author:linchartimport jiebaimport pandas as pdfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.feature_extraction.text import TfidfTransformercontents = [['萨德系统核心装备X波段雷达'],\\ ['美韩当局部署萨德的步伐也在加速进行'],\\ ['纵观如今的手机处理器市场已经不是高通一家独大的局面'],\\ ['三星的Exynos处理器以及华为的海思麒麟芯片这些年风头正盛'],\\ ['魅族每年数以千万计的销量对于芯片厂商的贡献也是不可小看的']]def cut_words(text): results = [] for content in contents: seg_list = jieba.cut(content[0],cut_all=False) # 实际应用过程中，这里需要去除停用词 seg = ' '.join(seg_list) results.append(seg) return resultsdef tf_idf(words): vectorizer = CountVectorizer() vector_location = vectorizer.fit_transform(result) transformer = TfidfTransformer() tf_idf = transformer.fit_transform(vector_location) weight = tf_idf.toarray() word = vectorizer.get_feature_names() df = pd.DataFrame(weight) df.columns = word return dfresult = cut_words(contents)df = tf_idf(result)print(df)","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/tags/NLP/"}]},{"title":"爬取微信文章","date":"2017-03-13T09:59:18.349Z","path":"posts/2017/03/13/WeChat_Article1.html","text":"&emsp;&emsp;有时在微信公众号上面看到一些写的比较好的文章，但又没有时间细看，闲下来想找这些文章的时候又忘了是在哪个公众号看的了、文章名字也想不起来，因此想搞个爬虫把想看的文章爬下来，一来可以在闲时咀嚼一下，二来也可以收藏一些好文章，做些知识积累。只是想把自己平常做的一些东西记录下来，非教程 工具 Python 3.5.1 使用的库 re pdfkit requests BeautifulSoup 功能输入微信文章名称或者对应的文章链接，输出文章的pdf文件。 思路 如果同时提供文章链接和文章名称，则优先通过文章链接爬取，如果文章链接爬取失败，则通过文章名称爬取； 如果仅提供文章链接，则通过文章链接爬取； 如果仅提供文章名称，则通过搜狗微信接口搜索微信文章，找到对应文章链接，然后在通过文章链接爬取。 爬取流程获取文章链接将提供的文章名称传入搜狗微信搜索引擎搜索，将结果列表中的第一篇文章作为目标文章下载。下面代码返回目标文章链接。 12345678910111213def get_article_link(query): base_url = r'http://weixin.sogou.com/weixin' User_Agent = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36' Host = 'weixin.sogou.com' Connection = 'keep-alive' headers = &#123;'User-Agent': User_Agent, 'Host': Host, 'Connection': Connection&#125; params = &#123;'type': 2, 'ie': 'utf-8', 'w': '01019900', 'sut': '707','query':query&#125; request = requests.get(base_url, headers=headers, params=params) request.encoding = 'utf-8' bsobj = BeautifulSoup(request.text, 'lxml') # 仅提取列表中的第一篇文章 first_article_link = bsobj.select('#sogou_vr_11002601_title_0')[0]['href'] return first_article_link 将文章转为html解析文章链接，将文章内容保存为html文件。这里需要注意的是，在解析文章的时候，如果文章中包含有图片的话，正常情况下是无法下载下来的，因为爬取的文章链接为临时链接，非永久链接，无法直接解析src里面的链接。但是，data-src这个属性的值还是可以解析出来的，所以只要把data-src替换为src就可以下载图片了。 12345678910111213141516171819def get_article_html(link): # 为了保险起见，这里使用不同的headers User_Agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/3.0 Safari/536.11' article_headers = &#123;'User_Agent': User_Agent&#125; article_obj = requests.get(link, headers=article_headers) article_obj.encoding = 'utf-8' soup = BeautifulSoup(article_obj.content, 'html5lib') # 以实际的文章名称为准 article_name = soup.select('#activity-name')[0].text.strip() content = soup.find('div', &#123;'id': 'page-content'&#125;) html = str(content) # 把属性data-src替换成src,前面无法将属性src解析出来，data-src，只是LAZY用的， # 延迟加载图片所以显示不出来，LAZYLOAD src_compile = re.compile('data-src') html_new = re.sub(src_compile, 'src', html) # 存储成html with open('wechat_article.html', 'w', encoding='GB18030') as f: f.write(html_new) return article_name html转pdfhtml文件转pdf调用了pdfkit这个包，使用这个包需要安装wkhtmltopdf软件（pdfkit依赖于wkhtmltopdf，因此需要配置路径）。在运行过程中，发现pdfkit在html转pdf时，生成的pdf文件名中如果包含有| / *这些特殊符号时会报错，因此如果以原文章名对pdf命名失败时，仅保留文章名的汉字、字母和数字进行命名。 1234567891011121314151617def html_to_pdf(query_article): path_wk = r'D:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe' config = pdfkit.configuration(wkhtmltopdf=path_wk) options = &#123; 'page-size': 'Letter', 'encoding': \"GB18030\", 'custom-header': [ ('Accept-Encoding', 'gzip') ] &#125; try : pdfkit.from_file('wechat_article.html', '%s.pdf' % query_article, configuration=config, options=options) except: name_compile = re.compile('[a-zA-Z\\u4e00-\\u9fa5][a-zA-Z0-9\\u4e00-\\u9fa5]+') pdf_name = re.findall(name_compile,query_article)[0] pdfkit.from_file('wechat_article.html', '%s.pdf' % pdf_name, configuration=config, options=options) print('文件名已被修改为:%s' %pdf_name) 源代码最后附上文章爬取的完整代码。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283#coding:utf-8#author:linchartimport requestsfrom bs4 import BeautifulSoupimport reimport pdfkitdef get_article_link(query): base_url = r'http://weixin.sogou.com/weixin' User_Agent = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36' Host = 'weixin.sogou.com' Connection = 'keep-alive' headers = &#123;'User-Agent': User_Agent, 'Host': Host, 'Connection': Connection&#125; params = &#123;'type': 2, 'ie': 'utf-8', 'w': '01019900', 'sut': '707','query':query&#125; request = requests.get(base_url, headers=headers, params=params) request.encoding = 'utf-8' bsobj = BeautifulSoup(request.text, 'lxml') # 仅提取列表中的第一篇文章 first_article_link = bsobj.select('#sogou_vr_11002601_title_0')[0]['href'] return first_article_linkdef get_article_html(link): # 需要不同的headers User_Agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/3.0 Safari/536.11' article_headers = &#123;'User_Agent': User_Agent&#125; article_obj = requests.get(link, headers=article_headers) article_obj.encoding = 'utf-8' soup = BeautifulSoup(article_obj.content, 'html5lib') # 以实际的文章名称为准 article_name = soup.select('#activity-name')[0].text.strip() content = soup.find('div', &#123;'id': 'page-content'&#125;) html = str(content) # 把属性data-src替换成src,前面无法将属性src解析出来，data-src，只是LAZY用的， # 延迟加载图片所以显示不出来，LAZYLOAD src_compile = re.compile('data-src') html_new = re.sub(src_compile, 'src', html) # 存储成html with open('wechat_article.html', 'w', encoding='GB18030') as f: f.write(html_new) return article_namedef html_to_pdf(query_article): path_wk = r'D:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe' config = pdfkit.configuration(wkhtmltopdf=path_wk) options = &#123; 'page-size': 'Letter', 'encoding': \"GB18030\", 'custom-header': [ ('Accept-Encoding', 'gzip') ] &#125; try : pdfkit.from_file('wechat_article.html', '%s.pdf' % query_article, configuration=config, options=options) except: name_compile = re.compile('[a-zA-Z\\u4e00-\\u9fa5][a-zA-Z0-9\\u4e00-\\u9fa5]+') pdf_name = re.findall(name_compile,query_article)[0] pdfkit.from_file('wechat_article.html', '%s.pdf' % pdf_name, configuration=config, options=options) print('文件名已被修改为:%s' %pdf_name)def wechat_article(query=None,link=None): if link : try : article_name = get_article_html(link) html_to_pdf(article_name) print('文章下载成功') except : article_link = get_article_link(query) get_article_html(article_link) html_to_pdf(query) print('文章下载成功') else : article_link = get_article_link(query) get_article_html(article_link) html_to_pdf(query) print('文章下载成功')# PDF可以用中文命名，但是命名中不可以包含* \\/|等特殊字符。if __name__ == '__main__': link = None query = '文本分析|词频与余弦相似度' wechat_article(query=query,link=link)","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"博客的搭建","date":"2017-03-04T18:08:32.626Z","path":"posts/2017/03/05/blog.html","text":"参考文档这个博客的搭建，完全得益于教你免费搭建个人博客，Hexo&amp;Github和使用GitHub和Hexo搭建免费静态Blog这两篇文章，非常详细地描述了基于hexo+github搭建个人博客的准备工作及安装和配置流程。 遇到的问题虽然上面这两篇文章写得很详细，但是我在按着教程搭建的过程中还是遇到一些小问题，这里记录一下： 1、注意运行路径在浏览器中查看自带的hello world文章，需要执行 hexo generate 和hexo server两个命令，这里要注意一下这两个命令的执行路径，需要在hello world文章路径下执行。 2、MarkdownPad无法预览win10下首次安装MarkdownPad会出现右侧浏览页面无法浏览的情况，这种情况下需要安装Awesomium 1.6.6 SDK，安装完成之后问题可解决。 3、yilia主题头像无法显示若加载头像后，头像无法显示，需要将’themes/yilia/layout/_partial’路径下的文件left-col.ejs中的第6行修改为： &lt;img src=\"&lt;%=theme.avatar%&gt;\" class=\"js-avatar show\"&gt; 4、部署上传部署上传时执行以下命令时 hexo d 报’ERROR Deployer not found: git’错误，可能是deployer-git插件未安装，在根目录下执行下面代码安装该插件即可。 npm install hexo-deployer-git --save","tags":[]}]