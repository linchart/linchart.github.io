<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>linchart</title>
  <subtitle>I&#39;m on the way to the future, where you are there.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-09-01T17:08:37.523Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>山久丰</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基于牛顿法优化逻辑回归损失函数（二）</title>
    <link href="http://yoursite.com/posts/2018/09/01/Newton&#39;s_method_2.html"/>
    <id>http://yoursite.com/posts/2018/09/01/Newton&#39;s_method_2.html</id>
    <published>2018-09-01T06:48:40.829Z</published>
    <updated>2018-09-01T17:08:37.523Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="最优化" scheme="http://yoursite.com/tags/%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>基于牛顿法优化逻辑回归损失函数（一）</title>
    <link href="http://yoursite.com/posts/2018/09/01/Newton&#39;s_method_1.html"/>
    <id>http://yoursite.com/posts/2018/09/01/Newton&#39;s_method_1.html</id>
    <published>2018-09-01T06:47:52.521Z</published>
    <updated>2018-09-02T15:52:12.469Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;牛顿法，又称为牛顿-拉佛森方法，最开始是用来求解高次方程的根，其核心思想就是利用迭代点在曲线上的切线不断逼近曲线的根，直至收敛。我们知道，连续可微曲线的最值可以通过对曲线进行求导，并令导数为0来求解，所以牛顿法也可以作为一种最优化方法求最值。如果是把最优化（最小值）比作从山顶移动到山脚的过程，之前在文章<a href="https://linchart.github.io/posts/2017/08/07/Gradient_descent.html" target="_blank" rel="external">梯度下降法</a>有提到，梯度下降法是选择往相对当前位置来说最陡峭的地方向下移动，而牛顿法的目光会相对长远一点，牛顿法在选择方向的时候，不仅会考虑当前这一步是否是最陡峭的，还会结合下一步一起考虑，就像下象棋能够看到走完这一步之后下一步还应该怎么走才是最有利的，这是因为牛顿法是二阶收敛，梯度下降法是一阶收敛的。关于牛顿法二阶收敛的证明<a href="https://wenku.baidu.com/view/c5a755d850e2524de5187eb9.html" target="_blank" rel="external">请看这里</a>，根据收敛的方式，牛顿法的收敛速度显然要快于梯度下降算法。</p>
<a id="more"></a>  
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>&emsp;&emsp;其实在上大学的时候有学过牛顿法，但是作为数学系一名不折不扣的学渣，当然是考完就忘啦。关于牛顿法的原理，强烈推荐马同学在知乎上的这个回答<a href="https://www.zhihu.com/question/20690553" target="_blank" rel="external">如何通俗易懂地讲解牛顿迭代法求开方？</a> 行文流畅，脉络清晰明了，一看就懂，总结起来就是：   </p>
<ul>
<li>（1）牛顿法首先随机找一个初始点$x_0$，作为迭代点；</li>
<li>（2）计算函数$f(x)$ 在改点的切线，将该切线与$x$轴的交点作为新的迭代点，其更新方式为$$x_{n+1} = x_n - \frac{f(x_n)}{f’(x_n)}$$ </li>
<li>（3）重复步骤（2），直至找到函数的根或者满足指定的迭代条件。   </li>
</ul>
<h1 id="牛顿法应用"><a href="#牛顿法应用" class="headerlink" title="牛顿法应用"></a>牛顿法应用</h1><p>下面来看下，牛顿法是如何对逻辑回归的损失函数进行优化的。<br>上面提及的迭代更新方式$$x_{n+1} = x_n - \frac{f(x_n)}{f’(x_n)} \qquad （1）$$ 是求函数的根的，也就是求解$f(x)=0$。而求曲线的最值，是令曲线导数等于0来求解，所以，逻辑回归损失函数的优化目标应该是$f’(x) = 0$。<br>举个例子，对于函数$f(x) = (x+2)(x-2)(x-4)$，基于（1）进行迭代，其实就是求解$f(x)$的根，也就是下图的$x_0,x_1,x_2 $三个点。<div align="center"><img src="https://i.imgur.com/rU6o2rI.png" alt=""></div>    </p>
<p>但这不是我们的目的，我们的目的是要求该图中的点 $ x_3, x_4 $，这个好办，只要对$f(x)$求导，并求$f’(x) =0$ 的解就可以了，也就是下图的点   </p>
<div align="center"><img src="https://i.imgur.com/4KqP7nu.png" alt=""></div>   

<p>我们在函数$f’(x)$随机找一个点，作该点在$f’(x)$的上切线，将该切线与$x$轴的交点作为新的迭代点，那么更新方式就变为(也可以通过二阶泰勒展开式直接得出)：$$x_{n+1} = x_n - \frac{f’(x_n)}{f’’(x_n)} $$    </p>
<p><div align="center"><img src="https://i.imgur.com/xF3U0x9.png" alt=""></div><br>逻辑回归的方程为：<div align="center"><img src="https://i.imgur.com/G4IOlSh.png" alt=""></div><br>则其对应的似然损失函数为：</p>
<p><div align="center"><img src="https://i.imgur.com/KlUa8r7.png" alt=""></div><br>求损失函数的一阶导，有<div align="center"><img src="https://i.imgur.com/JWnzUHe.png" alt=""></div><br>由一阶导组成的矩阵，就是雅可比矩阵，记为$\nabla$<br>对应的二阶导为：</p>
<p><div align="center"><img src="https://i.imgur.com/S6p00wn.png" alt=""></div><br>由二阶导组成的矩阵，就是Hessian矩阵，记为$H$，所以，损失函数优化迭代的方式为：<br>$$\theta \leftarrow \theta -H^{-1}\nabla $$ </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;牛顿法，又称为牛顿-拉佛森方法，最开始是用来求解高次方程的根，其核心思想就是利用迭代点在曲线上的切线不断逼近曲线的根，直至收敛。我们知道，连续可微曲线的最值可以通过对曲线进行求导，并令导数为0来求解，所以牛顿法也可以作为一种最优化方法求最值。如果是把最优化（最小值）比作从山顶移动到山脚的过程，之前在文章&lt;a href=&quot;https://linchart.github.io/posts/2017/08/07/Gradient_descent.html&quot;&gt;梯度下降法&lt;/a&gt;有提到，梯度下降法是选择往相对当前位置来说最陡峭的地方向下移动，而牛顿法的目光会相对长远一点，牛顿法在选择方向的时候，不仅会考虑当前这一步是否是最陡峭的，还会结合下一步一起考虑，就像下象棋能够看到走完这一步之后下一步还应该怎么走才是最有利的，这是因为牛顿法是二阶收敛，梯度下降法是一阶收敛的。关于牛顿法二阶收敛的证明&lt;a href=&quot;https://wenku.baidu.com/view/c5a755d850e2524de5187eb9.html&quot;&gt;请看这里&lt;/a&gt;，根据收敛的方式，牛顿法的收敛速度显然要快于梯度下降算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="最优化" scheme="http://yoursite.com/tags/%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>为什么R方是解析变量的非减函数</title>
    <link href="http://yoursite.com/posts/2018/09/01/R-Square.html"/>
    <id>http://yoursite.com/posts/2018/09/01/R-Square.html</id>
    <published>2018-09-01T06:45:09.118Z</published>
    <updated>2018-09-01T17:03:06.663Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;在回归模型中，决定系数$R^2$表示反应变量$y$的总变异中可由回归模型中自变量解释的部分所占的比例，它是衡量所建立模型效果好坏的评价指标之一。根据$R^2$的计算方法，显然$R^2$越大越好，但是有一点需要注意的是，向模型中增加变量会导致$R^2$增大，或者至少保持不变，这就会造成一种假象，只要我不断地向模型中增加变量，$R^2$会越来越大，模型效果貌似越来越好，即使所增加的变量对于目标变量来说没有任何意义。为什么会这样子呢，下面从理论证明之。<br><a id="more"></a>  </p>
<h1 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h1><p>先把$R^2$的公式丢上来：$$R^2 =\frac{ESS}{TSS} =1 - \frac{\sum{(y-\hat{y})^2}}{\sum{(y-\bar{y})^2}}$$<br>&emsp;&emsp;其中，$y$是样本真实值，$\bar{y}$ 是样本均值$\frac{\sum_{i=1}^m{y_i}}{n}$，$\hat{y}$是预测值。<br>乍一看好像$R^2$的大小跟自变量$X$没有半毛钱关系，只跟实际$y$与预测$\hat{y}$值有关，其实事情并没有那么简单。<br>对于线性回归模型$$y = X\beta + \mu$$<br>其中$ X = (x_0,x_1,x_2,…,x_n) $ ，n为变量个数。 $\mu$ 为回归模型随机误差。   </p>
<p>我们一般会通过最小二乘法(OLS)来估计未知参数$\beta$，也就是将线性回归模型的残差平方和(sum of squared residuals,SSR)作为损失函数来优化，如下：   </p>
<div align="center"><img src="https://i.imgur.com/8quMaBj.png" alt="ssr"></div>

<p>显然，残差平方和越小越好，我们的目标就是找到一组参数$\beta$，使得残差平方和最小，这时拟合的曲线是最好的。求一个函数的最值，顺手一个求导并令导数等于0，就可搞定。   $$\min SSR(\beta) = min \sum_{i=1}^m{(y_i - X_i \hat{\beta})^2} $$<br>求关于$\beta$的导数，并令导数等于0</p>
<p><div align="center"><img src="https://i.imgur.com/VcXhMkp.png" alt=""></div><br>根据上面公式，可以得到OLS估计量满足一阶条件$-2X(y-X \hat{\beta})^T = 0$，从而有$X\mu^T = 0$<br>&emsp;&emsp;<br>   </p>
<h1 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h1><p>有了上面的知识点，证明起来就非常简单了。<br>设线性回归模型$y = X \beta + \mu $&emsp;&emsp;(1)的决定系数为:$$R_1^2 =1 - \frac{\sum{(y-\hat{y_1})^2}}{\sum{(y-\bar{y})^2}}$$<br>在原有模型基础上，增加一个变量，线性回归模型变为$y = X_0 \hat{\beta}_0+X \hat{\beta} + \nu $&emsp;&emsp;(2)，对应的决定系数为$$R_2^2 =1 - \frac{\sum{(y-\hat{y_2})^2}}{\sum{(y-\bar{y})^2}}$$<br>现在需要证明$R_2^2 \geqslant R_1^2$<br>&emsp;&emsp;<br> 证明：<br>根据$R^2$的计算公式，可以推导出</p>
<p><div align="center"><img src="https://i.imgur.com/zQxnfrV.png" alt=""></div><br>因此，只要证明$\mu^T \mu \geqslant \nu^T \nu $ 即可。</p>
<p>由变量之间相互独立，以及OLS的一阶条件推导出的$X\mu^T = 0$，所以有$\mu^T = X_0 \nu^t = X \nu^T = 0$<br>将(1)和(2)式合并起来:$$ X \beta + \mu = X_0 \hat{\beta}_0+X \hat{\beta} + \nu $$<br>上式两边乘以$\mu^T$ ，有</p>
<p><div align="center"><img src="https://i.imgur.com/gNUM6AD.png" alt=""></div><br>相似地，两边乘$\nu^T$，有</p>
<p><div align="center"><img src="https://i.imgur.com/zfS3MpP.png" alt=""></div><br>结合上面两个式子，有</p>
<p><div align="center"><img src="https://i.imgur.com/B7XYG0S.png" alt=""></div><br>为了方便后面公式展示，我这里先令</p>
<p><div align="center"><img src="https://i.imgur.com/YIGOBoI.png" alt=""></div><br>对$\nu^T \nu$进行推导，如下：</p>
<p><div align="center"><img src="https://i.imgur.com/IwRzSF4.png" alt=""></div><br>因此有</p>
<p><div align="center"><img src="https://i.imgur.com/Ep1mlcM.png" alt=""></div><br>证明完毕。   </p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>由证明可知，用$R^2$评价回归模型拟合效果具有一定的局限性，$R^2$越大并不能代表模型效果越好，针对此问题，出现了校正的决定系数(公式如下)，相对于$R^2$，当向模型中增加相对于目标变量无意义的自变量时，校正的决定系数值会减小。当然，稍微看下校正的决定系数计算公式就知道，问题还是存在的，就是当建模的样本量远远大于自变量个数的时候，$\bar{R^2}$趋近于$R^2$，此时校正的效果几乎消失。$$\bar{R^2} = 1 - (1-R^2)\frac{n-1}{n-p-1}$$</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;在回归模型中，决定系数$R^2$表示反应变量$y$的总变异中可由回归模型中自变量解释的部分所占的比例，它是衡量所建立模型效果好坏的评价指标之一。根据$R^2$的计算方法，显然$R^2$越大越好，但是有一点需要注意的是，向模型中增加变量会导致$R^2$增大，或者至少保持不变，这就会造成一种假象，只要我不断地向模型中增加变量，$R^2$会越来越大，模型效果貌似越来越好，即使所增加的变量对于目标变量来说没有任何意义。为什么会这样子呢，下面从理论证明之。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="效果评估" scheme="http://yoursite.com/tags/%E6%95%88%E6%9E%9C%E8%AF%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/posts/2018/09/01/k-means.html"/>
    <id>http://yoursite.com/posts/2018/09/01/k-means.html</id>
    <published>2018-09-01T06:42:01.011Z</published>
    <updated>2018-08-14T12:17:14.912Z</updated>
    
    <content type="html"><![CDATA[<p>k-means 、minibatch、k-pro</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;k-means 、minibatch、k-pro&lt;/p&gt;

    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>基于wrapper的无监督学习特征选择方法</title>
    <link href="http://yoursite.com/posts/2018/09/01/Unsupervised_feature_selection.html"/>
    <id>http://yoursite.com/posts/2018/09/01/Unsupervised_feature_selection.html</id>
    <published>2018-09-01T06:41:27.318Z</published>
    <updated>2018-08-13T15:53:02.195Z</updated>
    
    <content type="html"><![CDATA[<p>balabala</p>
<h1 id="无监督特征选择方法"><a href="#无监督特征选择方法" class="headerlink" title="无监督特征选择方法"></a>无监督特征选择方法</h1><p>遗传算法、基于模式相似性判断、信息增益、复相关系数、wrapper方法   </p>
<h1 id="如何有效选择特征子集"><a href="#如何有效选择特征子集" class="headerlink" title="如何有效选择特征子集"></a>如何有效选择特征子集</h1><h1 id="使用哪种聚类算法"><a href="#使用哪种聚类算法" class="headerlink" title="使用哪种聚类算法"></a>使用哪种聚类算法</h1><h1 id="如何筛选恰当的K"><a href="#如何筛选恰当的K" class="headerlink" title="如何筛选恰当的K"></a>如何筛选恰当的K</h1><h1 id="如何评估聚类效果"><a href="#如何评估聚类效果" class="headerlink" title="如何评估聚类效果"></a>如何评估聚类效果</h1><p>统一特征、不同K；不同特征，不同K。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;balabala&lt;/p&gt;
&lt;h1 id=&quot;无监督特征选择方法&quot;&gt;&lt;a href=&quot;#无监督特征选择方法&quot; class=&quot;headerlink&quot; title=&quot;无监督特征选择方法&quot;&gt;&lt;/a&gt;无监督特征选择方法&lt;/h1&gt;&lt;p&gt;遗传算法、基于模式相似性判断、信息增益、复相关系数、
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="聚类" scheme="http://yoursite.com/tags/%E8%81%9A%E7%B1%BB/"/>
    
      <category term="特征选择" scheme="http://yoursite.com/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
      <category term="无监督学习" scheme="http://yoursite.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>聚类算法使用小结</title>
    <link href="http://yoursite.com/posts/2018/09/01/unsupervised.html"/>
    <id>http://yoursite.com/posts/2018/09/01/unsupervised.html</id>
    <published>2018-09-01T06:39:59.980Z</published>
    <updated>2018-09-23T13:46:52.678Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;在实际的工程应用中，无监督聚类算法的使用频率相对于有监督分类预测算法的使用频率并不高，我们在开展业务过程中，很多时候，会很明确需要挖掘哪种类型的目标客户，比如是否会购买产品、是否潜在流失、是否投诉用户、是否欺诈用户等等。有时候也会有客户直接丢一堆无标签的数据过来，让你分析下这坨数据有什么特征，或者客户对他们产品的使用用户都是由哪些群体构成的并不清晰，无法针对性的做营销，这时候就可能需要用到聚类模型去对数据、用户群进行划分了。<br><a id="more"></a>         </p>
<p>&emsp;&emsp;聚类算法和分类预测算法的建模流程大同小异，都包括前期数据清洗、特征工程、模型构建、效果评估，由于聚类算法没有label作为参照物，在特征工程、效果评估上面会相对分类预测模型就显得更加麻烦、更有意思了。现在，对项目中经常用到的一些聚类算法做个简单的总结。</p>
<h1 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>&emsp;&emsp; k-means 可以说是最简单的聚类算法了（但是看sklearn里面k-means的源码实现发现事情并不简单orz）。算法首先随机选取k个质心，对每一个样本，分别计算该样本到这K个质心的距离，然后将该样本归到距离最短的簇中。将簇中所有样本的均值作为新的质心，再将每个样本分配到最近的簇中，一直迭代直至达到指定的迭代次数或者质心不在发生变化时，算法结束。   </p>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>简单直接高效   </li>
<li>收敛贼快（业界用的多不是没有原因的）   </li>
<li>结果解析性较强（效果好的情况下）   <h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3></li>
<li>基于样本中心作为质心注定了k-means会对异常值、噪声敏感</li>
<li>追求类内平方和最小也决定了其只能处理凸型数据，对复杂形状的数据无能为力，大多数时候聚类结果还是比较粗糙的。   </li>
<li>无法处理离散型数据     </li>
<li>需要指定聚类的个数（很多时候并不知道数据应该分成几类才是最好的）</li>
<li>需要指定初始点（初始点的选定对聚类结果影响较大）  </li>
</ul>
<h3 id="sklearn-调参"><a href="#sklearn-调参" class="headerlink" title="sklearn 调参"></a>sklearn 调参</h3><p>sklearn 里的kmeans 比较重要的参数有两个，n_clusters(聚类个数) 和 init(质心初始化方法)。n_clusters的选定，业务跟数据结合着来会比较好，或者你觉得这个数据大概可以分成10个类，可以从5-15类都试下，挑个效果最好的。但有时效果最好的其业务解析性并不一定好，还有的时候将数据分成6类效果是最好的，但业务部门说了我就要5类…<br>init 的值其实对聚类结果影响蛮大的，初始值选的不好可能无法得到有效的聚类结果。kmeans 对init的初始化有三种方法:random,k-means++,或者传入一个ndarray向量，默认是使用k-means++方法来初始化质心，其核心思想是：初始化的聚类中心之间的相互距离要尽可能的远。一般默认就行。</p>
<h2 id="凝聚聚类"><a href="#凝聚聚类" class="headerlink" title="凝聚聚类"></a>凝聚聚类</h2><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>&emsp;&emsp;凝聚聚类是一种自低向上的聚类方法。首先将每个样本当做一个聚类，然后合并距离最近或者最相似的两个类，直到满足某种停止准则未知。最核心的点是在如何衡量两个类的距离或者相似度。找了些相关资料，发现定义相似度的方法还是蛮多的，这里列举几个。   </p>
<ul>
<li>单链，两个不同的簇中，离得最近的两个点之间的距离，取距离值最小的两个簇进行合并，即MIN()     </li>
<li>全链，两个不同的簇中，离得最远的两个点之间的距离，取距离值最小的两个簇进行合并，即MAX()   </li>
<li>平均连,每个簇中所有点之间的平均距离，取点平均距离值最小的的两个簇进行合并，即AVERAGE   </li>
<li>方差，将簇中方差增加最小的两个簇进行合并(sklearn 中的ward)   </li>
</ul>
<h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><ul>
<li>能够处理具有复杂形状(非凸型)的数据   </li>
<li>无需指定聚类的个数（sklearn 中的 AgglomerativeClustering 将聚类的数量作为算法的停止准则）   </li>
</ul>
<h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>每次只合并两个簇，计算复杂度高，不适用于大数据量的聚类</li>
<li>只能基于已有的数据聚类，无法对新的数据进行预测</li>
</ul>
<h3 id="sklearn-调参-1"><a href="#sklearn-调参-1" class="headerlink" title="sklearn 调参"></a>sklearn 调参</h3><h2 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h2><h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><p>DBSCAN(density-based spatial clustering of applications with noise)，翻译过来就是：具有噪声的基于密度的空间聚类应用。DBSCAN能够根据数据中的密度识别密集区域，并将密度高的区域划分为簇，这些簇由数据中相对密度相对较低的区域分隔开。</p>
<h3 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h3><ul>
<li>无需指定聚类的个数</li>
<li>可以处理复杂形状的数据</li>
<li>抗噪声，能够识别噪声点<h3 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h3></li>
<li>簇之间的密度不均匀时，聚类效果可能不好。（比如某个簇，密度较大，另一个簇密度较稀疏，当调大邻域内最小样本点时，密度较稀疏的簇会变化较快。）</li>
<li>跟凝聚聚类一样，无法对新的数据进行预测<h3 id="sklearn-调参-2"><a href="#sklearn-调参-2" class="headerlink" title="sklearn 调参"></a>sklearn 调参</h3>该算法在大多数数据集上面都能够获得不错的效果，但是调参过程有时非常坎坷。关键是目前对复杂形状的聚类评估效果并不理想。 eps min_samples</li>
</ul>
<h2 id="高斯混合聚类"><a href="#高斯混合聚类" class="headerlink" title="高斯混合聚类"></a>高斯混合聚类</h2><h3 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h3><p>算法假定每个聚类的簇都符合高斯分布（正太分布），样本数据呈现的分布就是各个聚类的分布的叠加，所以称为高斯混合。该算法首先指定高斯混合成分个数K（这里K就是要聚类的个数），随机给每一个分布的均值和方差（协方差）赋初始值。对每一个样本，计算其在各个高斯分布下的后验概率（EM中的E步），在根据最大似然估计，将每个样本对该高斯分布的概率作为权重来计算加权均值和方差（协方差）（EM中的M步），用更新之后的值替换原来的初始值，直至模型满足停止条件（比如迭代次数），算法结束。</p>
<h3 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h3><ul>
<li>只要给定的成分个数足够多，理论上可以任意逼近任何连续的概率分布   </li>
<li><h3 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点"></a>缺点</h3><h3 id="sklearn-调参-3"><a href="#sklearn-调参-3" class="headerlink" title="sklearn 调参"></a>sklearn 调参</h3>调参经验</li>
</ul>
<h2 id="MeanShift"><a href="#MeanShift" class="headerlink" title="MeanShift"></a>MeanShift</h2><h3 id="原理-4"><a href="#原理-4" class="headerlink" title="原理"></a>原理</h3><p>MeanShift是一种非参数聚类算法，无需指定聚类的数目。主要涉及两个概念Mean(均值)、Shift(偏移)。其算法思想很简单，算法首先随机选取初始迭代点$x$，将该点到附近区域内所有点分别组成向量求和取平均偏移量，移动该点$x$至平均偏移向量末端，作为新的迭代点，不断移动，直至满足指定条件，算法结束。   </p>
<h3 id="优点-4"><a href="#优点-4" class="headerlink" title="优点"></a>优点</h3><h3 id="缺点-4"><a href="#缺点-4" class="headerlink" title="缺点"></a>缺点</h3><h1 id="sklearn-调参-4"><a href="#sklearn-调参-4" class="headerlink" title="sklearn 调参"></a>sklearn 调参</h1><h1 id="高维可视化PCA、TSNE"><a href="#高维可视化PCA、TSNE" class="headerlink" title="高维可视化PCA、TSNE"></a>高维可视化PCA、TSNE</h1><p>&emsp;&emsp;很多时候，即使做了前期的业务了解和数据探索，你也很难判断应该选择哪种聚类模型。如果每个模型都试一遍的话，时间成本未免太高，而将高维数据可视化，能够为我们选择聚类模型提供一些指导性意见。数据可视化首先是将高维数据降到低维（一般二维），然后基于低维数据进行可视化，常用的方法有两种：PCA 和TSNE。<br>&emsp;&emsp;PCA是一种被广泛应用的数据压缩、数据降维方法，该方法以方差最大的方向作为坐标轴方向对数据旋转以保留主要信，旋转后的特征在统计上不相关。这里以sklearn里的iris数据集为例，数据属性为：花萼长度、花萼宽度、花瓣长度、花瓣宽度 四个特征，将这四个特征标准化处理后用PCA降维得到下图。</p>
<p><div align="center"><img src="https://i.imgur.com/ABEu70N.png" alt=""></div><br>PCA在旋转时没有用到任何类别信息，降维后的数据相对于原数据其实相当于新的数据变换，一般很难对图中的两个轴做出解析。有一类用于可视化的算法称为流行学习算法，TSNE是其中的一种，降维效果奇好，其思想是找到数据的一个二维表示，尽可能地保持数据点之间的距离。让在原始特征空间中距离较近的点更加靠近，原始特征空间中相距较远的点更加远离。t-SNE 重点关注距离较近的点，而不是保持距离较远的点之间的距离。换句话说，它试图保存那些表示哪些点比较靠近的信息，这里同样使用iris数据进行tsne可视化。</p>
<p><div align="center"><img src="https://i.imgur.com/jKTWaMn.png" alt=""></div><br>tsne只能应用于训练的数据，不能应用于测试集，并且在处理稍大数据量的时候效率很低，如果要聚类的数据比较大，可以考虑抽样可视化。如果数据形状复杂，这时候基于指标的效果评估并不好，可以考虑基于抽样数据聚类，然后基于聚类结果建立分类模型。<br>这里pca和tsne可视化iris数据的效果区分度不是很明显，其实从使用经验来看，在数据量在千以上的情况下，同一份数据使用tsne进行可视化，数据的区分度比pca要好很多，这是因为pca需要对数据进行标准化处理，使得大部分数据都集中在一个很小的范围内。但是数据量越大tsne的处理速度越慢，很无奈。   </p>
<h1 id="无监督特征选择"><a href="#无监督特征选择" class="headerlink" title="无监督特征选择"></a>无监督特征选择</h1><p>&emsp;&emsp;无监督机器学习模型的特征选择，sklearn上还没有相关的API，其实挺麻烦的，应该使用何种评判标准来认为这个特征是对聚类算法有效的？网上有一些方法，比如基于遗传算法、模式相似性判断、复相关系数之类的，也找了几篇论文，看到一篇讲到可以参考有监督学习的wrapper方法进行特征选择，感觉还蛮有意思的，参考着实现了下，用起来效果也还OK。</p>
<h1 id="效果评估"><a href="#效果评估" class="headerlink" title="效果评估"></a>效果评估</h1><p>&emsp;&emsp;聚类结果的效果评估，方法还是挺多的，有分有标签的评价指标和无标签的评价指标两类。有标签的评价。比如，这些方法的核心都基本一样，就是将类，尤其是针对任意形状的簇，很多时候，你用某个指标评估聚类效果，如果指标值非常好，那说明模型用对了，如果指标值表现很差，那很有可能是评估方法选错了。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>“所有模型都是错的，但有些模型是有用的”</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;在实际的工程应用中，无监督聚类算法的使用频率相对于有监督分类预测算法的使用频率并不高，我们在开展业务过程中，很多时候，会很明确需要挖掘哪种类型的目标客户，比如是否会购买产品、是否潜在流失、是否投诉用户、是否欺诈用户等等。有时候也会有客户直接丢一堆无标签的数据过来，让你分析下这坨数据有什么特征，或者客户对他们产品的使用用户都是由哪些群体构成的并不清晰，无法针对性的做营销，这时候就可能需要用到聚类模型去对数据、用户群进行划分了。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="聚类" scheme="http://yoursite.com/tags/%E8%81%9A%E7%B1%BB/"/>
    
      <category term="无监督监督学习" scheme="http://yoursite.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>softmax</title>
    <link href="http://yoursite.com/posts/2017/09/03/softmax.html"/>
    <id>http://yoursite.com/posts/2017/09/03/softmax.html</id>
    <published>2017-09-03T07:35:43.317Z</published>
    <updated>2017-09-12T08:41:45.087Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;上一篇讲了逻辑回归的由来，在分类问题上，逻辑回归是一个二分类模型，而实际的项目中有可能需要处理一些多分类的问题（比如经典的MNIST），这时候如果使用二分类模型去处理多分类问题，会相对麻烦，这里介绍一种处理多分类问题的简单模型-<a href="https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0" target="_blank" rel="external">softmax</a>。softmax相当于做了一个归一化的工作，让强者相对更强，弱者相对更弱。<br><a id="more"></a>   </p>
<h1 id="模型推导"><a href="#模型推导" class="headerlink" title="模型推导"></a>模型推导</h1><p>&emsp;&emsp;softmax模型是逻辑回归模型的推广，逻辑回归模型是softmax模型的一个特例，这两者都算是<a href="https://zh.wikipedia.org/wiki/%E5%BB%A3%E7%BE%A9%E7%B7%9A%E6%80%A7%E6%A8%A1%E5%9E%8B" target="_blank" rel="external">广义线性模型(Generalized Linear Model,GLM)</a>。广义线性模型假设在给定属性$x$和参数$\theta$之后，类别$y$的条件概率$p(y|x;\theta)$服从指数分布族，它是长这样子的：$$P(y;\eta)=b(y)exp(\eta^TT(y) - a(\eta))$$<br>下面就是根据类别的联合分布概率密度搞成上面公式的样子。假设现在有$k$种分类$y \in (1,2,…,k)$的数据，对于每一条观测（样本）都有一个对应的类别，假设每种分类对应的概率是$(p_1,p_2,…,p_k)$，那么所有类别的概率之和就是：</p>
<p><div align="center"><img src="https://i.imgur.com/yTTGVJg.png" alt=""></div><br>则第$k$类的概率也可以写成：</p>
<p><div align="center"><img src="https://i.imgur.com/j17i1nv.png" alt=""></div><br>对于多分类，可以把类别写成编码向量的形式，向量的第$i$个位置为1，表示第$i$个类别，向量的其他位置为0。比如有5个分类，类别1可以表示成$(1,0,0,0,0)$，类别2可以表示成$(0,1,0,0,0)$。因为第$k$类可以用前$k-1$类表示，所以可以用$k-1$维向量$T(y)$表示类别。</p>
<p><div align="center"><img src="https://i.imgur.com/ir9jE10.png" alt=""></div><br>用函数$\mu(y=i)=1$表示第$y=i$为真，当$y=i$为假时，有$\mu(y=i)=0$。这样就可以将所有了类别的概率整合起来：<br>$$P(y;p)=p_1^{\mu(y=1)}p_2^{\mu(y=2)}…p_k^{\mu(y=k)}$$<br>因为第$k$类可以用前$k-1$类表示，所以上面的式子可以写成：</p>
<p><div align="center"><img src="https://i.imgur.com/oMxFahJ.png" alt=""></div><br>当$y=1$时，$P(y;\eta)=p_1$，当$y=2$时，$P(y;\eta)=p_2$，以此类推。再用向量$T(y)$表示，就变成了：</p>
<p><div align="center"><img src="https://i.imgur.com/VZjY1Dj.png" alt=""></div><br>将上面式子的右边，写成下面这种形式：</p>
<p><div align="center"><img src="https://i.imgur.com/gXyMkFx.png" alt=""></div><br>也就是先取对数，再将结果作为$e$的指数（整个推导过程中，最关键的技巧就是在这里了），然后稍微做下推导，就变成这样子：</p>
<p><div align="center"><img src="https://i.imgur.com/GO6DwZR.png" alt=""></div><br>然后，另：$b(y)=1$，以及$a(\eta)=-lnp_k$，还有$\eta$：</p>
<p><div align="center"><img src="https://i.imgur.com/lgeu369.png" alt=""></div><br>上面的公式就可以写成（终于凑成了指数分布族的形式）：</p>
<p><div align="center"><img src="https://i.imgur.com/Wm0WbO3.png" alt=""></div><br>由于</p>
<p><div align="center"><img src="https://i.imgur.com/CP4NAIL.png" alt=""></div><br>因为</p>
<p><div align="center"><img src="https://i.imgur.com/WPVnzVN.png" alt=""></div><br>所以</p>
<p><div align="center"><img src="https://i.imgur.com/PVvowO3.png" alt=""></div><br>所以：</p>
<p><div align="center"><img src="https://i.imgur.com/AKfeo3C.png" alt=""></div><br>再根据$p_i=p_ke^{\eta_i}$，可以得到:</p>
<p><div align="center"><img src="https://i.imgur.com/hbh5E4H.png" alt=""></div><br>再令$\eta = w^Tx+b$，所以对于类别$i$，其概率可以表示成：</p>
<p><div align="center"><img src="https://i.imgur.com/1U9QI1q.png" alt=""></div><br>&emsp;&emsp;softmax模型推导到这里已经算是结束了。<br>&emsp;&emsp;我们再来看看，为什么说softmax相当于做了一个归一化的工作，让强者相对更强，弱者相对更弱。先看归一化，很简单，分母其实就是将分子从$1$到$k$累加起来，所以对于任意的$p_i$都有$p_i \in [0,1]$。对于后者，softmax相当于做了一个拉大差距的工作，假设已经算好$w^Tx+b$就是以下几个值$[5,4,3,2,1]$，根据公式计算出来的对应的softmax值为$[0.636, 0.234, 0.086, 0.032, 0.012]$，可以看到，各个数之间的相对差距拉的更远了，所以softmax能够很好地凸显较大的数值。   </p>
<h1 id="似然函数"><a href="#似然函数" class="headerlink" title="似然函数"></a>似然函数</h1><p>&emsp;&emsp;像上篇逻辑回归一样，继续使用对数似然函数去估计参数。对于单个样本，当然是命中样本实际类别的概率越大越好。   </p>
<p><div align="center"><img src="https://i.imgur.com/VIef1f9.png" alt=""></div><br>这里$x^{(i)}$表示第$i$个样本，相对应的,$y^(i)$表示第$i$个样本对应的类别。<br>同理，对于所有的样本，也是希望命中样本实际类别的概率越大越好，然后再取个对数（这里为了方便书写公式，将$w^Tx+b$写成$\theta^T x$），这样就有：</p>
<p><div align="center"><img src="https://i.imgur.com/u9zVvpo.png" alt=""></div>   </p>
<h1 id="最优化"><a href="#最优化" class="headerlink" title="最优化"></a>最优化</h1><p>&emsp;&emsp;搞定了代价函数，剩下的工作就是求解参数了，一般可以使用梯度下降（上升）或者是牛顿迭代法来求解，这里以梯度下降（转化成求极小值）为例（梯度上升的话就是直接求解上面公式的极大值）。<br>&emsp;&emsp;因为公式会写的比较长，这里为了方便，将$w^Tx+b$写成$\theta ^Tx$的形式（都毕业了，还要把微积分捡回来，也是蛋疼）。<br>&emsp;&emsp;首先是求导，在求导之前，先放两个需要用到的公式，这样整个求导过程会更加清晰。对于对数和分数的求导有下面两个公式：</p>
<p><div align="center"><img src="https://i.imgur.com/z4v1gM6.png" alt=""></div><br>对第$l$类的参数$\theta _l$进行求导，所以就会有：</p>
<p><div align="center"><img src="https://i.imgur.com/5BIYhy0.png" alt=""></div><br>然后通过梯度上升更新参数：</p>
<p><div align="center"><img src="https://i.imgur.com/qzGUwzb.png" alt=""></div><br>这里的$\alpha$为学习率。   </p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>哈哈</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;上一篇讲了逻辑回归的由来，在分类问题上，逻辑回归是一个二分类模型，而实际的项目中有可能需要处理一些多分类的问题（比如经典的MNIST），这时候如果使用二分类模型去处理多分类问题，会相对麻烦，这里介绍一种处理多分类问题的简单模型-&lt;a href=&quot;https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0&quot;&gt;softmax&lt;/a&gt;。softmax相当于做了一个归一化的工作，让强者相对更强，弱者相对更弱。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="监督学习" scheme="http://yoursite.com/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="对数线性模型" scheme="http://yoursite.com/tags/%E5%AF%B9%E6%95%B0%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="广义线性模型" scheme="http://yoursite.com/tags/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="多分类模型" scheme="http://yoursite.com/tags/%E5%A4%9A%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归</title>
    <link href="http://yoursite.com/posts/2017/09/02/logistic.html"/>
    <id>http://yoursite.com/posts/2017/09/02/logistic.html</id>
    <published>2017-09-02T02:11:02.897Z</published>
    <updated>2018-09-02T15:09:54.700Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;逻辑回归（Logistic Regression）是一个非常经典的回归模型，在神经网络里作为激活函数时也称为sigmoid函数。逻辑回归模型既可以处理线性分类问题也可以处理非线性分类问题，但其本质上是一个线性模型。作为机器学习入门级模型，该模型既简单又强大，在现实中有着广泛应用，比如国外成熟的信用卡评分模型就是基于逻辑回归模型建立的。<br><a id="more"></a>   </p>
<h1 id="模型推导"><a href="#模型推导" class="headerlink" title="模型推导"></a>模型推导</h1><p>&emsp;&emsp;线性模型试图通过属性间的线性组合来预测目标值，它的模型形式是这样的：$$y=w_1x_1+w_2x_2+…+w_dx_d+b$$<br>其中$x_i,  i\in (1,2,…,d)$表示$x$在第$i$个属性上的取值。上式表示成向量的形式就是：$$y=w^Tx+b$$<br>其中$w=(w_1;w_2;…;w_3)$。<br>&emsp;&emsp;在二维平面上，线性回归模型，其实就是一条直线，再来看下另外一个函数$y=e^x$。<div align="center"><img src="https://i.imgur.com/FvaFjR8.png" alt=""></div><br>不难看出对数函数和线性函数都是单调的，那么肯定可以找到一个映射函数使得对于直线上的任意一个点$y=w^Tx+b$，都能在函数$y=e^x$上找到唯一的一个点与之相对应。对于函数$y=e^x$不妨令：$$x \leftarrow w^Tx+b$$则有$y=e^{w^Tx+b}$，两边取对数之后，就有$lny=w^Tx+b$。</p>
<p><div align="center"><img src="https://i.imgur.com/zfjSPxA.png" alt=""></div><br>给定数据集$D= \{ (x_1,y_1),(x_2,y_2),…,(x_m,y_m) \} $，其中 $y_i \in \{0,1\}$。我们可以用概率$p,p \in [0,1]$来表示样本$x$被判为$y=1$时的可能性，$p$值越大，代表$x$被归为正样本$y=1$的可能性越大，也就是$p(y=1|x)$的概率为$p$，那么对应的$p(y=0|x)$的概率就是$1-p$，这两者的比值$$\frac{p}{1-p}$$称为“几率”也有些家伙会称它为“优势比”。<br>这个时候，我们再回到取对数之后的等式：$lny=w^Tx+b$，令$y=\frac{p}{1-p}$，可以将等式转化为：$$ln\frac{p}{1-p}=w^Tx+b$$ 同时将等式两边的值作为指数，以$e$为底，可以得到：$$\frac{p}{1-p} = e^{w^Tx+b}$$ 稍微处理一下就可以推导出经典的逻辑回归模型:<div align="center"><img src="https://i.imgur.com/LLuWYHk.png" alt=""></div><br>也就是说，对于$y=1$，我们有$$p(y=1|x)=\frac {e^{w^Tx+b}}{1+e^{w^Tx+b}}$$<br>对于$y=0$，我们有$$p(y=0|x)=\frac {1}{1+e^{w^Tx+b}}$$<br>为了看的更舒服一点，令$z=w^Tx+b$，分子分母再同时除以$e^z$，则模型写成以下形式：$$f(z)=\frac {1}{1+e^{-z}}$$<br>上面的等式其实是一个单调“S”形函数，$f(z)$的取值在$(0,1)$之间，当$z$取值越大时，$f(z)$越接近于1，反之，当$z$取值越小时，$f(z)$越接近于0，当$z=0$时，$f(z)=0.5$。<div align="center"><img src="https://i.imgur.com/zmzPE1t.png" alt=""></div><br>其工作方式是，当$z&gt;0$时，对应的样本被判为正类$y=1$，当$z&lt;0$时，对应的样本被判为负类$y=0$，当$z=0$时，可视情况将样本归为任意类别。   </p>
<h1 id="线性and非线性"><a href="#线性and非线性" class="headerlink" title="线性and非线性"></a>线性and非线性</h1><p>为什么逻辑回归模型，既可以解决线性分类，也可以解决部分非线性分类问题呢？这里举两个例子。   </p>
<h2 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h2><p>假设有$y \in \{0,1\}$两类样本，如下图，存在一条直线$f(x_1,x_2)=\frac{1}{2}x_1-x_2+1$（我随便写的）能把这两类样本完全分割开，也就是对于所有的$y=1$有$f(x_1,x_2)&gt;0$，对于所有的$y=0$有$f(x_1,x_2)&lt;0$。   </p>
<p><div align="center"><img src="https://i.imgur.com/U543AYa.png" alt=""></div><br>我们将$f(x_1,x_2)$代入逻辑回归模型，就会有：$$f(f(x_1,x_2))=\frac {1}{1+e^{-f(x_1,x_2)}}$$<br>显然，根据刚才提到的逻辑回归的工作方式，当$f(x_1,x_2)&gt;0$时，逻辑回归函数的取值$f(f(x_1,x_2))&gt;0.5$，也就是说，样本$(x_1,x_2)$被判为正类的可能性更大，相对应地，当$f(x_1,x_2)&lt;0$时，逻辑回归函数的取值$f(f(x_1,x_2))&lt;0.5$，样本$(x_1,x_2)$被判为负类的可能性更大。   </p>
<h2 id="非线性分类"><a href="#非线性分类" class="headerlink" title="非线性分类"></a>非线性分类</h2><p>&emsp;&emsp;同样，假设有$y \in \{0,1\}$两类样本，但是这两类样本在二维平面上不再线性可分，而是长成下面的样子。   </p>
<p><div align="center"><img src="https://i.imgur.com/hz0Zzvq.png" alt=""></div><br>这个时候继续使用直线一刀切地划分显然是不行的（当然，你可以将样本映射到高维空间，然后寻找划分超平面这两类样本分隔开）。但是我们可以找到这样的一个圆（如下图），能把正负样本完全分隔开，假设关于这个圆的函数是$(x1-4)^2+(x2-3)^2=0$（还是随便写的）。   </p>
<p><div align="center"><img src="https://i.imgur.com/wGa2z5I.png" alt=""></div><br>函数$(x1-4)^2+(x2-3)^2&gt;0$时，代表样本点位于圆的外面，此时有$y=1$，把函数代入逻辑回归模型，会有$f(x)&gt;0.5$，此时样本$(x_1,x_2)$被判为正类的可能性更大。同样地，函数$(x1-4)^2+(x2-3)^2&lt;0$时，代表样本点位于圆的里面，此时有$y=0$，把函数代入逻辑回归模型，会有$f(x)&lt;0.5$，此时样本$(x_1,x_2)$被判为负类的可能性更大。<br>&emsp;&emsp;因此，逻辑回归模型能够很好地处理线性分类以及部分非线性分类问题。   </p>
<h1 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h1><p>&emsp;&emsp;模型找出来了，但是参数$w$和$b$还没有求解，所以又到了寻找损失函数的时候了。在线性回归中，一般会使用均方误差（也叫最小二乘法）来衡量预测的好坏，该损失函数试图找到这样的一条直线，能够使得所有样本到直线上的距离之和最小，说白了就是用欧氏距离来衡量预测效果。</p>
<p><div align="center"><img src="https://i.imgur.com/wd7PCwB.png" alt=""></div><br>&emsp;&emsp;用均方误差求解线性回归参数，是因为该损失函数是一个凸函数，凸函数有一个比较好的性质就是，局部极小点就是全局最小点，在达到全局最优的过程中不会经历太多的波折，更不会走到半路就以为是终点。<div align="center"><img src="https://i.imgur.com/fL6A9Bv.png" alt=""></div><br>不妨也试一下，使用均方误差来衡量逻辑回归模型的拟合效果如何，把上面的$h_ \theta(x)$换成逻辑回归的方程式可以得到：<div align="center"><img src="https://i.imgur.com/GsLNrEl.png" alt=""></div><br>很遗憾的是，这个函数是非凸的，它可能是长成这个样子的：<div align="center"><img src="https://i.imgur.com/X9sec3T.png" alt=""></div><br>这样的函数，在进行最优化求解时，函数值很有可能一直逗留在某些奇怪的地方比如图中红色的点，而没办法达到全局最优。<br>&emsp;&emsp;我们可以从概率的角度去思考，根据逻辑回归模型预测出来的概率值，肯定是离样本的真实标记越接近越好。比如对于正样本$x_1$，肯定是值$f(x_1)$越大越好，对于负样本$x_2$，值$f(x_2)$越小越好，因为值$f(x_2)$越小，样本$x_2$被判为负样本的可能性越大。<br>对所有训练样本，我们希望最大化其似然函数：</p>
<p><div align="center"><img src="https://i.imgur.com/tt9pKjt.png" alt=""></div><br>当$y_i=1$时，$p$越大，$L(p)$越大。当$y_i=0$时，$p$越小，$L(p)$越大。对上式取对数之后，就得到对数似然函数：<div align="center"><img src="https://i.imgur.com/j0iIYfg.png" alt=""></div><br>这里的$p(y_i=1|x_i)$对我们来说还是未知的，想要求解参数$w$，还需要稍微推导下。在模型推导中，我们知道：$$p(y=1|x)=\frac {e^{w^Tx+b}}{1+e^{w^Tx+b}}$$<br>把这个等式代入到损失函数（对数似然函数）中：</p>
<p><div align="center"><img src="https://i.imgur.com/d01eJlT.png" alt=""></div><br>这样就可以使用最优化方法来求解参数$w$了。对$L(w)$求极大值，就可以得到$w$的估计值。当然，也可以将对数似然函数写成以下形式，就变成求极小值：<div align="center"><img src="https://i.imgur.com/cyfV69c.png" alt=""></div>   </p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>哈哈</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;逻辑回归（Logistic Regression）是一个非常经典的回归模型，在神经网络里作为激活函数时也称为sigmoid函数。逻辑回归模型既可以处理线性分类问题也可以处理非线性分类问题，但其本质上是一个线性模型。作为机器学习入门级模型，该模型既简单又强大，在现实中有着广泛应用，比如国外成熟的信用卡评分模型就是基于逻辑回归模型建立的。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类模型" scheme="http://yoursite.com/tags/%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="监督学习" scheme="http://yoursite.com/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="对数线性模型" scheme="http://yoursite.com/tags/%E5%AF%B9%E6%95%B0%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="广义线性模型" scheme="http://yoursite.com/tags/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机（三）</title>
    <link href="http://yoursite.com/posts/2017/09/02/SVM3.html"/>
    <id>http://yoursite.com/posts/2017/09/02/SVM3.html</id>
    <published>2017-09-02T01:47:45.236Z</published>
    <updated>2018-09-06T05:40:04.647Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;硬间隔支持向量机要求训练数据是完全线性可分的，否则会宕机（无法工作）；软间隔支持向量机则允许很少量的样本线性不可分，模型的容错能力相对前者好一点；但是，对于非线性问题应该如何处理呢，怎么用一个平面将二维平面中的两个不同半径的同心圆分隔开，怎么将高维线性不可分的图形分隔开，在实际应用中，遇到非线性问题（大部分都是），如何用SVM解决？这就需要引入核函数的概念了。<br><a id="more"></a>   </p>
<h1 id="核函数定义"><a href="#核函数定义" class="headerlink" title="核函数定义"></a>核函数定义</h1><p>李航在《统计学习方法》中对核函数的定义如下：</p>
<div align="center"><img src="https://i.imgur.com/LwzIAFA.png" alt=""></div>   


<h1 id="核函数原理"><a href="#核函数原理" class="headerlink" title="核函数原理"></a>核函数原理</h1><p>先上总结：简单来说，核函数就是内积，或者说是用来计算映射到高维空间之后的内积的一种简便方法。<br>我们现在来细看下，核函数到底是个什么东西。在<a href="https://linchart.github.io/posts/2017/08/10/SVM.html" target="_blank" rel="external">支持向量机（一）</a> 中，我们已经推导出来，分类函数为：</p>
<div align="center"><img src="/img/SVM1/model1.png" alt=""> (1)</div><br>其中，$x_i^Tx$其实就是向量内积，可以用$&lt;·,·&gt;$表示。<br>继续开篇提的问题，如下图，怎么用一条直线将二维平面中的两个不同半径的同心圆分隔开？<br><div align="center"><img src="https://i.imgur.com/kpVjmAA.png" alt=""></div><br>设上面的点用$x=(x_1,x_2)$ 表示，如果直接基于分类函数(1)找一个平面将两个圆分割开，这样肯定不行，因为在二维平面上无论怎么调整，你都无法找到一条直线完好地区分上图中的两个圆。<br>但是换个角度看可能就不一样了，我们把它映射到三维空间看一下。<br><div align="center"><img src="https://i.imgur.com/RHtru1Y.png" alt=""></div>

<p>通过将数据从二维平面映射到三维空间，两个无法线性分隔的图形变成线性可分的了。这里使用的映射函数是$\phi(x)=(x_1^2,\sqrt{2}x_1x_2,x_2^2)$。</p>
<div align="center"><img src="https://i.imgur.com/EKs0aqj.png" alt=""></div><br>再看下面这个例子，如何用一条直线将两类不同颜色的点分开，显然在二维平面上无论怎么划分都无济于事。<br><div align="center"><img src="https://i.imgur.com/XzqVagK.png" alt=""></div><br>类似的方法，可以将这些点映射到三维空间，然后用一个平面将这两类数据轻易地区分出来。这里使用的映射函数是$x=(x1,x2,x1*x2)$<br><div align="center"><img src="https://i.imgur.com/5dy5iXk.png" alt=""></div><br>上面两个例子，套路都是：本来非线性的数据，找到一个映射函数，将其从低维映射到高维，然后在高维空间用线性分类方法将数据分割开来。<br>到目前为止，推理起来都比较顺畅，现在针对上面的例子做一些延伸，针对一个二次曲线，可以表示成：$$a1X_1 + a2X_1^2 + a3X_2 + a4X_2^2 + a5X_1X_2 + a6 = 0$$<br>构造一个五维空间，其中五个坐标的值为$$Z1 = X_1;Z2 = X_1^2 ;Z3 = X_2;Z4 = X_2^2 ;Z5 = X_1X_2$$<br><br>如果是一个三维曲线，那么需要构造一个19维的空间，可以看到，这个映射空间的维度会随着原空间维度的增加而成指数增长，这给会给$\phi(\cdot)$的计算带来了非常大的困难，而且如果遇到无穷维的情况，就根本无从计算了。更何况，映射到高维空间数据是否就线性可分还不一定呢。<br><br>如果我先在低维空间计算好两点之间的内积呢，这样就无需先映射到高维空间找映射函数，然后在根据映射函数计算内积了。针对第一个例子，设曲线上的任意两点为$x=(x_1,x_2),y=(y_1,y_2)$，其映射函数为$\phi(x)=(x_1^2,\sqrt{2}x_1x_2,x_2^2)$，那么映射函数计算内积后其实是跟$ x\cdot y $ 的平方的结果一样的，对应的核函数为。<br><div align="center"><img src="https://i.imgur.com/9x3ePe5.png" alt=""></div>

<div align="center"><img src="https://i.imgur.com/KtNPe1Y.png" alt=""></div>

<p>这样的话，针对三次曲线方程或者更高维曲线方程，无需先映射到高维空间，可以直接将两点之间的内积代入分类函数求解未知参数。这种直接在低维空间中计算内积的方法就称为核函数或者核技巧。    </p>
<p>&emsp;&emsp;核函数真正强大之处在于，只需要定义核函数<img class="kenel" src="https://i.imgur.com/NnE350g.png">在低维空间进行内积计算，而不需要显式地定义映射函数，这对本来就是高维或者无限维的特征空间来说尤为有效。另外，映射函数并不是唯一的，比如还是回到第一个同心圆的例子，映射函数可以是$\phi(x)=(x_1^2,\sqrt{2}x_1x_2,x_2^2)$，也可以是$\phi(x)=(x_1,x_2,x_1^2+x_2^2)$<br>&emsp;&emsp;网上其实有很多关于SVM、核函数的讲解和讨论，关于讲解可以参考<a href="https://blog.csdn.net/v_JULY_v/article/details/7624837" target="_blank" rel="external">支持向量机通俗导论（理解SVM的三层境界）</a>讲得是真心好。关于讨论可以参考知乎上的一个问题：<a href="https://www.zhihu.com/question/24627666" target="_blank" rel="external">机器学习有很多关于核函数的说法，核函数的定义和作用是什么？</a>    </p>
<h1 id="核函数类型"><a href="#核函数类型" class="headerlink" title="核函数类型"></a>核函数类型</h1><p>sklearn 中，可选的核函数有以下四种：</p>
<ul>
<li>线性 : <img class="math" src="https://i.imgur.com/zQGvu45.png">   </li>
<li>多项式：<img class="math" src="https://i.imgur.com/V6ApGyd.png">    </li>
<li>RBF ：<img class="math" src="https://i.imgur.com/ox1vqKT.png"></li>
<li>sigmoid: <img class="math" src="https://i.imgur.com/BhryA5V.png"></li>
</ul>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>&emsp;&emsp;核函数的作用其实很简单，一言蔽之，就是计算映射到高维空间之后的内积的一种简便方法或者技巧，它能够将原本需要在高维空间计算的内积直接在原特征空间进行，无需变换空间。   </p>
<p>&emsp;&emsp;说实话，写的这几篇文章中，核函数这篇是最耗时的，写到这里，感觉我对kenel trick 的理解还是很懵懂，不深刻，比如kenel 可以往那些方面进行推广、在实际应用中应该怎么选择kenel。写本文的时候，希望能够尽量通俗易懂地把它写明白，但是通俗易懂的前提是自己对这些概念的理解要很深入，所以在写的过程中出现过几次定好思路，写了大部分的时候发现不理想，重新整理的情况。不过从应用层面来说，了解SVM 和核函数的基本概念原理，知道模型是怎么运作的，在遇到问题的时候能够定位问题，并根据模型、方法的特性进行恰当的调整已经基本够用了。其他的再漫漫求索吧。   </p>
<p>ps: 再次吐槽， mardown 编辑公式有时出个bug真能逼死强迫症。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;硬间隔支持向量机要求训练数据是完全线性可分的，否则会宕机（无法工作）；软间隔支持向量机则允许很少量的样本线性不可分，模型的容错能力相对前者好一点；但是，对于非线性问题应该如何处理呢，怎么用一个平面将二维平面中的两个不同半径的同心圆分隔开，怎么将高维线性不可分的图形分隔开，在实际应用中，遇到非线性问题（大部分都是），如何用SVM解决？这就需要引入核函数的概念了。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类模型" scheme="http://yoursite.com/tags/%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="监督学习" scheme="http://yoursite.com/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机（二）</title>
    <link href="http://yoursite.com/posts/2017/09/02/SVM2.html"/>
    <id>http://yoursite.com/posts/2017/09/02/SVM2.html</id>
    <published>2017-09-02T01:46:27.994Z</published>
    <updated>2017-10-13T05:50:04.776Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;上一篇对线性支持向量机的原理做了推导，这个推导的前提跟感知机模型一样，假设训练样本是线性可分的，也就是存在一个超平面能够将正负样本完全分隔开，但是这种假设在实际项目中存在的可能性跟国足挺进世界杯的可能性是一样一样的。<br><a id="more"></a>   </p>
<h1 id="软间隔"><a href="#软间隔" class="headerlink" title="软间隔"></a>软间隔</h1><p>&emsp;&emsp;上一篇文章中提到的间隔，严格来说应该称之为“硬间隔”，因为该间隔存在的前提是所有训练样本都要满足约束条件$y_i(w^Tx_i+b) \geq 1$，但是对于下图中的两类样本点，线性支持向量机并不适用，因为没办法在不映射到高维的情况下找到一条直线能把两个类别分隔开。   </p>
<p><div align="center"><img src="https://i.imgur.com/28gricA.png" alt=""></div><br>&emsp;&emsp;如果使用感知机的话，模型会无法收敛，并且找到的超平面有可能效果还可以也有可能很差。上图两个类别之间，混淆的样本点其实不多，总不能因为几颗芝麻就丢了西瓜吧。那么有没有办法使得支持向量机“容忍”这些不多的样本点出错，而保证绝大部分的样本点被正确分类呢，其中一个办法就是使用“软间隔”，允许某些样本<strong>不满足</strong>约束条件$y_i(w^Tx_i+b) \geq 1$，也就是让某些样本点满足条件$y_i(w^Tx_i+b) &lt; 1$   </p>
<p><div align="center"><img src="https://i.imgur.com/kCVPfCt.png" alt=""></div></p>
<h1 id="软间隔最大化"><a href="#软间隔最大化" class="headerlink" title="软间隔最大化"></a>软间隔最大化</h1><p>&emsp;&emsp;为了获得最佳的划分超平面，还需要找出对应的损失函数来衡量划分超平面的效果。一方面，对于支持向量及以外的点，可以继续使用上一篇介绍的间隔最大化$\frac{2}{||w||}$来评估，也就是$min \frac{1}{2}||w||^2$；另一方面，对于支持向量内的点，没办法满足间隔大于等于1的约束条件，这种情况下，可以考虑引入松弛变量$\xi _i$ （其中$ \xi _i \geq 0$），使得函数间隔在加上松弛变量之后大于等于1，也就是：$$y_i(w^Tx_i+b) + \xi _i \geq 1 $$<br>将$\xi _i$挪到右边：$$y_i(w^Tx_i+b) \geq 1 -\xi _i $$<br>&emsp;&emsp;松弛变量可以理解为一种作用力，将误分类的点往正确分类的方向拉，拉回到对应的支持向量上。误分类的点与正确分类的边界越远，需要拉扯的力量越大，也就是$\xi_i$越大，对于支持向量后方的点，$\xi_i$值可以为0。<br>&emsp;&emsp;然后，同时对于误分类的点，给它一个惩罚因子(cost)，用字母$C$表示（$C \geq 0$），用来衡量误分类的代价，或者说，对离群点的重视程度（其尝试找出，在出现误分类的情况下，边界间隔最大的超平面以及保证数据点偏差量最小），这样损失函数就变成了：</p>
<p><div align="center"><img src="https://i.imgur.com/YDxHHzl.png" alt=""></div><br>&emsp;&emsp;这里的损失函数相对上篇只是多了后半部分。显然，对于惩罚因子$C$，当$C$无穷大时，表明只要有一个样本点是特异点的话就需要付出无限的代价，上面式子趋向于无穷大，这时将会使得所有的样本点都满足式子$y_i(w^Tx_i+b) \geq 1 $，这样的话软间隔最大化问题就变成硬间隔最大化问题（或者说是线性支持向量问题），但是由于给定的数据不是线性可分的，这样就会导致问题没有解；当$C$取有限值时，上式允许存在一些的样本不满足约束。<br>&emsp;&emsp;加上约束条件，目标函数变成如下凸二次规划问题：</p>
<p><div align="center"><img src="https://i.imgur.com/DI8wNXM.png" alt=""></div><br>跟<a href="https://linchart.github.io/posts/2017/08/10/SVM.html" target="_blank" rel="external">支持向量机（一）</a>中，目标函数的求解方式一样，可以通过拉格朗日乘子法得到对应的拉格朗日函数：</p>
<p><div align="center"><img src="https://i.imgur.com/6iBf3zp.png" alt=""></div><br>其中$\alpha _i \geq 0 , \mu _i \geq 0$是拉格朗日乘子。<br>一样的套路，求偏导，令$L(w,b,\alpha _i,\xi _i,\mu _i)$对$w,b,\xi _i$的偏导为0，可得：</p>
<p><div align="center"><img src="https://i.imgur.com/rqR8rWG.png" alt=""></div><br>将上面三个等式代入拉格朗日函数可以得到（公式推理写的有点宽，就不在这里展示了）：</p>
<p><div align="center"><img src="/img/SVM2/dual_problem.png" alt=""></div><br>与上一篇相似，上式同样需要满足KKT条件，也就是要求：</p>
<p><div align="center"><img src="/img/SVM2/KKT.png" alt=""></div><br>&emsp;&emsp;这里的$f(x_i)$其实就是$w^Tx_i+b$，通过KKT条件，我们能更加深入地了解SVM的本质–<strong>不管是硬间隔支持向量机还是软间隔支持向量机，最终根据模型求得的分隔超平面仅与支持向量有关。</strong><br>&emsp;&emsp;对于上面KKT条件中的等式$\alpha_i(y_i f(x_i)-1+\xi _i)=0$，对于任意的训练样本，总会有$\alpha_i = 0$或者$y_i f(x_i)-1+\xi _i=0$，不妨把训练样本分为两种：支持向量以及支持向量以外的点，现在来分情况讨论下：</p>
<ul>
<li>$\alpha_i=0$，这个时候$y_i f(x_i)-1+\xi _i$等于任意值都是ok的，也就是$\alpha_i=0$的时候，不会对样本有任何的影响。如上一篇提到的那样，在衡量目标损失的时候，对于非支持向量（也就是支持向量以外的点），为了满足损失函数最大化，$\alpha_i$必定等于0。</li>
<li>$\alpha_i&gt; 0$，为了满足KKT条件，必然有$y_i f(x_i)-1+\xi _i=0$，也就意味着，样本$(x_i,y_i)$是支持向量</li>
</ul>
<p>再结合偏导的等式$C=\alpha_i + \mu_i$ 和KKT条件$\mu_i \xi_i=0$、$\mu_i \geq 0$。由于$\mu_i$是大于等于0的，所以$\alpha_i$只能小于或等于$C$：</p>
<ul>
<li>$\alpha_i&lt;C$，此时有$\mu_i$大于0，因此$\xi_i=0$，这个时候，则对应的样本$(x_i,y_i)$刚好落在最大间隔边界上面。</li>
<li>$\alpha_i = C$，此时有$\mu_i =0$，此时若$\xi_i \leq 1$，则对应的样本$(x_i,y_i)$落在最大间隔内部；若$\xi_i &gt; 1$，则样本$(x_i,y_i)$分类错误。<br>引出核函数<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1>&emsp;&emsp;软间隔支持向量机虽然比硬间隔支持向量机好那么一丢丢，对噪声的容忍程度比硬间隔支持向量机稍微强一点，但是用来处理现实复杂的数据，还是远远不够的。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;上一篇对线性支持向量机的原理做了推导，这个推导的前提跟感知机模型一样，假设训练样本是线性可分的，也就是存在一个超平面能够将正负样本完全分隔开，但是这种假设在实际项目中存在的可能性跟国足挺进世界杯的可能性是一样一样的。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类模型" scheme="http://yoursite.com/tags/%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="监督学习" scheme="http://yoursite.com/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机（一）</title>
    <link href="http://yoursite.com/posts/2017/08/10/SVM.html"/>
    <id>http://yoursite.com/posts/2017/08/10/SVM.html</id>
    <published>2017-08-10T14:15:11.873Z</published>
    <updated>2017-10-13T05:49:45.602Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;支持向量机(Support Vector Machines, SVM)在实际应用中，算是一大分类神器了。原始的支持向量机是一种线性二分类模型，当支持向量机使用一些核技巧之后，可以从本质上变成非线性分类器。区别于感知机模型，线性可分支持向量机利用间隔（两个不同类的支持向量到超平面的距离）最大化求最优分离超平面，求得的解是唯一的。<br><a id="more"></a>   </p>
<h1 id="感知机回顾"><a href="#感知机回顾" class="headerlink" title="感知机回顾"></a>感知机回顾</h1><p>&emsp;&emsp;从<a href="https://linchart.github.io/posts/2017/08/01/perceptron_model.html" target="_blank" rel="external">感知机模型</a>我们知道，感知机的目标是只要能找到一个将训练数据集的正样本和负样本完全正确分割开的分离超平面就可以了，并不要求样本能被最大限度地划分，但是这样的话会导致模型的泛化能力不强，比如下左图，直线Y能把A,B两类样本完全分割开，是感知机模型的一个解，如果我们用Y去预测未知的数据，如下右图，$a1,a2$两个数据点实际为类别A，但却被直线Y误判为B类，说明该直线对未知数据的泛化能力并不强。  </p>
<div align="center"><img src="http://i.imgur.com/tClnuSC.png" alt=""></div><br>&emsp;&emsp;直观上，下图中的红色直线的划分效果比其他直线的划分效果都要好，因为该直线恰好在两个类的中间，尽量“公平”地远离了两个不同的类别。但是，如何才能找到这样的一个超平面使之能最好地区分正负样本，并且对未见的数据的泛化能力也是最强的呢？这就是支持向量机要解决的问题。<br><div align="center"><img src="http://i.imgur.com/Kfnltd2.png" alt=""></div>   

<h1 id="什么是支持向量？"><a href="#什么是支持向量？" class="headerlink" title="什么是支持向量？"></a>什么是支持向量？</h1><p>&emsp;&emsp;支持向量(Support Vector)的定义其实很简单，其实就是距离超平面最近的样本点，如下图中的$a1,a2,a3$三个点就称为支持向量。      </p>
<p><div align="center"><img src="http://i.imgur.com/yeygZgR.png" alt=""></div><br>为什么叫支持向量呢，因为所有的训练样本点其实都是以向量的形式表示的（尤其是当属性很多的时候），而上图的划分的超平面，仅仅由$a1,a2,a3$这三个点决定，与其他训练样本点一毛钱关系都没有（这也就是为什么SVM效率贼高的原因）！也就是说我们要找的划分超平面，其实是由这三个向量(vector)来支撑(support)的，因此我们称$a1,a2,a3$这三个点为Support Vector。so，只要找到支持向量，划分超平面也就找到了。</p>
<p><div align="center"><img src="http://i.imgur.com/gs3jyQ1.png" alt=""></div><br>好了，知道了支持向量的概念，我们就可以按照模型→策略→算法 的步骤去求解SVM了。   </p>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>&emsp;&emsp;上面的分析我们已经清楚地知道，我们的模型（目标函数）其实就是一个超平面，这个超平面可以通过如下线性方程来描述：$$w^Tx+b = 0$$   </p>
<h1 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h1><p>&emsp;&emsp;有了模型，还需要有一个恰当的策略（损失函数）来评估模型的分类效果。前面有提到，我们要找的超平面，应该尽可能地离两个不同样本点都远，以保证泛化能力，而在空间中，衡量样本点到划分超平面的远近，很自然会想到用欧式距离来度量。样本空间中，任意一点$x$到超平面$w^Tx+b = 0$的距离可写为：$$d=\frac{1}{||w||}|w^Tx+b|$$<br>带上类别之后，上面的距离公式可以写成：$$d=\frac{1}{||w||}y_i(w^Tx_i+b)$$ 对于下图类别A中的数据点$a1$，假设，在该点处，与分割超平面平行的超平面为：$w^Tx+b = c$，因为超平面的参数$w$和$b$同时放大或缩小$k(k\neq 0)$倍时，超平面是一样的（比如$2x+4y=6$和参数同时除以2之后的$x+2y=3$）。那么，不妨对$w^Tx+b = c$做一个变换，令$$w \leftarrow \frac{w}{c}$$ $$b \leftarrow \frac{b}{c}$$   </p>
<p>所以$a1$点处的超平面可以写成$w^Tx+b = 1$，同理，$a2,a3$处的超平面可以写成：$w^Tx+b = -1$。显然，对于所有的正例（类别A），以及所有的负例（类别B），使得以下不等式成立：</p>
<p><div align="center"><img src="http://i.imgur.com/oG5uKId.png" alt=""></div><br>这个时候，我们套上距离的公式，对于两个不同类别的支持向量$a1$和$a2,a3$到超平面的距离之和它可以写成：$$d = \frac{2}{||w||} $$<br>这里$\frac{2}{||w||} $也称为间隔。</p>
<p><div align="center"><img src="http://i.imgur.com/HNiyP6u.png" alt=""></div><br>我们的目标是让间隔尽量地大，这样划分超平面对未见数据的预测能力会更强，也就是希望：</p>
<p><div align="center"><img src="http://i.imgur.com/3CtCrXa.png" alt=""></div><br>因为函数$\frac{2}{||w||}$的单调性与$\frac{1}{2} {||w||}^2$是相反的，所以，最大化$\frac{2}{||w||}$时，相当于最小化$\frac{1}{2} {||w||}^2$，这样，最优化问题可以转化为凸二次规划问题（目标函数是二次的，约束条件是线性的）：</p>
<p><div align="center"><img src="http://i.imgur.com/fveIwjw.png" alt=""></div><br>so，损失函数就这样被找出来了。</p>
<h1 id="最优化"><a href="#最优化" class="headerlink" title="最优化"></a>最优化</h1><p>&emsp;&emsp;在求取有约束条件的最优化问题时，为了更容易求解，我们可以使用<a href="https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0" target="_blank" rel="external">拉格朗日乘子法</a>将其转化为原问题的对偶问题进行求解。也就是，对于上面式子的每条约束添加拉格朗日乘子$\alpha_i \geq 0$，对应的拉格朗日函数可以写为:<div align="center"><img src="http://i.imgur.com/TmaWsKK.png" alt=""></div><br>这里的$\alpha=(\alpha 1;\alpha 2;…;\alpha m)$，$\alpha 1$代表第一个不等式的拉格朗日乘子。成功避开约束条件之后，我们就可以更加便利地去求解极值了。对于函数$L(w,b,\alpha)$分别求参数$w$和$b$的偏导数，并令偏导数为0，可以得到：<div align="center"><img src="http://i.imgur.com/lgpPRj3.png" alt=""></div><br>把等式$w$代入到函数$L(w,b,\alpha)$:<div align="center"><img src="http://i.imgur.com/XbFF2vd.png" alt=""></div><br>因为<div align="center"><img src="http://i.imgur.com/HCvsZ1H.png" alt=""></div><br>所以可以把等式中的<div align="center"><img src="http://i.imgur.com/TuwthYm.png" alt=""></div>划掉。最后得到：<div align="center"><img src="https://i.imgur.com/9kod4YT.png" alt=""></div><br>根据对偶问题的性质，也就是任何一个求极小值的线性规划问题都可以转化为求极大值的线性规划问题。所以：<div align="center"><img src="http://i.imgur.com/tu14PxE.png" alt=""></div><br>可以计算出：<div align="center"><img src="http://i.imgur.com/hWrZsqw.png" alt=""></div><br>这样，最终的模型可以写成：</p>
<p><div align="center"><img src="/img/SVM1/model.png" alt=""></div><br>这里还需要注意一下的就是，$x_i^Tx$其实就是向量内积，可以用$&lt;·,·&gt;$表示，也就是：</p>
<p><div align="center"><img src="/img/SVM1/model1.png" alt=""></div><br>之所以写成这样，是方便后面理解支持向量机是如何使用Kernel进行非线性分类的。上面这个式子可以说是支持向量机核函数的基本形式了。<br>对于含有不等式约束的最优化，还必须满足KKT条件，也就是：</p>
<p><div align="center"><img src="/img/SVM1/kkt.png" alt=""></div><br>根据条件中的等式$\alpha_i(y_if(x_i)-1)=0$，对于任意的训练样本，总会有$\alpha_i = 0$或者$y_i f(x_i)-1=0$，若$\alpha_i=0$，样本$(x_i,y_i)$不会对损失函数有任何的影响；若$\alpha &gt;0$，则有$y_i f(x_i)-1 =0$，此时，样本$(x_i,y_i)$其实就是支持向量。</p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>&emsp;&emsp;基于数据线性可分假设的SVM显然会对噪声很敏感。相对于硬间隔支持向量机，后续会介绍对噪声容忍度稍微强一点的软间隔支持向量机，以及装备核函数之后的强大SVM；总的来说，SVM的基本原理以及使用都是比较简单的，如果只是为了调包，了解基础原理已经可以了。如果是想深入了解，仅仅知道这些还是远远不够的。<br>&emsp;&emsp;公式编辑已弃疗，复杂一点的公式还是截图显示吧，哎。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;支持向量机(Support Vector Machines, SVM)在实际应用中，算是一大分类神器了。原始的支持向量机是一种线性二分类模型，当支持向量机使用一些核技巧之后，可以从本质上变成非线性分类器。区别于感知机模型，线性可分支持向量机利用间隔（两个不同类的支持向量到超平面的距离）最大化求最优分离超平面，求得的解是唯一的。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类模型" scheme="http://yoursite.com/tags/%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="监督学习" scheme="http://yoursite.com/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降</title>
    <link href="http://yoursite.com/posts/2017/08/07/Gradient_descent.html"/>
    <id>http://yoursite.com/posts/2017/08/07/Gradient_descent.html</id>
    <published>2017-08-07T08:19:01.063Z</published>
    <updated>2017-08-10T09:39:11.797Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;李航在其《统计学习方法》中提到，机器学习的核心由三大块组成，分别为模型、策略、算法，其实说白了，机器学习就是一个模型 + 一个损失函数 + 一个最优化算法。损失函数是为了让模型能够更好地拟合或分类数据，最优化算法是用来优化损失函数的，而在机器学习中，最常用的最优化算法应该算梯度下降了，这篇文章就来讲讲梯度下降，探究下它是如何优化损失函数的。<br><a id="more"></a>   </p>
<h1 id="梯度下降原理"><a href="#梯度下降原理" class="headerlink" title="梯度下降原理"></a>梯度下降原理</h1><p>&emsp;&emsp;上文的感知机模型中有提到，梯度，其实就是求偏导，梯度下降就是沿着梯度的负方向移动。为了更直观地理解梯度下降，这里举一个网上的例子（实在想不出比这个更形象的解析了），比如我们现在站在山上的某个位置，如下图中的点A，目标是要最快速地下到山脚下，怎么一步一步走下去呢？我们可以选择往相对当前位置来说最陡峭的地方向下移动（也就是梯度的负方向），这样一直走到山脚。这个山脚有可能是整个大山的最低处（B点），也有可能是大山的局部低处（C或D点），这取决于我们移动的方向和幅度。这种情况下，梯度下降不一定能够找到全局最优解，有可能是一个局部最优解。  </p>
<div align="center"> <img src="http://i.imgur.com/dBpYr7I.png" alt=""> </div><br>但如果是下面这个凸函数图，我们就一定可以移动到最低处，因为不管我们处于山上的哪一个位置，只要我们保持每次移动都是向下的，就一定能到达最低点。这种情况下，梯度下降法得到的解就一定是全局最优解。<br><div align="center"><img src="http://i.imgur.com/fjgYW4z.png" alt=""></div>   

<h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><p>梯度下降算法如下（该算法摘抄自李航《统计学习方法》）：<br>输入：目标函数$f(x)$，梯度函数$g(x) = \nabla f(x)$，计算精度$\epsilon $<br>输出：$f(x)$的极小点$x^*$<br>&emsp;&emsp;(1) 取初始值$x^{(0)} \in R^n$，置$k=0$<br>&emsp;&emsp;(2) 计算$f(x^{(k)})$<br>&emsp;&emsp;(3) 计算梯度$g_k = g(x^{(k)})$，当$||g_k||&lt;\epsilon$时，停止迭代，令$x^* = x^{(k)}$；否则，令$p_k = -g(x^{(k)})$，求$\lambda_k　$使得$$f(x^{(k)}+ \lambda_k p_k) = min f(x^{(k)} + \lambda p_k) $$<br>&emsp;&emsp;(4) 置$x^{(k+1)} = x^{(k)} + \lambda_k p_k $，计算$f(x^{(k+1)})$，当$||f(x^{(k+1)}) - f(x^{(k)})|| &lt; \epsilon$ 或 $x^{(k+1)} - x^{(k)} &lt; \epsilon$时，停止迭代，令$x^* = x^{(k+1)}$<br>&emsp;&emsp;(5) 否则，令$k=k+1$，转$(3)$   </p>
<p>&emsp;&emsp;这里的$p_k$是搜索方向，$\lambda_k$是步长。假设有一个损失函数$y = x^2 - 4*x - 5 $，目标是找到$x$的值使得$y$取最小值，使用梯度下降求解如下：    </p>
<ul>
<li>梯度<br>$$\nabla x = -(2*x - 4)$$  </li>
<li>更新$x$的值<br>$$x \leftarrow x + \eta \nabla x $$   </li>
</ul>
<p>设$x$的初始值为10，学习速率$\eta$为0.2，则<br>第一次迭代：梯度$$\nabla x =-(2*10 - 4) = -16$$<br>&emsp;&emsp;$x$的值相应更新为$$ x = 10 - 0.2 * 16 = 6.8$$<br>&emsp;&emsp;对应的$y$值为$$y = 6.8*6.8 - 4*6.8 - 5 = 14.04$$<br>第二次迭代：梯度$$\nabla x =-(2*6.8 - 4) = -9.6$$<br>&emsp;&emsp;$x$的值相应更新为$$ x = 6.8 + 0.2 * （-9.6） = 4.88$$<br>&emsp;&emsp;对应的$y$值为$$y = 4.88*4.88 - 4*4.88 - 5 = -0.7056$$<br>&emsp;&emsp;如此循环迭代，直到达到指定迭代次数或者$y$值最优。这里一直迭代到第38次时，求得极小值$ y =-9.0$。因此，梯度下降就是一个不断沿着梯度的负方向移动直到达到局部或全局最优点的一个过程。</p>
<div align="center"><img src="http://i.imgur.com/WLUBgKm.png" alt=""></div>

<h1 id="梯度下降的三种形式"><a href="#梯度下降的三种形式" class="headerlink" title="梯度下降的三种形式"></a>梯度下降的三种形式</h1><p>梯度下降算法可以分为以下三种：</p>
<ul>
<li>批量梯度下降法(Batch Gradient Descent)   </li>
<li>随机梯度下降法(Stochastic Gradient Descent)   </li>
<li>小批量梯度下降法(Mini-batch Gradient Descent)   </li>
</ul>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>&emsp;&emsp;为了更好地阐明这三种方式的本质与区别，这里以线性回归模型为例。对于任意一个线性方程，我们可以写成以下形式：$$h_\theta (x)= \theta_0 + \theta_1 x_1 + \theta_2 x_2 + …+\theta_n x_n$$ 这里的$(\theta_0,\theta_1,…,\theta_n)$为参数，也称为权重。假设现在有一堆数据$X=(x^0,x^1,x^2,…,x^m)$以及对应的$y$值$Y=(y^0,y^1,y^2,…,y^m)$。我们的目标是要找到参数$(\theta_0,\theta_1,…,\theta_n)$使得$y$值与theta（符号theta在这里硬是显示不出来，我也是醉了）值尽可能地接近。这里用h_theta(x)与y的平方误差作为损失函数来评估h_theta(x)与y值的接近程度。<div align="center"><img src="http://i.imgur.com/Gge2YPo.png" alt=""></div><br>有了损失函数之后，我们就可以来聊一聊这三种梯度下降的形式了。   </p>
<h2 id="批量梯度下降法"><a href="#批量梯度下降法" class="headerlink" title="批量梯度下降法"></a>批量梯度下降法</h2><blockquote>
<p>批量梯度下降法每次迭代都需要用到全量的训练数据，优化过程比较耗时。   </p>
</blockquote>
<p>&emsp;&emsp;前面我们已经知道了梯度下降其实就是对参数求偏导，然后沿着梯度的负方向去更新参数。对于上面的损失函数$J(\theta)$，我们随机初始化权重$\theta$，然后重复执行以下更新：$$\theta_j:=\theta_j + (-\alpha \frac{\partial}{\partial\theta_j}J(\theta))$$<br>&emsp;&emsp;这里的$\theta_j:$是指分别对$j=0,1,2,…,n$个参数求偏导。$\alpha$指学习速率，代表每次向着$J(\theta)$最陡峭的方向移动的步幅。从上面公式可以看到，为了更新参数$\theta_j:$，式子右边的$\frac{\partial}{\partial\theta_j}J(\theta))$我们还没有知道的。当我们只有一个数据点$(x,y)$的时候，$J$的偏导数：   </p>
<p><div align="center"><img src="http://i.imgur.com/MCvj18T.png" alt=""></div><br>因此，对于单个训练样本，其更新规则为：<div align="center"><img src="http://i.imgur.com/rGou5cf.png" alt=""></div><br>对于所有的训练样本，累加上述损失函数的偏导为：<div align="center"><img src="http://i.imgur.com/eopLnnE.png" alt=""></div><br>于是，每个参数的更新规则就变成为：<div align="center"><img src="http://i.imgur.com/XlxR2g8.png" alt=""></div><br>&emsp;&emsp;从上面公式可以清楚看到，参数的每一次更新（迭代）都要用到全量训练数据（下图红色框位置）<div align="center"><img src="http://i.imgur.com/txXGQRl.png" alt=""></div><br>&emsp;&emsp;这种更新方式，在迭代过程中，参数的方差是最小的，收敛的过程也是最稳定的，但是如果训练数据量$m$很大，批量梯度下降将会是一个非常耗时的过程。</p>
<h2 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h2><blockquote>
<p>随机梯度下降法的每次更新，是随机对一个样本求梯度并更新相应的参数。  </p>
</blockquote>
<p>&emsp;&emsp;从上面批量梯度下降法的求解过程可以看到，对于一个样本的损失函数，其对应参数的更新方式为：<div align="center"><img src="http://i.imgur.com/rdbGBMV.png" alt=""></div><br>&emsp;&emsp;这种做法在面对大数据集时不会出现冗余，能够进行快速的迭代。因为每次仅迭代一个样本，随机梯度下降法求解极值的过程，并不是总是向着整体最优的方向迭代的，参数的方差变化很大，收敛很不稳定，相比批量梯度下降会更加曲折，因此准确率相对于批量梯度下降法会有所下降。   </p>
<h2 id="小批量梯度下降法"><a href="#小批量梯度下降法" class="headerlink" title="小批量梯度下降法"></a>小批量梯度下降法</h2><blockquote>
<p>小批量梯度下降法每次更新参数，仅使用一小部分的训练数据进行迭代   </p>
</blockquote>
<p>&emsp;&emsp;使用批量梯度下降法准确率很高，但是效率低，随机梯度下降法效率高，但是准确率低，而小批量梯度下降法算是批量下降法和随机梯度下降法的一种折衷，其集合了前面两种下降方法的优势，具体操作过程如下：</p>
<p><div align="center"><img src="http://i.imgur.com/4PAFFYu.png" alt=""></div><br>&emsp;&emsp;这里的$n$一般会选一个很小的数，比如10或者100，这样的话，在迭代过程中，参数值的方差不至于太大，收敛的过程会更加稳定。</p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>&emsp;&emsp;在机器学习中，梯度下降法通常用于优化损失函数，优化的过程，是沿着梯度的负方向不断逼近极值。从第二点‘梯度下降算法’里的实例图可以看到，一开始算法的下降速度很快，但是在极值附近时，算法的收敛速度很慢，比如在第二点的例子，在第四次迭代时已经逼近极小值了，但是在第38次迭代才解出极小值。另外，步长$\alpha$的选取很关键，步长过程可能会达不到极值点，甚至有可能发散，步长过短会导致收敛速度很慢。<br>&emsp;&emsp;批量梯度下降法每次迭代都用到所有训练数据，准确度高同时复杂度也高；随机梯度下降法每次迭代随机选取一个数据样本进行更新，准确度不高，复杂度较低；小批量梯度下降法集合了前面两种下降方法的优势，训练复杂度较低，精确度也较高。<br>&emsp;&emsp;最后，不得不吐槽下，markdown编辑公式真的是非常非常非常蛋疼。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;李航在其《统计学习方法》中提到，机器学习的核心由三大块组成，分别为模型、策略、算法，其实说白了，机器学习就是一个模型 + 一个损失函数 + 一个最优化算法。损失函数是为了让模型能够更好地拟合或分类数据，最优化算法是用来优化损失函数的，而在机器学习中，最常用的最优化算法应该算梯度下降了，这篇文章就来讲讲梯度下降，探究下它是如何优化损失函数的。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="最优化" scheme="http://yoursite.com/tags/%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>感知机模型</title>
    <link href="http://yoursite.com/posts/2017/08/01/perceptron_model.html"/>
    <id>http://yoursite.com/posts/2017/08/01/perceptron_model.html</id>
    <published>2017-08-01T03:16:20.734Z</published>
    <updated>2017-08-03T01:29:36.189Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;刚从广发项目撤出来，未来几周应该都不会很忙，趁着闲暇时间整理一下机器学习方面的知识。<br>&emsp;&emsp;先从最简单的感知机模型说起。感知机是一种二分类的线性模型，其假设训练数据集是线性可分的，目标是找到一个能够将训练数据集的正样本和负样本完全正确分割开的分离超平面。其实，模型只要找到该分离超平面，学习目的就达到了，并不要求样本能被最大限度地划分。<br><a id="more"></a>   </p>
<h1 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h1><p>&emsp;&emsp;分类器能够通过输入的特征向量映射到给定类别中的一个，也就是所谓的物以聚类人以群分。如下图一，蓝色直线将A和B完全分割开，那么该直线$y=ax+b$就是一个分类器。根据分类器是否线性可分的性质，分类器有线性分类器(图一)和非线性分类器（图二图三），从类别上，有二元分类器（图一）和多元分类器（两个类别以上，图二图三），而本章要讨论的感知机模型，就是一个二分类线性模型。<div align="center"><img src="http://i.imgur.com/jH3FvKL.png" alt=""></div>   </p>
<h1 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h1><p>&emsp;&emsp;感知机模型很简单，为什么说它简单呢，因为该模型只用了一个线性方程$y = ax + b$ 以及 一个符号函数$sign(x)$（初中的知识是不是？）<br>&emsp;&emsp;下面来看下感知机模型的定义。<br>&emsp;&emsp;假设有$N$维的特征输入$X = \lbrace x_1,x_2,…,x_n \rbrace $，输出的类别有两类$ Y = \lbrace +1,-1\rbrace $，则由特征输入映射到类别输出的函数$$f(x) = sign(w*x+b)$$称为感知机模型，其中，$w$ 和$b$是我们要求解的模型的参数，这里的$w$其实是一个跟特征输入维度一样$N$维的权值向量（也称为权重），而参数$b$为偏置（在二维平面上，$b$就是截距）。<br>&emsp;&emsp;我们从公式上去解读下为什么说感知机模型是一个线性二分类模型。<br>&emsp;&emsp;先看符号函数里面的线性方程$$y=w*x+b$$显然，方程$ y = w*x +b $在二维平面上是一条直线，在三维空间上是一个平面，在四维甚至更高维空间上就是一个超平面。（哈哈哈，这个实在是画不出来）<div align="center"><img src="http://i.imgur.com/RtXJNgB.png" alt=""></div><br>也就是说，不管是在二维三维空间还是更高维空间，超平面都可以将空间一分为二，并且都可以表示成方程$ y = w*x +b $，所以感知机模型首先是一个线性模型。<br>&emsp;&emsp;再看符号函数，这个更简单，符号函数的定义如下：<br>$$<br>sign(x)=\begin{cases}<br>+1,\quad x\geq 0\\<br>-1, \quad x&lt;0<br>\end{cases}$$   </p>
<p>如果$x&gt;=0$则$f(x)=+1$，否则$f(x)=-1$。那么对于函数$f(x) = sign(w*x+b)$，如果输入样本$w*x+b&gt;=0$，那么该样本会被判为+1类，否则判为-1类。举个例子，回到上文图一，对于直线上方有$w*x+b&gt;0$，所以所有的A样本都会归为+1类，对于直线下方有$w*x+b&lt;0$，这样所有的B样本都会归为-1类。因此，感知机模型是一个线性二分类模型，其任务就是，找到这样的一个超平面，能够把两个不同的分类完全分割开。   </p>
<h1 id="学习策略"><a href="#学习策略" class="headerlink" title="学习策略"></a>学习策略</h1><p>&emsp;&emsp;原理清楚了，那么现在的问题是，给定一个包含正负类的训练样本，我们应该如何找出这样的一个超平面，使之能够将正样例和负样例点完全分割开，也就是应该如何确定模型的参数$w$和$b$ <div align="center"> <img src="http://i.imgur.com/cZbjVQP.png" alt=""> </div><br>&emsp;&emsp;如上图，如何找到一条直线，使之能够将蓝色实例和褐色实例分割开（这里只是举个例子哈，在实际构建模型过程中，涉及的数据动不动就成百上千万，你想拿支笔来，往两个样本点之间一画一条直线，这是妥妥的不行的，更何况实际处理的数据维度一般都有两位数以上）。直接求解好像无从下手，不妨先假设参数$w$和$b$已知，也就是说这个超平面$S$我们已经找到了，但是不知道这超平面对样例数据的分类效果怎样。如何去评估分类的效果呢，一个很自然的想法就是看有多少样例数据是误分类的，也就是将总误分类数作为模型的损失函数，但是这样的函数对于参数$w$和$b$来说不是连续可导的，难以优化。另一种方法就是可以通过衡量误分类点到超平面的总距离来评估效果。高中的时候我们就学过点到平面的距离的公式是这样子的。$$d=\frac{|A*x_0+B*y_0+C*z_0+D|}{\sqrt{A^2+B^2+C^2}}$$ 其中$(A,B,C)$是平面法向量，在这里是权值向量，上面距离公式也可以简写成$$d=\frac{1}{||w||}|wx_0+b|$$ 其中$w=(A,B,C)$， 这里||w||也称为$w$的L2范数。<br>依然以上文图一为例，对于任意一个样本点$(x_i,y_i)$:</p>
<ul>
<li>如果该样本点是位于直线上方，并且刚好属于类别A的话，那么该样本点被直线正确分类，此时有$w*x_i+b&gt;0,y_i=+1$，显然$-y_i*(w*x_i+b)&lt;0$   </li>
<li>如果该样本点位于直线下方，并且刚好属于类别B的的话，那么该样本点被直线正确分类，此时有$w*x_i+b&lt;0,y_i=-1$，显然$-y_i*(w*x_i+b)&lt;0$   </li>
<li>如果该样本点位于直线上方，并且刚好属于类别B的的话，那么该样本点被直线错误分类，此时有$w*x_i+b&gt;0,y_i=-1$，显然$-y_i*(w*x_i+b)&gt;0$   </li>
<li>如果该样本点位于直线下方，并且刚好属于类别A的的话，那么该样本点被直线错误分类，此时有$w*x_i+b<0,y_i=+1$，显然$-y_i*(w*x_i+b)>0$   </0,y_i=+1$，显然$-y_i*(w*x_i+b)></li>
</ul>
<p>&emsp;&emsp;所以，对于误分类的点$(x_i,y_i)$来说$-y_i*(w*x_i+b)&gt;0$成立。<br>&emsp;&emsp;我们可以用$-y_i*(w*x_i+b)$（相当于$|w*x_i+b|$）去衡量误分类点的效果，$w*x_i+b$值越大，说明该点离分离超平面越远，误分类效果越差（虽然分错了就是分错了）。如下图中的红色直线对于点A的分类效果明显比点B的要差   </p>
<p><div align="center"><img src="http://i.imgur.com/0vGucMO.png" alt=""> </div><br>有了单个误分类点离超平面的距离，那么对于所有的误分类点有：$$ -\frac{1}{||w||}\sum_i^m y_i(wx_i+b)$$ 其中$m$为误分类点数量。当不考虑$\frac{1}{||w||}$时（$\frac{1}{||w||}$恒大于0，去掉的话不影响评估效果），就得到感知机模型的损失函数$Loss  Function$（用来度量模型预测的好坏）：$$ L(w,b) =-\sum_i^m y_i(wx_i+b)$$既然误分类点离分离超平面越远，误分类效果越差，那么，误分类点离分离超平面越近，误分类效果越好，当$$-\sum_i^m y_i(wx_i+b) = 0$$时，样本没有被误分类，所有样本都被超平面恰当地分割开。所以只要找到$w$和$b$使得$L(w,b)$取最小值，此时超平面的分类效果是最好的，这个时候我们可以将参数$w$和$b$的求解转化为最小化函数$L(w,b)$ $$minL(w,b) = -\sum_i^m y_i(wx_i+b)$$   </p>
<h1 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h1><p>&emsp;&emsp;最优化方法有很多，比如牛顿下降法、拉格朗日乘子法、共轭梯度下降法、梯度下降法，这里使用最简单也最常用的一样方法：随机梯度下降法（梯度下降法的一种方法）来优化上文提到的损失函数$L(w,b)$ $$L(w,b) = -\sum_i^m y_i(wx_i+b)$$ 梯度其实就是求偏导，随机梯度下降其实就是一次随机选一个误分类点，使其函数值$L(w,b)$往着负梯度方向移动，不断逼近最小值的一个过程。函数$L(w,b)$关于参数$w$和$b$的梯度分别为：$$\nabla_w L(w,b)= -\sum_i^m y_i x_i $$ $$\nabla_b L(w,b)= -\sum_i^m y_i $$ 现在，我们随便找一个分割超平面，比如$w=0,b=0$（注意$w$是一个向量），然后随机选一个实例点，判断$-y_i*(w*x_i+b)$ 是否大于等于0，如果大于等于0则说明该点被误分类，这时对$(w,b)$进行如下更新（如果$-y_i*(w*x_i+b)$小于0则不更新$w$和$b$）：$$w \leftarrow w+\eta y_i x_i$$ $$b \leftarrow b+\eta y_i$$ 这里的$\eta$$(0&lt;=\eta&lt;=1)$为步长，也称为学习率，这样，通过不断选取误分类点，更新参数$(w,b)$，降低函数$L(w,b)$的值，直到$L(w,b)=0$时对应的$(w,b)$的取值就是要求解的值。<br>算法很简单，分成4步，也就是：   </p>
<ul>
<li>(1)选取初值$(w_0,b_0)$;   </li>
<li>(2)在训练数据集中随机选取一个数据$(x_i,y_i)$;   </li>
<li>(3)如果$-y_i*(w*x_i+b)&gt;=0$，则进行如下更新：$$w \leftarrow w+\eta y_i x_i$$ $$b \leftarrow b+\eta y_i$$   </li>
<li>(4)转至(2)，直至数据集中没有误分类的点或者达到指定的迭代次数。</li>
</ul>
<p>用python实现基于随机梯度下降的简单感知机模型。</p>
<pre><code class="python"><span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> random
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt
%matplotlib inline
<span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs
<span class="comment">#随机生成特征维度为2，分别以[-1,-1],[1,1]为中心，类别方差为0.4,0.5的两个类</span>
x,y = make_blobs(n_samples=<span class="number">100</span>,n_features=<span class="number">2</span>,centers=[[<span class="number">-1</span>,<span class="number">-1</span>],[<span class="number">1</span>,<span class="number">1</span>]],cluster_std=[<span class="number">0.4</span>,<span class="number">0.5</span>])
<span class="comment"># 将 0 替换成-1</span>
y[y==<span class="number">0</span>] = <span class="number">-1</span>
w = np.zeros(<span class="number">2</span>)<span class="comment">#初始权重赋值为0</span>
b = <span class="number">0</span> <span class="comment">#初始偏置为0</span>
k=<span class="number">200</span> <span class="comment">#最大迭代次数</span>
l_rate = <span class="number">0.5</span> <span class="comment">#学习率</span>
i=<span class="number">0</span>
<span class="keyword">while</span> i &lt;= k:
    i = i+<span class="number">1</span>
    <span class="comment">#生成随机数</span>
    random_num = random.randint(<span class="number">0</span>,<span class="number">99</span>)
    <span class="comment">#损失函数</span>
    <span class="keyword">if</span> sum(-y[random_num]*(w*x[random_num] + b)) &gt;= <span class="number">0</span>:
        <span class="comment">#梯度更新权重</span>
        w = w + l_rate*y[random_num]*x[random_num]
        <span class="comment">#梯度更新偏置</span>
        b = b + l_rate*y[random_num]
<span class="comment">#---------------------画图-------------------------</span>

x1 = <span class="number">-3.0</span>
y1 = -(b + w[<span class="number">0</span>] * x1) /w[<span class="number">1</span>]
x2 = <span class="number">3.0</span>
y2 = -(b + w[<span class="number">0</span>] * x2) / w[<span class="number">1</span>]
plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))  
plt.plot([x1,x2],[y1,y2],<span class="string">'r'</span>)
plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],marker=<span class="string">'o'</span>,c=y,s=<span class="number">50</span>)
plt.xlabel(<span class="string">'x1'</span>)
plt.ylabel(<span class="string">'x2'</span>)
plt.show()
</code></pre>
<p>&emsp;&emsp;因为这里的两个样本区分度太明显，所以经过几次迭代就已经收敛了。最终求得参数$w=[1.03972853 \  0.97360177] ,b=0$，根据参数$(w,b)$画出的直线如下图所示：<div align="center"><img src="http://i.imgur.com/WftOMnU.png" alt=""></div><br>&emsp;&emsp;选取不同的初始值$(w,b)$或者不同的误分类点，划分的超平面很有可能会不一样，这也就是为什么在线性可分数据集中，用感知机模型求出来的解有无穷多个。如下面直线$A,B,C$都可作为解。</p>
<p><div align="center"> <img src="http://i.imgur.com/ibURFh2.png" alt=""></div> </p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>&emsp;&emsp;感知机模型假设训练数据集是线性可分的，如果线性不可分，算法会一直震荡无法收敛，所以其无法处理一些复杂的数据，如在分类器中提到的图二和图三。考虑到实际业务的数据一般比较复杂，简单的感知机模型无法有效地处理如此复杂的数据，所以在建模中很少会使用该模型。但是感知机模型还是比较重要的，为什么这么说呢，因为只要稍微修改下模型的损失函数，感知机模型就可转变为广受欢迎的分类神器：支持向量机，而通过简单的堆叠可演变为神经网络模型。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;刚从广发项目撤出来，未来几周应该都不会很忙，趁着闲暇时间整理一下机器学习方面的知识。&lt;br&gt;&amp;emsp;&amp;emsp;先从最简单的感知机模型说起。感知机是一种二分类的线性模型，其假设训练数据集是线性可分的，目标是找到一个能够将训练数据集的正样本和负样本完全正确分割开的分离超平面。其实，模型只要找到该分离超平面，学习目的就达到了，并不要求样本能被最大限度地划分。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类模型" scheme="http://yoursite.com/tags/%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="监督学习" scheme="http://yoursite.com/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>TF-IDF原理及应用</title>
    <link href="http://yoursite.com/posts/2017/03/15/TF-IDF.html"/>
    <id>http://yoursite.com/posts/2017/03/15/TF-IDF.html</id>
    <published>2017-03-15T09:36:24.153Z</published>
    <updated>2017-10-13T03:35:06.514Z</updated>
    
    <content type="html"><![CDATA[<p>你说广州塔，我知道是在广州，你说黄果树瀑布，我知道是在贵州，你说布达拉宫，我知道是在拉萨，你说公交车，我都不知道你在说哪个城市的公交车。这就是<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>。<br><a id="more"></a></p>
<h1 id="概念及原理"><a href="#概念及原理" class="headerlink" title="概念及原理"></a>概念及原理</h1><p><a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>全称Term Frequency and Inverse Document Frequency，直译过来就是’词频-逆向文件频率’，’TF’是指某一个给定的词语在该文件中出现的频率，’IDF’是指总文件数除以包含该词的文件数，再取对数。<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>一般用来评估在一堆语料库或一堆文件集中，某个字词对于该语料库或该文件的重要程度。怎么理解呢，举个例子，假设现在手上有10篇文章，‘水果’这个词在某一篇文章出现的频率很高，但是在这10篇文章中的仅有2篇文章提到，那么‘水果’这个词的<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>会很高，如果10篇文章中有8篇提到‘水果’这个词，那么这个词的‘<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>’会相对偏低。主要思想就是，一个词越能将一篇文章与其他文章区分开来，那么这个词的权重越高。</p>
<h1 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h1><h2 id="TF计算："><a href="#TF计算：" class="headerlink" title="TF计算："></a>TF计算：</h2><p><img src="http://i.imgur.com/B1xMUhQ.png" alt=""> </p>
<p>（markdown编辑数学公式还不怎么熟，先用mathtype搞好再截图吧）比如上面的例子，’水果’，’硬盘’在文章1（共有10个词）中出现的次数分别为2次，4次，那么:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">TF(水果) = 2/10 = 0.2   </div><div class="line">TF(硬盘) = 4/10 = 0.4</div></pre></td></tr></table></figure></p>
<h2 id="IDF计算："><a href="#IDF计算：" class="headerlink" title="IDF计算："></a>IDF计算：</h2><p><img src="http://i.imgur.com/oTT2Zpz.png" alt=""></p>
<p>如果这10篇文章中，有2篇文章包含有’水果’这个词，有5篇包含’硬盘’这个词，那么：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">IDF(水果) = <span class="built_in">log</span>(10/2) = 1.6094   </div><div class="line">IDF(硬盘) = <span class="built_in">log</span>(10/5) = 0.6931</div></pre></td></tr></table></figure></p>
<h2 id="TF-IDF计算"><a href="#TF-IDF计算" class="headerlink" title="TF-IDF计算"></a>TF-IDF计算</h2><p>算好TF和IDF之后，就可以计算’水果’和’硬盘’的<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>了，只需要将TF和IDF相乘就ok。<br><img src="http://i.imgur.com/4jhJzZI.png" alt=""><br>所以’水果’的TF-IDF为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0.2*1.6094</div></pre></td></tr></table></figure></p>
<p>‘硬盘’的TF-IDF为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0.4*0.6931</div></pre></td></tr></table></figure></p>
<p>如果算’水果’和’硬盘’这两个词与文章1的相关性呢，很简单，只要将这两个词的<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>加起来。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0.2*1.6094 + 0.4*0.6931</div></pre></td></tr></table></figure></p>
<h1 id="python中计算TF-IDF"><a href="#python中计算TF-IDF" class="headerlink" title="python中计算TF-IDF"></a>python中计算TF-IDF</h1><h2 id="使用的工具"><a href="#使用的工具" class="headerlink" title="使用的工具"></a>使用的工具</h2><ul>
<li>jieba</li>
<li>scikit-learn</li>
</ul>
<h2 id="切词"><a href="#切词" class="headerlink" title="切词"></a>切词</h2><p>其实切词只是计算TF-IDF的前期准备工作，在对中文文本进行<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>计算的话，切词这一步应该是怎么也逃不过去了。平常工作中基本都是用jieba切词，这里也打算用jieba对文本进行处理。<br>例如我现在有5个文本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">content = [[<span class="string">'萨德系统核心装备X波段雷达'</span>],[<span class="string">'美韩当局部署萨德的步伐也在加速进行'</span>],[<span class="string">'纵观如今的手机处理器市场已经不是高通一家独大的局面'</span>],[<span class="string">'三星的Exynos处理器以及华为的海思麒麟芯片这些年风头正盛'</span>],[<span class="string">'魅族每年数以千万计的销量对于芯片厂商的贡献也是不可小看的'</span>]]</div></pre></td></tr></table></figure></p>
<p>首先需要对文本进行切词，切词代码及结果如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_words</span><span class="params">(text)</span>:</span></div><div class="line">    results = []</div><div class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> contents:</div><div class="line">        seg_list = jieba.cut(content[<span class="number">0</span>],cut_all=<span class="keyword">False</span>)</div><div class="line">        <span class="comment"># 实际应用过程中，这里需要去除停用词</span></div><div class="line">        seg = <span class="string">' '</span>.join(seg_list)</div><div class="line">        results.append(seg)</div><div class="line">    <span class="keyword">return</span> results</div><div class="line">result = cut_words(contents)</div><div class="line"></div><div class="line">result = [<span class="string">'萨德 系统核心 装备 X 波段 雷达'</span>, <span class="string">'美韩 当局 部署 萨德 的 步伐 也 在 加速 进行'</span>, <span class="string">'纵观 如今 的 手机 处理器 市场 已经 不是 高通 一家独大 的 局面'</span>, <span class="string">'三星 的 Exynos 处理器 以及 华为 的 海思 麒麟 芯片 这些 年 风头 正 盛'</span>, <span class="string">'魅族 每年 数以千万计 的 销量 对于 芯片 厂商 的 贡献 也 是 不可 小看 的'</span>]</div></pre></td></tr></table></figure></p>
<p>准备工作做好之后，我们就可以进行<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>计算了。</p>
<h2 id="词语转矩阵"><a href="#词语转矩阵" class="headerlink" title="词语转矩阵"></a>词语转矩阵</h2><p>词语转矩阵需要用到<a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" target="_blank" rel="external">CountVectorizer</a>这个函数，其作用是统计词汇的数量，并转为矩阵。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding:utf-8</span></div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</div><div class="line">vectorizer = CountVectorizer()</div><div class="line">vector_location = vectorizer.fit_transform(result)</div></pre></td></tr></table></figure></p>
<p>通过type(vector_location)可以看到，函数fit_transform把result二维数组表示成一个稀疏矩阵:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">print(type(vector_location))</div><div class="line"><span class="comment">#输出</span></div><div class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">scipy</span>.<span class="title">sparse</span>.<span class="title">csr</span>.<span class="title">csr_matrix</span>'&gt;</span></div></pre></td></tr></table></figure></p>
<p>同时可以看下，vercot_location的输出结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">print(vector_location)</div><div class="line"><span class="comment">#输出</span></div><div class="line"><span class="comment">#(0, 27)	1</span></div><div class="line"><span class="comment">#(0, 23)	1</span></div><div class="line"><span class="comment">#(0, 28)	1</span></div><div class="line"><span class="comment">#(0, 21)	1</span></div><div class="line"><span class="comment">#(0, 34)	1</span></div><div class="line"><span class="comment">#(1, 27)	1</span></div><div class="line"><span class="comment">#(1, 25)	1</span></div><div class="line"><span class="comment">#(1, 16)	1</span></div><div class="line"><span class="comment">#(1, 32)	1</span></div><div class="line"><span class="comment">#(1, 19)	1</span></div><div class="line"><span class="comment">#(1, 6)	    1</span></div><div class="line"><span class="comment">#(1, 31)	1</span></div><div class="line"><span class="comment">#(2, 24)	1</span></div><div class="line"><span class="comment">#(2, 10)	1</span></div><div class="line"><span class="comment">#(2, 17)	1</span></div><div class="line"><span class="comment">#(2, 9)	    1</span></div><div class="line"><span class="comment">#(2, 15)	1</span></div><div class="line"><span class="comment">#(2, 14)	1</span></div><div class="line"><span class="comment">#(2, 4)	    1</span></div><div class="line"><span class="comment">#(2, 36)	1</span></div><div class="line"><span class="comment">#(2, 1)	    1</span></div><div class="line"><span class="comment">#(2, 13)	1</span></div><div class="line"><span class="comment">#(3, 9)	    1</span></div><div class="line"><span class="comment">#(3, 2)	    1</span></div><div class="line"><span class="comment">#(3, 0)  	1</span></div><div class="line"><span class="comment">#(3, 5) 	1</span></div><div class="line"><span class="comment">#(3, 7) 	1</span></div><div class="line"><span class="comment">#(3, 22)	1</span></div><div class="line"><span class="comment">#(3, 38)	1</span></div><div class="line"><span class="comment">#(3, 26)	1</span></div><div class="line"><span class="comment">#(3, 30)	1</span></div><div class="line"><span class="comment">#(3, 35)	1</span></div><div class="line"><span class="comment">#(4, 26)	1</span></div><div class="line"><span class="comment">#(4, 37)	1</span></div><div class="line"><span class="comment">#(4, 20)	1</span></div><div class="line"><span class="comment">#(4, 18)	1</span></div><div class="line"><span class="comment">#(4, 33)	1</span></div><div class="line"><span class="comment">#(4, 11)	1</span></div><div class="line"><span class="comment">#(4, 8) 	1</span></div><div class="line"><span class="comment">#(4, 29)	1</span></div><div class="line"><span class="comment">#(4, 3)    	1</span></div><div class="line"><span class="comment">#(4, 12)	1</span></div></pre></td></tr></table></figure></p>
<p>输出结果表示的是这个稀疏矩阵的第几行第几列有值，比如(0, 27)    1表示矩阵的第0行第27列有值。<br>转成矩阵的形式之后，我们就可以很容易地算出每个词对应的<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>了，这里使用<a href="http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html" target="_blank" rel="external">TfidfTransformer</a>函数进行计算。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</div><div class="line">transformer = TfidfTransformer()</div><div class="line">tf_idf = transformer.fit_transform(vector_location)</div><div class="line">print(type(tf_idf))</div><div class="line"><span class="comment">#输出，同样是稀疏矩阵的形式</span></div><div class="line"><span class="comment">#&lt;class 'scipy.sparse.csr.csr_matrix'&gt;</span></div><div class="line">print(tf_idf)</div><div class="line"><span class="comment">#输出</span></div><div class="line"><span class="comment">#(0, 34)	0.463693222732</span></div><div class="line"><span class="comment">#(0, 21)	0.463693222732</span></div><div class="line"><span class="comment">#(0, 28)	0.463693222732</span></div><div class="line"><span class="comment">#(0, 23)	0.463693222732</span></div><div class="line"><span class="comment">#(0, 27)	0.37410477245</span></div><div class="line"><span class="comment">#(1, 31)	0.387756660106</span></div><div class="line"><span class="comment">#(1, 6) 	0.387756660106</span></div><div class="line"><span class="comment">#(1, 19)	0.387756660106</span></div><div class="line"><span class="comment">#(1, 32)	0.387756660106</span></div><div class="line"><span class="comment">#(1, 16)	0.387756660106</span></div><div class="line"><span class="comment">#(1, 25)	0.387756660106</span></div><div class="line"><span class="comment">#(1, 27)	0.312839631859</span></div><div class="line"><span class="comment">#(2, 13)	0.321896111462</span></div><div class="line"><span class="comment">#(2, 1) 	0.321896111462</span></div><div class="line"><span class="comment">#(2, 36)	0.321896111462</span></div><div class="line"><span class="comment">#(2, 4) 	0.321896111462</span></div><div class="line"><span class="comment">#(2, 14)	0.321896111462</span></div><div class="line"><span class="comment">#(2, 15)	0.321896111462</span></div><div class="line"><span class="comment">#(2, 9) 	0.259703755905</span></div><div class="line"><span class="comment">#(2, 17)	0.321896111462</span></div><div class="line"><span class="comment">#(2, 10)	0.321896111462</span></div><div class="line"><span class="comment">#(2, 24)	0.321896111462</span></div><div class="line"><span class="comment">#(3, 35)	0.327880622184</span></div><div class="line"><span class="comment">#(3, 30)	0.327880622184</span></div><div class="line"><span class="comment">#(3, 26)	0.264532021474</span></div><div class="line"><span class="comment">#(3, 38)	0.327880622184</span></div><div class="line"><span class="comment">#(3, 22)	0.327880622184</span></div><div class="line"><span class="comment">#(3, 7) 	0.327880622184</span></div><div class="line"><span class="comment">#(3, 5) 	0.327880622184</span></div><div class="line"><span class="comment">#(3, 0) 	0.327880622184</span></div><div class="line"><span class="comment">#(3, 2) 	0.327880622184</span></div><div class="line"><span class="comment">#(3, 9) 	0.264532021474</span></div><div class="line"><span class="comment">#(4, 12)	0.321896111462</span></div><div class="line"><span class="comment">#(4, 3) 	0.321896111462</span></div><div class="line"><span class="comment">#(4, 29)	0.321896111462</span></div><div class="line"><span class="comment">#(4, 8) 	0.321896111462</span></div><div class="line"><span class="comment">#(4, 11)	0.321896111462</span></div><div class="line"><span class="comment">#(4, 33)	0.321896111462</span></div><div class="line"><span class="comment">#(4, 18)	0.321896111462</span></div><div class="line"><span class="comment">#(4, 20)	0.321896111462</span></div><div class="line"><span class="comment">#(4, 37)	0.321896111462</span></div><div class="line"><span class="comment">#(4, 26)	0.259703755905</span></div></pre></td></tr></table></figure></p>
<p>如果需要把稀疏矩阵转成平常用的行列形式的矩阵的话。这里可以使用todense()或者toarray()函数，前者是将稀疏矩阵转成matrix的形式，后者是将稀疏矩阵转成ndarray的形式<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">weight = tf_idf.toarray()</div><div class="line"><span class="comment">#or</span></div><div class="line">weight1 = tf_idf.todense()</div><div class="line"></div><div class="line">print(weight)</div><div class="line"><span class="comment">#输出</span></div><div class="line"><span class="comment">#(5,39)</span></div></pre></td></tr></table></figure></p>
<p>这里还有一个问题，就是我怎么知道每个权重对应的是哪个词呢？这里可以将词作为列名，将数组转成Dataframe进行查看。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">word=vectorizer.get_feature_names()</div><div class="line">df = pd.DataFrame(weight)</div><div class="line">df.columns = word</div><div class="line">print(df)</div></pre></td></tr></table></figure></p>
<h1 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h1><p>最后照例附上本次分析的源代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding:utf-8</span></div><div class="line"><span class="comment">#author:linchart</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> jieba</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</div><div class="line">contents = [[<span class="string">'萨德系统核心装备X波段雷达'</span>],\</div><div class="line">            [<span class="string">'美韩当局部署萨德的步伐也在加速进行'</span>],\</div><div class="line">            [<span class="string">'纵观如今的手机处理器市场已经不是高通一家独大的局面'</span>],\</div><div class="line">            [<span class="string">'三星的Exynos处理器以及华为的海思麒麟芯片这些年风头正盛'</span>],\</div><div class="line">            [<span class="string">'魅族每年数以千万计的销量对于芯片厂商的贡献也是不可小看的'</span>]]</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_words</span><span class="params">(text)</span>:</span></div><div class="line">    results = []</div><div class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> contents:</div><div class="line">        seg_list = jieba.cut(content[<span class="number">0</span>],cut_all=<span class="keyword">False</span>)</div><div class="line">        <span class="comment"># 实际应用过程中，这里需要去除停用词</span></div><div class="line">        seg = <span class="string">' '</span>.join(seg_list)</div><div class="line">        results.append(seg)</div><div class="line">    <span class="keyword">return</span> results</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_idf</span><span class="params">(words)</span>:</span></div><div class="line">    vectorizer = CountVectorizer()</div><div class="line">    vector_location = vectorizer.fit_transform(result)</div><div class="line">    transformer = TfidfTransformer()</div><div class="line">    tf_idf = transformer.fit_transform(vector_location)</div><div class="line">    weight = tf_idf.toarray()</div><div class="line">    word = vectorizer.get_feature_names()</div><div class="line">    df = pd.DataFrame(weight)</div><div class="line">    df.columns = word</div><div class="line">    <span class="keyword">return</span> df</div><div class="line"></div><div class="line">result = cut_words(contents)</div><div class="line">df = tf_idf(result)</div><div class="line">print(df)</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;你说广州塔，我知道是在广州，你说黄果树瀑布，我知道是在贵州，你说布达拉宫，我知道是在拉萨，你说公交车，我都不知道你在说哪个城市的公交车。这就是&lt;a href=&quot;https://zh.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt;。&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>爬取微信文章</title>
    <link href="http://yoursite.com/posts/2017/03/13/WeChat_Article1.html"/>
    <id>http://yoursite.com/posts/2017/03/13/WeChat_Article1.html</id>
    <published>2017-03-13T09:59:18.349Z</published>
    <updated>2017-08-10T14:23:05.156Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;有时在微信公众号上面看到一些写的比较好的文章，但又没有时间细看，闲下来想找这些文章的时候又忘了是在哪个公众号看的了、文章名字也想不起来，因此想搞个爬虫把想看的文章爬下来，一来可以在闲时咀嚼一下，二来也可以收藏一些好文章，做些知识积累。<br><em>只是想把自己平常做的一些东西记录下来，非教程</em><br><a id="more"></a>   </p>
<h1 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h1><ul>
<li>Python 3.5.1   </li>
</ul>
<h1 id="使用的库"><a href="#使用的库" class="headerlink" title="使用的库"></a>使用的库</h1><ul>
<li>re</li>
<li>pdfkit</li>
<li>requests</li>
<li>BeautifulSoup   </li>
</ul>
<h1 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h1><p>输入微信文章名称或者对应的文章链接，输出文章的pdf文件。</p>
<h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><ul>
<li>如果同时提供文章链接和文章名称，则优先通过文章链接爬取，如果文章链接爬取失败，则通过文章名称爬取；   </li>
<li>如果仅提供文章链接，则通过文章链接爬取；</li>
<li>如果仅提供文章名称，则通过搜狗微信接口搜索微信文章，找到对应文章链接，然后在通过文章链接爬取。</li>
</ul>
<h1 id="爬取流程"><a href="#爬取流程" class="headerlink" title="爬取流程"></a>爬取流程</h1><h2 id="获取文章链接"><a href="#获取文章链接" class="headerlink" title="获取文章链接"></a>获取文章链接</h2><p>将提供的文章名称传入<a href="http://weixin.sogou.com/weixin" target="_blank" rel="external">搜狗微信搜索引擎搜索</a>，将结果列表中的第一篇文章作为目标文章下载。下面代码返回目标文章链接。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article_link</span><span class="params">(query)</span>:</span></div><div class="line">    base_url = <span class="string">r'http://weixin.sogou.com/weixin'</span></div><div class="line">    User_Agent = <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span></div><div class="line">    Host = <span class="string">'weixin.sogou.com'</span></div><div class="line">    Connection = <span class="string">'keep-alive'</span></div><div class="line">    headers = &#123;<span class="string">'User-Agent'</span>: User_Agent, <span class="string">'Host'</span>: Host, <span class="string">'Connection'</span>: Connection&#125;</div><div class="line">    params = &#123;<span class="string">'type'</span>: <span class="number">2</span>, <span class="string">'ie'</span>: <span class="string">'utf-8'</span>, <span class="string">'w'</span>: <span class="string">'01019900'</span>, <span class="string">'sut'</span>: <span class="string">'707'</span>,<span class="string">'query'</span>:query&#125;</div><div class="line">    request = requests.get(base_url, headers=headers, params=params)</div><div class="line">    request.encoding = <span class="string">'utf-8'</span></div><div class="line">    bsobj = BeautifulSoup(request.text, <span class="string">'lxml'</span>)</div><div class="line">    <span class="comment"># 仅提取列表中的第一篇文章</span></div><div class="line">    first_article_link = bsobj.select(<span class="string">'#sogou_vr_11002601_title_0'</span>)[<span class="number">0</span>][<span class="string">'href'</span>]</div><div class="line">    <span class="keyword">return</span> first_article_link</div></pre></td></tr></table></figure>
<h2 id="将文章转为html"><a href="#将文章转为html" class="headerlink" title="将文章转为html"></a>将文章转为html</h2><p>解析文章链接，将文章内容保存为html文件。这里需要注意的是，在解析文章的时候，如果文章中包含有图片的话，正常情况下是无法下载下来的，因为爬取的文章链接为临时链接，非永久链接，无法直接解析src里面的链接。但是，data-src这个属性的值还是可以解析出来的，所以只要把data-src替换为src就可以下载图片了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article_html</span><span class="params">(link)</span>:</span></div><div class="line">    <span class="comment"># 为了保险起见，这里使用不同的headers</span></div><div class="line">    User_Agent = <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/3.0 Safari/536.11'</span></div><div class="line">    article_headers = &#123;<span class="string">'User_Agent'</span>: User_Agent&#125;</div><div class="line">    article_obj = requests.get(link, headers=article_headers)</div><div class="line">    article_obj.encoding = <span class="string">'utf-8'</span></div><div class="line">    soup = BeautifulSoup(article_obj.content, <span class="string">'html5lib'</span>)</div><div class="line">    <span class="comment"># 以实际的文章名称为准</span></div><div class="line">    article_name = soup.select(<span class="string">'#activity-name'</span>)[<span class="number">0</span>].text.strip()</div><div class="line">    content = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'id'</span>: <span class="string">'page-content'</span>&#125;)</div><div class="line">    html = str(content)</div><div class="line">    <span class="comment"># 把属性data-src替换成src,前面无法将属性src解析出来，data-src，只是LAZY用的，</span></div><div class="line">    <span class="comment"># 延迟加载图片所以显示不出来，LAZYLOAD</span></div><div class="line">    src_compile = re.compile(<span class="string">'data-src'</span>)</div><div class="line">    html_new = re.sub(src_compile, <span class="string">'src'</span>, html)</div><div class="line">    <span class="comment"># 存储成html</span></div><div class="line">    <span class="keyword">with</span> open(<span class="string">'wechat_article.html'</span>, <span class="string">'w'</span>, encoding=<span class="string">'GB18030'</span>) <span class="keyword">as</span> f:</div><div class="line">        f.write(html_new)</div><div class="line">    <span class="keyword">return</span> article_name</div></pre></td></tr></table></figure>
<h2 id="html转pdf"><a href="#html转pdf" class="headerlink" title="html转pdf"></a>html转pdf</h2><p>html文件转pdf调用了pdfkit这个包，使用这个包需要安装<a href="http://wkhtmltopdf.org/downloads.html" target="_blank" rel="external">wkhtmltopdf</a>软件（pdfkit依赖于wkhtmltopdf，因此需要配置路径）。<br>在运行过程中，发现pdfkit在html转pdf时，生成的pdf文件名中如果包含有| / *这些特殊符号时会报错，因此如果以原文章名对pdf命名失败时，仅保留文章名的汉字、字母和数字进行命名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_to_pdf</span><span class="params">(query_article)</span>:</span></div><div class="line">    path_wk = <span class="string">r'D:\Program Files\wkhtmltopdf\bin\wkhtmltopdf.exe'</span></div><div class="line">    config = pdfkit.configuration(wkhtmltopdf=path_wk)</div><div class="line">    options = &#123;</div><div class="line">        <span class="string">'page-size'</span>: <span class="string">'Letter'</span>,</div><div class="line">        <span class="string">'encoding'</span>: <span class="string">"GB18030"</span>,</div><div class="line">        <span class="string">'custom-header'</span>: [</div><div class="line">            (<span class="string">'Accept-Encoding'</span>, <span class="string">'gzip'</span>)</div><div class="line">        ]</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">try</span> :</div><div class="line">        pdfkit.from_file(<span class="string">'wechat_article.html'</span>, <span class="string">'%s.pdf'</span> % query_article, configuration=config, options=options)</div><div class="line">    <span class="keyword">except</span>:</div><div class="line">        name_compile = re.compile(<span class="string">'[a-zA-Z\u4e00-\u9fa5][a-zA-Z0-9\u4e00-\u9fa5]+'</span>)</div><div class="line">        pdf_name = re.findall(name_compile,query_article)[<span class="number">0</span>]</div><div class="line">        pdfkit.from_file(<span class="string">'wechat_article.html'</span>, <span class="string">'%s.pdf'</span> % pdf_name, configuration=config, options=options)</div><div class="line">        print(<span class="string">'文件名已被修改为:%s'</span> %pdf_name)</div></pre></td></tr></table></figure>
<h1 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h1><p>最后附上文章爬取的完整代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding:utf-8</span></div><div class="line"><span class="comment">#author:linchart</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> pdfkit</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article_link</span><span class="params">(query)</span>:</span></div><div class="line">    base_url = <span class="string">r'http://weixin.sogou.com/weixin'</span></div><div class="line">    User_Agent = <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span></div><div class="line">    Host = <span class="string">'weixin.sogou.com'</span></div><div class="line">    Connection = <span class="string">'keep-alive'</span></div><div class="line">    headers = &#123;<span class="string">'User-Agent'</span>: User_Agent, <span class="string">'Host'</span>: Host, <span class="string">'Connection'</span>: Connection&#125;</div><div class="line">    params = &#123;<span class="string">'type'</span>: <span class="number">2</span>, <span class="string">'ie'</span>: <span class="string">'utf-8'</span>, <span class="string">'w'</span>: <span class="string">'01019900'</span>, <span class="string">'sut'</span>: <span class="string">'707'</span>,<span class="string">'query'</span>:query&#125;</div><div class="line">    request = requests.get(base_url, headers=headers, params=params)</div><div class="line">    request.encoding = <span class="string">'utf-8'</span></div><div class="line">    bsobj = BeautifulSoup(request.text, <span class="string">'lxml'</span>)</div><div class="line">    <span class="comment"># 仅提取列表中的第一篇文章</span></div><div class="line">    first_article_link = bsobj.select(<span class="string">'#sogou_vr_11002601_title_0'</span>)[<span class="number">0</span>][<span class="string">'href'</span>]</div><div class="line">    <span class="keyword">return</span> first_article_link</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article_html</span><span class="params">(link)</span>:</span></div><div class="line">    <span class="comment"># 需要不同的headers</span></div><div class="line">    User_Agent = <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/3.0 Safari/536.11'</span></div><div class="line">    article_headers = &#123;<span class="string">'User_Agent'</span>: User_Agent&#125;</div><div class="line">    article_obj = requests.get(link, headers=article_headers)</div><div class="line">    article_obj.encoding = <span class="string">'utf-8'</span></div><div class="line">    soup = BeautifulSoup(article_obj.content, <span class="string">'html5lib'</span>)</div><div class="line">    <span class="comment"># 以实际的文章名称为准</span></div><div class="line">    article_name = soup.select(<span class="string">'#activity-name'</span>)[<span class="number">0</span>].text.strip()</div><div class="line">    content = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'id'</span>: <span class="string">'page-content'</span>&#125;)</div><div class="line">    html = str(content)</div><div class="line">    <span class="comment"># 把属性data-src替换成src,前面无法将属性src解析出来，data-src，只是LAZY用的，</span></div><div class="line">    <span class="comment"># 延迟加载图片所以显示不出来，LAZYLOAD</span></div><div class="line">    src_compile = re.compile(<span class="string">'data-src'</span>)</div><div class="line">    html_new = re.sub(src_compile, <span class="string">'src'</span>, html)</div><div class="line">    <span class="comment"># 存储成html</span></div><div class="line">    <span class="keyword">with</span> open(<span class="string">'wechat_article.html'</span>, <span class="string">'w'</span>, encoding=<span class="string">'GB18030'</span>) <span class="keyword">as</span> f:</div><div class="line">        f.write(html_new)</div><div class="line">    <span class="keyword">return</span> article_name</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_to_pdf</span><span class="params">(query_article)</span>:</span></div><div class="line">    path_wk = <span class="string">r'D:\Program Files\wkhtmltopdf\bin\wkhtmltopdf.exe'</span></div><div class="line">    config = pdfkit.configuration(wkhtmltopdf=path_wk)</div><div class="line">    options = &#123;</div><div class="line">        <span class="string">'page-size'</span>: <span class="string">'Letter'</span>,</div><div class="line">        <span class="string">'encoding'</span>: <span class="string">"GB18030"</span>,</div><div class="line">        <span class="string">'custom-header'</span>: [</div><div class="line">            (<span class="string">'Accept-Encoding'</span>, <span class="string">'gzip'</span>)</div><div class="line">        ]</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">try</span> :</div><div class="line">        pdfkit.from_file(<span class="string">'wechat_article.html'</span>, <span class="string">'%s.pdf'</span> % query_article, configuration=config, options=options)</div><div class="line">    <span class="keyword">except</span>:</div><div class="line">        name_compile = re.compile(<span class="string">'[a-zA-Z\u4e00-\u9fa5][a-zA-Z0-9\u4e00-\u9fa5]+'</span>)</div><div class="line">        pdf_name = re.findall(name_compile,query_article)[<span class="number">0</span>]</div><div class="line">        pdfkit.from_file(<span class="string">'wechat_article.html'</span>, <span class="string">'%s.pdf'</span> % pdf_name, configuration=config, options=options)</div><div class="line">        print(<span class="string">'文件名已被修改为:%s'</span> %pdf_name)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">wechat_article</span><span class="params">(query=None,link=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> link :</div><div class="line">        <span class="keyword">try</span> :</div><div class="line">            article_name = get_article_html(link)</div><div class="line">            html_to_pdf(article_name)</div><div class="line">            print(<span class="string">'文章下载成功'</span>)</div><div class="line">        <span class="keyword">except</span> :</div><div class="line">            article_link = get_article_link(query)</div><div class="line">            get_article_html(article_link)</div><div class="line">            html_to_pdf(query)</div><div class="line">            print(<span class="string">'文章下载成功'</span>)</div><div class="line">    <span class="keyword">else</span> :</div><div class="line">        article_link = get_article_link(query)</div><div class="line">        get_article_html(article_link)</div><div class="line">        html_to_pdf(query)</div><div class="line">        print(<span class="string">'文章下载成功'</span>)</div><div class="line"></div><div class="line"><span class="comment"># PDF可以用中文命名，但是命名中不可以包含* \/|等特殊字符。</span></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    link = <span class="keyword">None</span></div><div class="line">    query = <span class="string">'文本分析|词频与余弦相似度'</span></div><div class="line">    wechat_article(query=query,link=link)</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;有时在微信公众号上面看到一些写的比较好的文章，但又没有时间细看，闲下来想找这些文章的时候又忘了是在哪个公众号看的了、文章名字也想不起来，因此想搞个爬虫把想看的文章爬下来，一来可以在闲时咀嚼一下，二来也可以收藏一些好文章，做些知识积累。&lt;br&gt;&lt;em&gt;只是想把自己平常做的一些东西记录下来，非教程&lt;/em&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>博客的搭建</title>
    <link href="http://yoursite.com/posts/2017/03/05/blog.html"/>
    <id>http://yoursite.com/posts/2017/03/05/blog.html</id>
    <published>2017-03-04T18:08:32.627Z</published>
    <updated>2017-03-14T07:58:02.019Z</updated>
    
    <content type="html"><![CDATA[<h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p>这个博客的搭建，完全得益于<a href="https://zhangslob.github.io/2017/02/28/%E6%95%99%E4%BD%A0%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%8CHexo-Github/" target="_blank" rel="external">教你免费搭建个人博客，Hexo&amp;Github</a>和<a href="http://www.jianshu.com/p/1cd86fac2585" target="_blank" rel="external">使用GitHub和Hexo搭建免费静态Blog</a>这两篇文章，非常详细地描述了基于hexo+github搭建个人博客的准备工作及安装和配置流程。<br><a id="more"></a></p>
<h1 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h1><p>虽然上面这两篇文章写得很详细，但是我在按着教程搭建的过程中还是遇到一些小问题，这里记录一下：   </p>
<h2 id="1、注意运行路径"><a href="#1、注意运行路径" class="headerlink" title="1、注意运行路径"></a>1、注意运行路径</h2><p>在浏览器中查看自带的hello world文章，需要执行 hexo generate 和hexo server两个命令，这里要注意一下这两个命令的执行路径，需要在hello world文章路径下执行。   </p>
<h2 id="2、MarkdownPad无法预览"><a href="#2、MarkdownPad无法预览" class="headerlink" title="2、MarkdownPad无法预览"></a>2、MarkdownPad无法预览</h2><p>win10下首次安装MarkdownPad会出现右侧浏览页面无法浏览的情况，<br><img src="http://i.imgur.com/O9hUr8v.png" alt=""><br>这种情况下需要安装<a href="http://markdownpad.com/download/awesomium_v1.6.6_sdk_win.exe" target="_blank" rel="external">Awesomium 1.6.6 SDK</a>，安装完成之后问题可解决。<br><img src="http://i.imgur.com/oous83g.png" alt=""></p>
<h2 id="3、yilia主题头像无法显示"><a href="#3、yilia主题头像无法显示" class="headerlink" title="3、yilia主题头像无法显示"></a>3、yilia主题头像无法显示</h2><p>若加载头像后，头像无法显示，需要将’themes/yilia/layout/_partial’路径下的文件left-col.ejs中的第6行修改为：</p>
<pre><code class="bash">&lt;img src=<span class="string">"&lt;%=theme.avatar%&gt;"</span> class=<span class="string">"js-avatar show"</span>&gt;
</code></pre>
<h2 id="4、部署上传"><a href="#4、部署上传" class="headerlink" title="4、部署上传"></a>4、部署上传</h2><p>部署上传时执行以下命令时  </p>
<pre><code class="bash">hexo d
</code></pre>
<p>报’ERROR Deployer not found: git’错误，可能是deployer-git插件未安装，在根目录下执行下面代码安装该插件即可。</p>
<pre><code class="bash">npm install hexo-deployer-git --save
</code></pre>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;参考文档&quot;&gt;&lt;a href=&quot;#参考文档&quot; class=&quot;headerlink&quot; title=&quot;参考文档&quot;&gt;&lt;/a&gt;参考文档&lt;/h1&gt;&lt;p&gt;这个博客的搭建，完全得益于&lt;a href=&quot;https://zhangslob.github.io/2017/02/28/%E6%95%99%E4%BD%A0%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%8CHexo-Github/&quot;&gt;教你免费搭建个人博客，Hexo&amp;amp;Github&lt;/a&gt;和&lt;a href=&quot;http://www.jianshu.com/p/1cd86fac2585&quot;&gt;使用GitHub和Hexo搭建免费静态Blog&lt;/a&gt;这两篇文章，非常详细地描述了基于hexo+github搭建个人博客的准备工作及安装和配置流程。&lt;br&gt;
    
    </summary>
    
      <category term="其他" scheme="http://yoursite.com/categories/%E5%85%B6%E4%BB%96/"/>
    
    
  </entry>
  
</feed>
