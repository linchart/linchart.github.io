<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>linchart</title>
  <subtitle>I&#39;m on the way to the future, where you are there.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-08-10T09:39:11.797Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>山久丰</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>梯度下降</title>
    <link href="http://yoursite.com/posts/2017/08/07/Gradient_descent.html"/>
    <id>http://yoursite.com/posts/2017/08/07/Gradient_descent.html</id>
    <published>2017-08-07T08:19:01.063Z</published>
    <updated>2017-08-10T09:39:11.797Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;李航在其《统计学习方法》中提到，机器学习的核心由三大块组成，分别为模型、策略、算法，其实说白了，机器学习就是一个模型 + 一个损失函数 + 一个最优化算法。损失函数是为了让模型能够更好地拟合或分类数据，最优化算法是用来优化损失函数的，而在机器学习中，最常用的最优化算法应该算梯度下降了，这篇文章就来讲讲梯度下降，探究下它是如何优化损失函数的。<br><a id="more"></a>   </p>
<h1 id="梯度下降原理"><a href="#梯度下降原理" class="headerlink" title="梯度下降原理"></a>梯度下降原理</h1><p>&emsp;&emsp;上文的感知机模型中有提到，梯度，其实就是求偏导，梯度下降就是沿着梯度的负方向移动。为了更直观地理解梯度下降，这里举一个网上的例子（实在想不出比这个更形象的解析了），比如我们现在站在山上的某个位置，如下图中的点A，目标是要最快速地下到山脚下，怎么一步一步走下去呢？我们可以选择往相对当前位置来说最陡峭的地方向下移动（也就是梯度的负方向），这样一直走到山脚。这个山脚有可能是整个大山的最低处（B点），也有可能是大山的局部低处（C或D点），这取决于我们移动的方向和幅度。这种情况下，梯度下降不一定能够找到全局最优解，有可能是一个局部最优解。  </p>
<div align="center"> <img src="http://i.imgur.com/dBpYr7I.png" alt=""> </div><br>但如果是下面这个凸函数图，我们就一定可以移动到最低处，因为不管我们处于山上的哪一个位置，只要我们保持每次移动都是向下的，就一定能到达最低点。这种情况下，梯度下降法得到的解就一定是全局最优解。<br><div align="center"><img src="http://i.imgur.com/fjgYW4z.png" alt=""></div>   

<h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><p>梯度下降算法如下（该算法摘抄自李航《统计学习方法》）：<br>输入：目标函数$f(x)$，梯度函数$g(x) = \nabla f(x)$，计算精度$\epsilon $<br>输出：$f(x)$的极小点$x^*$<br>&emsp;&emsp;(1) 取初始值$x^{(0)} \in R^n$，置$k=0$<br>&emsp;&emsp;(2) 计算$f(x^{(k)})$<br>&emsp;&emsp;(3) 计算梯度$g_k = g(x^{(k)})$，当$||g_k||&lt;\epsilon$时，停止迭代，令$x^* = x^{(k)}$；否则，令$p_k = -g(x^{(k)})$，求$\lambda_k　$使得$$f(x^{(k)}+ \lambda_k p_k) = min f(x^{(k)} + \lambda p_k) $$<br>&emsp;&emsp;(4) 置$x^{(k+1)} = x^{(k)} + \lambda_k p_k $，计算$f(x^{(k+1)})$，当$||f(x^{(k+1)}) - f(x^{(k)})|| &lt; \epsilon$ 或 $x^{(k+1)} - x^{(k)} &lt; \epsilon$时，停止迭代，令$x^* = x^{(k+1)}$<br>&emsp;&emsp;(5) 否则，令$k=k+1$，转$(3)$   </p>
<p>&emsp;&emsp;这里的$p_k$是搜索方向，$\lambda_k$是步长。假设有一个损失函数$y = x^2 - 4*x - 5 $，目标是找到$x$的值使得$y$取最小值，使用梯度下降求解如下：    </p>
<ul>
<li>梯度<br>$$\nabla x = -(2*x - 4)$$  </li>
<li>更新$x$的值<br>$$x \leftarrow x + \eta \nabla x $$   </li>
</ul>
<p>设$x$的初始值为10，学习速率$\eta$为0.2，则<br>第一次迭代：梯度$$\nabla x =-(2*10 - 4) = -16$$<br>&emsp;&emsp;$x$的值相应更新为$$ x = 10 - 0.2 * 16 = 6.8$$<br>&emsp;&emsp;对应的$y$值为$$y = 6.8*6.8 - 4*6.8 - 5 = 14.04$$<br>第二次迭代：梯度$$\nabla x =-(2*6.8 - 4) = -9.6$$<br>&emsp;&emsp;$x$的值相应更新为$$ x = 6.8 + 0.2 * （-9.6） = 4.88$$<br>&emsp;&emsp;对应的$y$值为$$y = 4.88*4.88 - 4*4.88 - 5 = -0.7056$$<br>&emsp;&emsp;如此循环迭代，直到达到指定迭代次数或者$y$值最优。这里一直迭代到第38次时，求得极小值$ y =-9.0$。因此，梯度下降就是一个不断沿着梯度的负方向移动直到达到局部或全局最优点的一个过程。</p>
<div align="center"><img src="http://i.imgur.com/WLUBgKm.png" alt=""></div>

<h1 id="梯度下降的三种形式"><a href="#梯度下降的三种形式" class="headerlink" title="梯度下降的三种形式"></a>梯度下降的三种形式</h1><p>梯度下降算法可以分为以下三种：</p>
<ul>
<li>批量梯度下降法(Batch Gradient Descent)   </li>
<li>随机梯度下降法(Stochastic Gradient Descent)   </li>
<li>小批量梯度下降法(Mini-batch Gradient Descent)   </li>
</ul>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>&emsp;&emsp;为了更好地阐明这三种方式的本质与区别，这里以线性回归模型为例。对于任意一个线性方程，我们可以写成以下形式：$$h_\theta (x)= \theta_0 + \theta_1 x_1 + \theta_2 x_2 + …+\theta_n x_n$$ 这里的$(\theta_0,\theta_1,…,\theta_n)$为参数，也称为权重。假设现在有一堆数据$X=(x^0,x^1,x^2,…,x^m)$以及对应的$y$值$Y=(y^0,y^1,y^2,…,y^m)$。我们的目标是要找到参数$(\theta_0,\theta_1,…,\theta_n)$使得$y$值与theta（符号theta在这里硬是显示不出来，我也是醉了）值尽可能地接近。这里用h_theta(x)与y的平方误差作为损失函数来评估h_theta(x)与y值的接近程度。<div align="center"><img src="http://i.imgur.com/Gge2YPo.png" alt=""></div><br>有了损失函数之后，我们就可以来聊一聊这三种梯度下降的形式了。   </p>
<h2 id="批量梯度下降法"><a href="#批量梯度下降法" class="headerlink" title="批量梯度下降法"></a>批量梯度下降法</h2><blockquote>
<p>批量梯度下降法每次迭代都需要用到全量的训练数据，优化过程比较耗时。   </p>
</blockquote>
<p>&emsp;&emsp;前面我们已经知道了梯度下降其实就是对参数求偏导，然后沿着梯度的负方向去更新参数。对于上面的损失函数$J(\theta)$，我们随机初始化权重$\theta$，然后重复执行以下更新：$$\theta_j:=\theta_j + (-\alpha \frac{\partial}{\partial\theta_j}J(\theta))$$<br>&emsp;&emsp;这里的$\theta_j:$是指分别对$j=0,1,2,…,n$个参数求偏导。$\alpha$指学习速率，代表每次向着$J(\theta)$最陡峭的方向移动的步幅。从上面公式可以看到，为了更新参数$\theta_j:$，式子右边的$\frac{\partial}{\partial\theta_j}J(\theta))$我们还没有知道的。当我们只有一个数据点$(x,y)$的时候，$J$的偏导数：   </p>
<p><div align="center"><img src="http://i.imgur.com/MCvj18T.png" alt=""></div><br>因此，对于单个训练样本，其更新规则为：<div align="center"><img src="http://i.imgur.com/rGou5cf.png" alt=""></div><br>对于所有的训练样本，累加上述损失函数的偏导为：<div align="center"><img src="http://i.imgur.com/eopLnnE.png" alt=""></div><br>于是，每个参数的更新规则就变成为：<div align="center"><img src="http://i.imgur.com/XlxR2g8.png" alt=""></div><br>&emsp;&emsp;从上面公式可以清楚看到，参数的每一次更新（迭代）都要用到全量训练数据（下图红色框位置）<div align="center"><img src="http://i.imgur.com/txXGQRl.png" alt=""></div><br>&emsp;&emsp;这种更新方式，在迭代过程中，参数的方差是最小的，收敛的过程也是最稳定的，但是如果训练数据量$m$很大，批量梯度下降将会是一个非常耗时的过程。</p>
<h2 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h2><blockquote>
<p>随机梯度下降法的每次更新，是随机对一个样本求梯度并更新相应的参数。  </p>
</blockquote>
<p>&emsp;&emsp;从上面批量梯度下降法的求解过程可以看到，对于一个样本的损失函数，其对应参数的更新方式为：<div align="center"><img src="http://i.imgur.com/rdbGBMV.png" alt=""></div><br>&emsp;&emsp;这种做法在面对大数据集时不会出现冗余，能够进行快速的迭代。因为每次仅迭代一个样本，随机梯度下降法求解极值的过程，并不是总是向着整体最优的方向迭代的，参数的方差变化很大，收敛很不稳定，相比批量梯度下降会更加曲折，因此准确率相对于批量梯度下降法会有所下降。   </p>
<h2 id="小批量梯度下降法"><a href="#小批量梯度下降法" class="headerlink" title="小批量梯度下降法"></a>小批量梯度下降法</h2><blockquote>
<p>小批量梯度下降法每次更新参数，仅使用一小部分的训练数据进行迭代   </p>
</blockquote>
<p>&emsp;&emsp;使用批量梯度下降法准确率很高，但是效率低，随机梯度下降法效率高，但是准确率低，而小批量梯度下降法算是批量下降法和随机梯度下降法的一种折衷，其集合了前面两种下降方法的优势，具体操作过程如下：</p>
<p><div align="center"><img src="http://i.imgur.com/4PAFFYu.png" alt=""></div><br>&emsp;&emsp;这里的$n$一般会选一个很小的数，比如10或者100，这样的话，在迭代过程中，参数值的方差不至于太大，收敛的过程会更加稳定。</p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>&emsp;&emsp;在机器学习中，梯度下降法通常用于优化损失函数，优化的过程，是沿着梯度的负方向不断逼近极值。从第二点‘梯度下降算法’里的实例图可以看到，一开始算法的下降速度很快，但是在极值附近时，算法的收敛速度很慢，比如在第二点的例子，在第四次迭代时已经逼近极小值了，但是在第38次迭代才解出极小值。另外，步长$\alpha$的选取很关键，步长过程可能会达不到极值点，甚至有可能发散，步长过短会导致收敛速度很慢。<br>&emsp;&emsp;批量梯度下降法每次迭代都用到所有训练数据，准确度高同时复杂度也高；随机梯度下降法每次迭代随机选取一个数据样本进行更新，准确度不高，复杂度较低；小批量梯度下降法集合了前面两种下降方法的优势，训练复杂度较低，精确度也较高。<br>&emsp;&emsp;最后，不得不吐槽下，markdown编辑公式真的是非常非常非常蛋疼。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;李航在其《统计学习方法》中提到，机器学习的核心由三大块组成，分别为模型、策略、算法，其实说白了，机器学习就是一个模型 + 一个损失函数 + 一个最优化算法。损失函数是为了让模型能够更好地拟合或分类数据，最优化算法是用来优化损失函数的，而在机器学习中，最常用的最优化算法应该算梯度下降了，这篇文章就来讲讲梯度下降，探究下它是如何优化损失函数的。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="最优化" scheme="http://yoursite.com/tags/%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>感知机模型</title>
    <link href="http://yoursite.com/posts/2017/08/01/perceptron_model.html"/>
    <id>http://yoursite.com/posts/2017/08/01/perceptron_model.html</id>
    <published>2017-08-01T03:16:20.734Z</published>
    <updated>2017-08-03T01:29:36.188Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;刚从广发项目撤出来，未来几周应该都不会很忙，趁着闲暇时间整理一下机器学习方面的知识。<br>&emsp;&emsp;先从最简单的感知机模型说起。感知机是一种二分类的线性模型，其假设训练数据集是线性可分的，目标是找到一个能够将训练数据集的正样本和负样本完全正确分割开的分离超平面。其实，模型只要找到该分离超平面，学习目的就达到了，并不要求样本能被最大限度地划分。<br><a id="more"></a>   </p>
<h1 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h1><p>&emsp;&emsp;分类器能够通过输入的特征向量映射到给定类别中的一个，也就是所谓的物以聚类人以群分。如下图一，蓝色直线将A和B完全分割开，那么该直线$y=ax+b$就是一个分类器。根据分类器是否线性可分的性质，分类器有线性分类器(图一)和非线性分类器（图二图三），从类别上，有二元分类器（图一）和多元分类器（两个类别以上，图二图三），而本章要讨论的感知机模型，就是一个二分类线性模型。<div align="center"><img src="http://i.imgur.com/jH3FvKL.png" alt=""></div>   </p>
<h1 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h1><p>&emsp;&emsp;感知机模型很简单，为什么说它简单呢，因为该模型只用了一个线性方程$y = ax + b$ 以及 一个符号函数$sign(x)$（初中的知识是不是？）<br>&emsp;&emsp;下面来看下感知机模型的定义。<br>&emsp;&emsp;假设有$N$维的特征输入$X = \lbrace x_1,x_2,…,x_n \rbrace $，输出的类别有两类$ Y = \lbrace +1,-1\rbrace $，则由特征输入映射到类别输出的函数$$f(x) = sign(w*x+b)$$称为感知机模型，其中，$w$ 和$b$是我们要求解的模型的参数，这里的$w$其实是一个跟特征输入维度一样$N$维的权值向量（也称为权重），而参数$b$为偏置（在二维平面上，$b$就是截距）。<br>&emsp;&emsp;我们从公式上去解读下为什么说感知机模型是一个线性二分类模型。<br>&emsp;&emsp;先看符号函数里面的线性方程$$y=w*x+b$$显然，方程$ y = w*x +b $在二维平面上是一条直线，在三维空间上是一个平面，在四维甚至更高维空间上就是一个超平面。（哈哈哈，这个实在是画不出来）<div align="center"><img src="http://i.imgur.com/RtXJNgB.png" alt=""></div><br>也就是说，不管是在二维三维空间还是更高维空间，超平面都可以将空间一分为二，并且都可以表示成方程$ y = w*x +b $，所以感知机模型首先是一个线性模型。<br>&emsp;&emsp;再看符号函数，这个更简单，符号函数的定义如下：<br>$$<br>sign(x)=\begin{cases}<br>+1,\quad x\geq 0\\<br>-1, \quad x&lt;0<br>\end{cases}$$   </p>
<p>如果$x&gt;=0$则$f(x)=+1$，否则$f(x)=-1$。那么对于函数$f(x) = sign(w*x+b)$，如果输入样本$w*x+b&gt;=0$，那么该样本会被判为+1类，否则判为-1类。举个例子，回到上文图一，对于直线上方有$w*x+b&gt;0$，所以所有的A样本都会归为+1类，对于直线下方有$w*x+b&lt;0$，这样所有的B样本都会归为-1类。因此，感知机模型是一个线性二分类模型，其任务就是，找到这样的一个超平面，能够把两个不同的分类完全分割开。   </p>
<h1 id="学习策略"><a href="#学习策略" class="headerlink" title="学习策略"></a>学习策略</h1><p>&emsp;&emsp;原理清楚了，那么现在的问题是，给定一个包含正负类的训练样本，我们应该如何找出这样的一个超平面，使之能够将正样例和负样例点完全分割开，也就是应该如何确定模型的参数$w$和$b$ <div align="center"> <img src="http://i.imgur.com/cZbjVQP.png" alt=""> </div><br>&emsp;&emsp;如上图，如何找到一条直线，使之能够将蓝色实例和褐色实例分割开（这里只是举个例子哈，在实际构建模型过程中，涉及的数据动不动就成百上千万，你想拿支笔来，往两个样本点之间一画一条直线，这是妥妥的不行的，更何况实际处理的数据维度一般都有两位数以上）。直接求解好像无从下手，不妨先假设参数$w$和$b$已知，也就是说这个超平面$S$我们已经找到了，但是不知道这超平面对样例数据的分类效果怎样。如何去评估分类的效果呢，一个很自然的想法就是看有多少样例数据是误分类的，也就是将总误分类数作为模型的损失函数，但是这样的函数对于参数$w$和$b$来说不是连续可导的，难以优化。另一种方法就是可以通过衡量误分类点到超平面的总距离来评估效果。高中的时候我们就学过点到平面的距离的公式是这样子的。$$d=\frac{|A*x_0+B*y_0+C*z_0+D|}{\sqrt{A^2+B^2+C^2}}$$ 其中$(A,B,C)$是平面法向量，在这里是权值向量，上面距离公式也可以简写成$$d=\frac{1}{||w||}|wx_0+b|$$ 其中$w=(A,B,C)$， 这里||w||也称为$w$的L2范数。<br>依然以上文图一为例，对于任意一个样本点$(x_i,y_i)$:</p>
<ul>
<li>如果该样本点是位于直线上方，并且刚好属于类别A的话，那么该样本点被直线正确分类，此时有$w*x_i+b&gt;0,y_i=+1$，显然$-y_i*(w*x_i+b)&lt;0$   </li>
<li>如果该样本点位于直线下方，并且刚好属于类别B的的话，那么该样本点被直线正确分类，此时有$w*x_i+b&lt;0,y_i=-1$，显然$-y_i*(w*x_i+b)&lt;0$   </li>
<li>如果该样本点位于直线上方，并且刚好属于类别B的的话，那么该样本点被直线错误分类，此时有$w*x_i+b&gt;0,y_i=-1$，显然$-y_i*(w*x_i+b)&gt;0$   </li>
<li>如果该样本点位于直线下方，并且刚好属于类别A的的话，那么该样本点被直线错误分类，此时有$w*x_i+b<0,y_i=+1$，显然$-y_i*(w*x_i+b)>0$   </0,y_i=+1$，显然$-y_i*(w*x_i+b)></li>
</ul>
<p>&emsp;&emsp;所以，对于误分类的点$(x_i,y_i)$来说$-y_i*(w*x_i+b)&gt;0$成立。<br>&emsp;&emsp;我们可以用$-y_i*(w*x_i+b)$（相当于$|w*x_i+b|$）去衡量误分类点的效果，$w*x_i+b$值越大，说明该点离分离超平面越远，误分类效果越差（虽然分错了就是分错了）。如下图中的红色直线对于点A的分类效果明显比点B的要差   </p>
<p><div align="center"><img src="http://i.imgur.com/0vGucMO.png" alt=""> </div><br>有了单个误分类点离超平面的距离，那么对于所有的误分类点有：$$ -\frac{1}{||w||}\sum_i^m y_i(wx_i+b)$$ 其中$m$为误分类点数量。当不考虑$\frac{1}{||w||}$时（$\frac{1}{||w||}$恒大于0，去掉的话不影响评估效果），就得到感知机模型的损失函数$Loss  Function$（用来度量模型预测的好坏）：$$ L(w,b) =-\sum_i^m y_i(wx_i+b)$$既然误分类点离分离超平面越远，误分类效果越差，那么，误分类点离分离超平面越近，误分类效果越好，当$$-\sum_i^m y_i(wx_i+b) = 0$$时，样本没有被误分类，所有样本都被超平面恰当地分割开。所以只要找到$w$和$b$使得$L(w,b)$取最小值，此时超平面的分类效果是最好的，这个时候我们可以将参数$w$和$b$的求解转化为最小化函数$L(w,b)$ $$minL(w,b) = -\sum_i^m y_i(wx_i+b)$$   </p>
<h1 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h1><p>&emsp;&emsp;最优化方法有很多，比如牛顿下降法、拉格朗日乘子法、共轭梯度下降法、梯度下降法，这里使用最简单也最常用的一样方法：随机梯度下降法（梯度下降法的一种方法）来优化上文提到的损失函数$L(w,b)$ $$L(w,b) = -\sum_i^m y_i(wx_i+b)$$ 梯度其实就是求偏导，随机梯度下降其实就是一次随机选一个误分类点，使其函数值$L(w,b)$往着负梯度方向移动，不断逼近最小值的一个过程。函数$L(w,b)$关于参数$w$和$b$的梯度分别为：$$\nabla_w L(w,b)= -\sum_i^m y_i x_i $$ $$\nabla_b L(w,b)= -\sum_i^m y_i $$ 现在，我们随便找一个分割超平面，比如$w=0,b=0$（注意$w$是一个向量），然后随机选一个实例点，判断$-y_i*(w*x_i+b)$ 是否大于等于0，如果大于等于0则说明该点被误分类，这时对$(w,b)$进行如下更新（如果$-y_i*(w*x_i+b)$小于0则不更新$w$和$b$）：$$w \leftarrow w+\eta y_i x_i$$ $$b \leftarrow b+\eta y_i$$ 这里的$\eta$$(0&lt;=\eta&lt;=1)$为步长，也称为学习率，这样，通过不断选取误分类点，更新参数$(w,b)$，降低函数$L(w,b)$的值，直到$L(w,b)=0$时对应的$(w,b)$的取值就是要求解的值。<br>算法很简单，分成4步，也就是：   </p>
<ul>
<li>(1)选取初值$(w_0,b_0)$;   </li>
<li>(2)在训练数据集中随机选取一个数据$(x_i,y_i)$;   </li>
<li>(3)如果$-y_i*(w*x_i+b)&gt;=0$，则进行如下更新：$$w \leftarrow w+\eta y_i x_i$$ $$b \leftarrow b+\eta y_i$$   </li>
<li>(4)转至(2)，直至数据集中没有误分类的点或者达到指定的迭代次数。</li>
</ul>
<p>用python实现基于随机梯度下降的简单感知机模型。</p>
<pre><code class="python"><span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> random
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt
%matplotlib inline
<span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs
<span class="comment">#随机生成特征维度为2，分别以[-1,-1],[1,1]为中心，类别方差为0.4,0.5的两个类</span>
x,y = make_blobs(n_samples=<span class="number">100</span>,n_features=<span class="number">2</span>,centers=[[<span class="number">-1</span>,<span class="number">-1</span>],[<span class="number">1</span>,<span class="number">1</span>]],cluster_std=[<span class="number">0.4</span>,<span class="number">0.5</span>])
<span class="comment"># 将 0 替换成-1</span>
y[y==<span class="number">0</span>] = <span class="number">-1</span>
w = np.zeros(<span class="number">2</span>)<span class="comment">#初始权重赋值为0</span>
b = <span class="number">0</span> <span class="comment">#初始偏置为0</span>
k=<span class="number">200</span> <span class="comment">#最大迭代次数</span>
l_rate = <span class="number">0.5</span> <span class="comment">#学习率</span>
i=<span class="number">0</span>
<span class="keyword">while</span> i &lt;= k:
    i = i+<span class="number">1</span>
    <span class="comment">#生成随机数</span>
    random_num = random.randint(<span class="number">0</span>,<span class="number">99</span>)
    <span class="comment">#损失函数</span>
    <span class="keyword">if</span> sum(-y[random_num]*(w*x[random_num] + b)) &gt;= <span class="number">0</span>:
        <span class="comment">#梯度更新权重</span>
        w = w + l_rate*y[random_num]*x[random_num]
        <span class="comment">#梯度更新偏置</span>
        b = b + l_rate*y[random_num]
<span class="comment">#---------------------画图-------------------------</span>

x1 = <span class="number">-3.0</span>
y1 = -(b + w[<span class="number">0</span>] * x1) /w[<span class="number">1</span>]
x2 = <span class="number">3.0</span>
y2 = -(b + w[<span class="number">0</span>] * x2) / w[<span class="number">1</span>]
plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))  
plt.plot([x1,x2],[y1,y2],<span class="string">'r'</span>)
plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],marker=<span class="string">'o'</span>,c=y,s=<span class="number">50</span>)
plt.xlabel(<span class="string">'x1'</span>)
plt.ylabel(<span class="string">'x2'</span>)
plt.show()
</code></pre>
<p>&emsp;&emsp;因为这里的两个样本区分度太明显，所以经过几次迭代就已经收敛了。最终求得参数$w=[1.03972853 \  0.97360177] ,b=0$，根据参数$(w,b)$画出的直线如下图所示：<div align="center"><img src="http://i.imgur.com/WftOMnU.png" alt=""></div><br>&emsp;&emsp;选取不同的初始值$(w,b)$或者不同的误分类点，划分的超平面很有可能会不一样，这也就是为什么在线性可分数据集中，用感知机模型求出来的解有无穷多个。如下面直线$A,B,C$都可作为解。</p>
<p><div align="center"> <img src="http://i.imgur.com/ibURFh2.png" alt=""></div> </p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>&emsp;&emsp;感知机模型假设训练数据集是线性可分的，如果线性不可分，算法会一直震荡无法收敛，所以其无法处理一些复杂的数据，如在分类器中提到的图二和图三。考虑到实际业务的数据一般比较复杂，简单的感知机模型无法有效地处理如此复杂的数据，所以在建模中很少会使用该模型。但是感知机模型还是比较重要的，为什么这么说呢，因为只要稍微修改下模型的损失函数，感知机模型就可转变为广受欢迎的分类神器：支持向量机，而通过简单的堆叠可演变为神经网络模型。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;刚从广发项目撤出来，未来几周应该都不会很忙，趁着闲暇时间整理一下机器学习方面的知识。&lt;br&gt;&amp;emsp;&amp;emsp;先从最简单的感知机模型说起。感知机是一种二分类的线性模型，其假设训练数据集是线性可分的，目标是找到一个能够将训练数据集的正样本和负样本完全正确分割开的分离超平面。其实，模型只要找到该分离超平面，学习目的就达到了，并不要求样本能被最大限度地划分。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类模型" scheme="http://yoursite.com/tags/%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="监督学习" scheme="http://yoursite.com/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>TF-IDF原理及应用</title>
    <link href="http://yoursite.com/posts/2017/03/15/TF-IDF.html"/>
    <id>http://yoursite.com/posts/2017/03/15/TF-IDF.html</id>
    <published>2017-03-15T09:36:24.153Z</published>
    <updated>2017-03-16T06:09:16.931Z</updated>
    
    <content type="html"><![CDATA[<p>你说广州塔，我知道是在广州，你说黄果树瀑布，我知道是在贵州，你说布达拉宫，我知道是在拉萨，你说公交车，我都不知道你在说哪个城市的公交车。这就是<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>。</p>
<h1 id="概念及原理"><a href="#概念及原理" class="headerlink" title="概念及原理"></a>概念及原理</h1><p><a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>全称Term Frequency and Inverse Document Frequency，直译过来就是’词频-逆向文件频率’，’TF’是指某一个给定的词语在该文件中出现的频率，’IDF’是指总文件数除以包含该词的文件数，再取对数。<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>一般用来评估在一堆语料库或一堆文件集中，某个字词对于该语料库或该文件的重要程度。怎么理解呢，举个例子，假设现在手上有10篇文章，‘水果’这个词在某一篇文章出现的频率很高，但是在这10篇文章中的仅有2篇文章提到，那么‘水果’这个词的<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>会很高，如果10篇文章中有8篇提到‘水果’这个词，那么这个词的‘<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>’会相对偏低。主要思想就是，一个词越能将一篇文章与其他文章区分开来，那么这个词的权重越高。<br><a id="more"></a></p>
<h1 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h1><h2 id="TF计算："><a href="#TF计算：" class="headerlink" title="TF计算："></a>TF计算：</h2><p><img src="http://i.imgur.com/B1xMUhQ.png" alt=""> </p>
<p>（markdown编辑数学公式还不怎么熟，先用mathtype搞好再截图吧）比如上面的例子，’水果’，’硬盘’在文章1中出现的次数分别为2次，4次，那么:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">TF(水果) = 2/10 = 0.2   </div><div class="line">TF(硬盘) = 4/10 = 0.4</div></pre></td></tr></table></figure></p>
<h2 id="IDF计算："><a href="#IDF计算：" class="headerlink" title="IDF计算："></a>IDF计算：</h2><p><img src="http://i.imgur.com/oTT2Zpz.png" alt=""></p>
<p>如果这10篇文章中，有2篇文章包含有’水果’这个词，有5篇包含’硬盘’这个词，那么：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">IDF(水果) = <span class="built_in">log</span>(10/2) = 1.6094   </div><div class="line">IDF(硬盘) = <span class="built_in">log</span>(10/5) = 0.6931</div></pre></td></tr></table></figure></p>
<h2 id="TF-IDF计算"><a href="#TF-IDF计算" class="headerlink" title="TF-IDF计算"></a>TF-IDF计算</h2><p>算好TF和IDF之后，就可以计算’水果’和’硬盘’的<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>了，只需要将TF和IDF相乘就ok。<br><img src="http://i.imgur.com/4jhJzZI.png" alt=""><br>所以’水果’的TF-IDF为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0.2*1.6094</div></pre></td></tr></table></figure></p>
<p>‘硬盘’的TF-IDF为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0.4*0.6931</div></pre></td></tr></table></figure></p>
<p>如果算’水果’和’硬盘’这两个词与文章1的相关性呢，很简单，只要将这两个词的<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>加起来。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0.2*1.6094 + 0.4*0.6931</div></pre></td></tr></table></figure></p>
<h1 id="python中计算TF-IDF"><a href="#python中计算TF-IDF" class="headerlink" title="python中计算TF-IDF"></a>python中计算TF-IDF</h1><h2 id="使用的工具"><a href="#使用的工具" class="headerlink" title="使用的工具"></a>使用的工具</h2><ul>
<li>jieba</li>
<li>scikit-learn</li>
</ul>
<h2 id="切词"><a href="#切词" class="headerlink" title="切词"></a>切词</h2><p>其实切词只是计算TF-IDF的前期准备工作，在对中文文本进行<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>计算的话，切词这一步应该是怎么也逃不过去了。平常工作中基本都是用jieba切词，这里也打算用jieba对文本进行处理。<br>例如我现在有5个文本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">content = [[<span class="string">'萨德系统核心装备X波段雷达'</span>],[<span class="string">'美韩当局部署萨德的步伐也在加速进行'</span>],[<span class="string">'纵观如今的手机处理器市场已经不是高通一家独大的局面'</span>],[<span class="string">'三星的Exynos处理器以及华为的海思麒麟芯片这些年风头正盛'</span>],[<span class="string">'魅族每年数以千万计的销量对于芯片厂商的贡献也是不可小看的'</span>]]</div></pre></td></tr></table></figure></p>
<p>首先需要对文本进行切词，切词代码及结果如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_words</span><span class="params">(text)</span>:</span></div><div class="line">    results = []</div><div class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> contents:</div><div class="line">        seg_list = jieba.cut(content[<span class="number">0</span>],cut_all=<span class="keyword">False</span>)</div><div class="line">        <span class="comment"># 实际应用过程中，这里需要去除停用词</span></div><div class="line">        seg = <span class="string">' '</span>.join(seg_list)</div><div class="line">        results.append(seg)</div><div class="line">    <span class="keyword">return</span> results</div><div class="line">result = cut_words(contents)</div><div class="line"></div><div class="line">result = [<span class="string">'萨德 系统核心 装备 X 波段 雷达'</span>, <span class="string">'美韩 当局 部署 萨德 的 步伐 也 在 加速 进行'</span>, <span class="string">'纵观 如今 的 手机 处理器 市场 已经 不是 高通 一家独大 的 局面'</span>, <span class="string">'三星 的 Exynos 处理器 以及 华为 的 海思 麒麟 芯片 这些 年 风头 正 盛'</span>, <span class="string">'魅族 每年 数以千万计 的 销量 对于 芯片 厂商 的 贡献 也 是 不可 小看 的'</span>]</div></pre></td></tr></table></figure></p>
<p>准备工作做好之后，我们就可以进行<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>计算了。</p>
<h2 id="词语转矩阵"><a href="#词语转矩阵" class="headerlink" title="词语转矩阵"></a>词语转矩阵</h2><p>词语转矩阵需要用到<a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" target="_blank" rel="external">CountVectorizer</a>这个函数，其作用是统计词汇的数量，并转为矩阵。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding:utf-8</span></div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</div><div class="line">vectorizer = CountVectorizer()</div><div class="line">vector_location = vectorizer.fit_transform(result)</div></pre></td></tr></table></figure></p>
<p>通过type(vector_location)可以看到，函数fit_transform把result二维数组表示成一个稀疏矩阵:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">print(type(vector_location))</div><div class="line"><span class="comment">#输出</span></div><div class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">scipy</span>.<span class="title">sparse</span>.<span class="title">csr</span>.<span class="title">csr_matrix</span>'&gt;</span></div></pre></td></tr></table></figure></p>
<p>同时可以看下，vercot_location的输出结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">print(vector_location)</div><div class="line"><span class="comment">#输出</span></div><div class="line"><span class="comment">#(0, 27)	1</span></div><div class="line"><span class="comment">#(0, 23)	1</span></div><div class="line"><span class="comment">#(0, 28)	1</span></div><div class="line"><span class="comment">#(0, 21)	1</span></div><div class="line"><span class="comment">#(0, 34)	1</span></div><div class="line"><span class="comment">#(1, 27)	1</span></div><div class="line"><span class="comment">#(1, 25)	1</span></div><div class="line"><span class="comment">#(1, 16)	1</span></div><div class="line"><span class="comment">#(1, 32)	1</span></div><div class="line"><span class="comment">#(1, 19)	1</span></div><div class="line"><span class="comment">#(1, 6)	    1</span></div><div class="line"><span class="comment">#(1, 31)	1</span></div><div class="line"><span class="comment">#(2, 24)	1</span></div><div class="line"><span class="comment">#(2, 10)	1</span></div><div class="line"><span class="comment">#(2, 17)	1</span></div><div class="line"><span class="comment">#(2, 9)	    1</span></div><div class="line"><span class="comment">#(2, 15)	1</span></div><div class="line"><span class="comment">#(2, 14)	1</span></div><div class="line"><span class="comment">#(2, 4)	    1</span></div><div class="line"><span class="comment">#(2, 36)	1</span></div><div class="line"><span class="comment">#(2, 1)	    1</span></div><div class="line"><span class="comment">#(2, 13)	1</span></div><div class="line"><span class="comment">#(3, 9)	    1</span></div><div class="line"><span class="comment">#(3, 2)	    1</span></div><div class="line"><span class="comment">#(3, 0)  	1</span></div><div class="line"><span class="comment">#(3, 5) 	1</span></div><div class="line"><span class="comment">#(3, 7) 	1</span></div><div class="line"><span class="comment">#(3, 22)	1</span></div><div class="line"><span class="comment">#(3, 38)	1</span></div><div class="line"><span class="comment">#(3, 26)	1</span></div><div class="line"><span class="comment">#(3, 30)	1</span></div><div class="line"><span class="comment">#(3, 35)	1</span></div><div class="line"><span class="comment">#(4, 26)	1</span></div><div class="line"><span class="comment">#(4, 37)	1</span></div><div class="line"><span class="comment">#(4, 20)	1</span></div><div class="line"><span class="comment">#(4, 18)	1</span></div><div class="line"><span class="comment">#(4, 33)	1</span></div><div class="line"><span class="comment">#(4, 11)	1</span></div><div class="line"><span class="comment">#(4, 8) 	1</span></div><div class="line"><span class="comment">#(4, 29)	1</span></div><div class="line"><span class="comment">#(4, 3)    	1</span></div><div class="line"><span class="comment">#(4, 12)	1</span></div></pre></td></tr></table></figure></p>
<p>输出结果表示的是这个稀疏矩阵的第几行第几列有值，比如(0, 27)    1表示矩阵的第0行第27列有值。<br>转成矩阵的形式之后，我们就可以很容易地算出每个词对应的<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>了，这里使用<a href="http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html" target="_blank" rel="external">TfidfTransformer</a>函数进行计算。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</div><div class="line">transformer = TfidfTransformer()</div><div class="line">tf_idf = transformer.fit_transform(vector_location)</div><div class="line">print(type(tf_idf))</div><div class="line"><span class="comment">#输出，同样是稀疏矩阵的形式</span></div><div class="line"><span class="comment">#&lt;class 'scipy.sparse.csr.csr_matrix'&gt;</span></div><div class="line">print(tf_idf)</div><div class="line"><span class="comment">#输出</span></div><div class="line"><span class="comment">#(0, 34)	0.463693222732</span></div><div class="line"><span class="comment">#(0, 21)	0.463693222732</span></div><div class="line"><span class="comment">#(0, 28)	0.463693222732</span></div><div class="line"><span class="comment">#(0, 23)	0.463693222732</span></div><div class="line"><span class="comment">#(0, 27)	0.37410477245</span></div><div class="line"><span class="comment">#(1, 31)	0.387756660106</span></div><div class="line"><span class="comment">#(1, 6) 	0.387756660106</span></div><div class="line"><span class="comment">#(1, 19)	0.387756660106</span></div><div class="line"><span class="comment">#(1, 32)	0.387756660106</span></div><div class="line"><span class="comment">#(1, 16)	0.387756660106</span></div><div class="line"><span class="comment">#(1, 25)	0.387756660106</span></div><div class="line"><span class="comment">#(1, 27)	0.312839631859</span></div><div class="line"><span class="comment">#(2, 13)	0.321896111462</span></div><div class="line"><span class="comment">#(2, 1) 	0.321896111462</span></div><div class="line"><span class="comment">#(2, 36)	0.321896111462</span></div><div class="line"><span class="comment">#(2, 4) 	0.321896111462</span></div><div class="line"><span class="comment">#(2, 14)	0.321896111462</span></div><div class="line"><span class="comment">#(2, 15)	0.321896111462</span></div><div class="line"><span class="comment">#(2, 9) 	0.259703755905</span></div><div class="line"><span class="comment">#(2, 17)	0.321896111462</span></div><div class="line"><span class="comment">#(2, 10)	0.321896111462</span></div><div class="line"><span class="comment">#(2, 24)	0.321896111462</span></div><div class="line"><span class="comment">#(3, 35)	0.327880622184</span></div><div class="line"><span class="comment">#(3, 30)	0.327880622184</span></div><div class="line"><span class="comment">#(3, 26)	0.264532021474</span></div><div class="line"><span class="comment">#(3, 38)	0.327880622184</span></div><div class="line"><span class="comment">#(3, 22)	0.327880622184</span></div><div class="line"><span class="comment">#(3, 7) 	0.327880622184</span></div><div class="line"><span class="comment">#(3, 5) 	0.327880622184</span></div><div class="line"><span class="comment">#(3, 0) 	0.327880622184</span></div><div class="line"><span class="comment">#(3, 2) 	0.327880622184</span></div><div class="line"><span class="comment">#(3, 9) 	0.264532021474</span></div><div class="line"><span class="comment">#(4, 12)	0.321896111462</span></div><div class="line"><span class="comment">#(4, 3) 	0.321896111462</span></div><div class="line"><span class="comment">#(4, 29)	0.321896111462</span></div><div class="line"><span class="comment">#(4, 8) 	0.321896111462</span></div><div class="line"><span class="comment">#(4, 11)	0.321896111462</span></div><div class="line"><span class="comment">#(4, 33)	0.321896111462</span></div><div class="line"><span class="comment">#(4, 18)	0.321896111462</span></div><div class="line"><span class="comment">#(4, 20)	0.321896111462</span></div><div class="line"><span class="comment">#(4, 37)	0.321896111462</span></div><div class="line"><span class="comment">#(4, 26)	0.259703755905</span></div></pre></td></tr></table></figure></p>
<p>做到这一步的话还差一点点，因为在实际应用过程中，我们还需要把稀疏矩阵转成平常用的行列形式的矩阵才方便使用。这里可以使用todense()或者toarray()函数，前者是将稀疏矩阵转成matrix的形式，后者是将稀疏矩阵转成ndarray的形式<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">weight = tf_idf.toarray()</div><div class="line"><span class="comment">#or</span></div><div class="line">weight1 = tf_idf.todense()</div><div class="line"></div><div class="line">print(weight)</div><div class="line"><span class="comment">#输出</span></div><div class="line"><span class="comment">#(5,39)</span></div></pre></td></tr></table></figure></p>
<p>其实print一下weight的shape可以看到，这是一个5行39列的数组，5代表5个文本，39代表切词并用于计算的词的数量。如果文本的数量很大，切出来的词有很多的话，你很有可能会遇到MemoryError的错误，比如有1万个文本，切词3万个，那么数组（矩阵）的大小就变成了(10000,30000)，这样就会变得极其稀疏。对于大数据量的文本在计算<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="external">TF-IDF</a>时遇到MemoryError时，除了加内存或者上服务器之外，目前尚未找到更好的解决办法。<br>这里还有一个问题，就是我怎么知道每个权重对应的是哪个词呢？这里可以将词作为列名，将数组转成Dataframe进行查看。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">word=vectorizer.get_feature_names()</div><div class="line">df = pd.DataFrame(weight)</div><div class="line">df.columns = word</div><div class="line">print(df)</div></pre></td></tr></table></figure></p>
<h1 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h1><p>最后照例附上本次分析的源代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding:utf-8</span></div><div class="line"><span class="comment">#author:linchart</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> jieba</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</div><div class="line">contents = [[<span class="string">'萨德系统核心装备X波段雷达'</span>],\</div><div class="line">            [<span class="string">'美韩当局部署萨德的步伐也在加速进行'</span>],\</div><div class="line">            [<span class="string">'纵观如今的手机处理器市场已经不是高通一家独大的局面'</span>],\</div><div class="line">            [<span class="string">'三星的Exynos处理器以及华为的海思麒麟芯片这些年风头正盛'</span>],\</div><div class="line">            [<span class="string">'魅族每年数以千万计的销量对于芯片厂商的贡献也是不可小看的'</span>]]</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_words</span><span class="params">(text)</span>:</span></div><div class="line">    results = []</div><div class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> contents:</div><div class="line">        seg_list = jieba.cut(content[<span class="number">0</span>],cut_all=<span class="keyword">False</span>)</div><div class="line">        <span class="comment"># 实际应用过程中，这里需要去除停用词</span></div><div class="line">        seg = <span class="string">' '</span>.join(seg_list)</div><div class="line">        results.append(seg)</div><div class="line">    <span class="keyword">return</span> results</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_idf</span><span class="params">(words)</span>:</span></div><div class="line">    vectorizer = CountVectorizer()</div><div class="line">    vector_location = vectorizer.fit_transform(result)</div><div class="line">    transformer = TfidfTransformer()</div><div class="line">    tf_idf = transformer.fit_transform(vector_location)</div><div class="line">    weight = tf_idf.toarray()</div><div class="line">    word = vectorizer.get_feature_names()</div><div class="line">    df = pd.DataFrame(weight)</div><div class="line">    df.columns = word</div><div class="line">    <span class="keyword">return</span> df</div><div class="line"></div><div class="line">result = cut_words(contents)</div><div class="line">df = tf_idf(result)</div><div class="line">print(df)</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;你说广州塔，我知道是在广州，你说黄果树瀑布，我知道是在贵州，你说布达拉宫，我知道是在拉萨，你说公交车，我都不知道你在说哪个城市的公交车。这就是&lt;a href=&quot;https://zh.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt;。&lt;/p&gt;
&lt;h1 id=&quot;概念及原理&quot;&gt;&lt;a href=&quot;#概念及原理&quot; class=&quot;headerlink&quot; title=&quot;概念及原理&quot;&gt;&lt;/a&gt;概念及原理&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://zh.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt;全称Term Frequency and Inverse Document Frequency，直译过来就是’词频-逆向文件频率’，’TF’是指某一个给定的词语在该文件中出现的频率，’IDF’是指总文件数除以包含该词的文件数，再取对数。&lt;a href=&quot;https://zh.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt;一般用来评估在一堆语料库或一堆文件集中，某个字词对于该语料库或该文件的重要程度。怎么理解呢，举个例子，假设现在手上有10篇文章，‘水果’这个词在某一篇文章出现的频率很高，但是在这10篇文章中的仅有2篇文章提到，那么‘水果’这个词的&lt;a href=&quot;https://zh.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt;会很高，如果10篇文章中有8篇提到‘水果’这个词，那么这个词的‘&lt;a href=&quot;https://zh.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt;’会相对偏低。主要思想就是，一个词越能将一篇文章与其他文章区分开来，那么这个词的权重越高。&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>爬取微信文章（一）</title>
    <link href="http://yoursite.com/posts/2017/03/13/WeChat_Article1.html"/>
    <id>http://yoursite.com/posts/2017/03/13/WeChat_Article1.html</id>
    <published>2017-03-13T09:59:18.349Z</published>
    <updated>2017-03-14T08:05:48.201Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;有时在微信公众号上面看到一些写的比较好的文章，但又没有时间细看，闲下来想找这些文章的时候又忘了是在哪个公众号看的了、文章名字也想不起来，因此想搞个爬虫把想看的文章爬下来，一来可以在闲时咀嚼一下，二来也可以收藏一些好文章，做些知识积累。<br><em>只是想把自己平常做的一些东西记录下来，非教程</em><br><a id="more"></a>   </p>
<h1 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h1><ul>
<li>Python 3.5.1   </li>
</ul>
<h1 id="使用的库"><a href="#使用的库" class="headerlink" title="使用的库"></a>使用的库</h1><ul>
<li>re</li>
<li>pdfkit</li>
<li>requests</li>
<li>BeautifulSoup   </li>
</ul>
<h1 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h1><p>输入微信文章名称或者对应的文章链接，输出文章的pdf文件。</p>
<h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><ul>
<li>如果同时提供文章链接和文章名称，则优先通过文章链接爬取，如果文章链接爬取失败，则通过文章名称爬取；   </li>
<li>如果仅提供文章链接，则通过文章链接爬取；</li>
<li>如果仅提供文章名称，则通过搜狗微信接口搜索微信文章，找到对应文章链接，然后在通过文章链接爬取。</li>
</ul>
<h1 id="爬取流程"><a href="#爬取流程" class="headerlink" title="爬取流程"></a>爬取流程</h1><h2 id="获取文章链接"><a href="#获取文章链接" class="headerlink" title="获取文章链接"></a>获取文章链接</h2><p>将提供的文章名称传入<a href="http://weixin.sogou.com/weixin" target="_blank" rel="external">搜狗微信搜索引擎搜索</a>，将结果列表中的第一篇文章作为目标文章下载。下面代码返回目标文章链接。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article_link</span><span class="params">(query)</span>:</span></div><div class="line">    base_url = <span class="string">r'http://weixin.sogou.com/weixin'</span></div><div class="line">    User_Agent = <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span></div><div class="line">    Host = <span class="string">'weixin.sogou.com'</span></div><div class="line">    Connection = <span class="string">'keep-alive'</span></div><div class="line">    headers = &#123;<span class="string">'User-Agent'</span>: User_Agent, <span class="string">'Host'</span>: Host, <span class="string">'Connection'</span>: Connection&#125;</div><div class="line">    params = &#123;<span class="string">'type'</span>: <span class="number">2</span>, <span class="string">'ie'</span>: <span class="string">'utf-8'</span>, <span class="string">'w'</span>: <span class="string">'01019900'</span>, <span class="string">'sut'</span>: <span class="string">'707'</span>,<span class="string">'query'</span>:query&#125;</div><div class="line">    request = requests.get(base_url, headers=headers, params=params)</div><div class="line">    request.encoding = <span class="string">'utf-8'</span></div><div class="line">    bsobj = BeautifulSoup(request.text, <span class="string">'lxml'</span>)</div><div class="line">    <span class="comment"># 仅提取列表中的第一篇文章</span></div><div class="line">    first_article_link = bsobj.select(<span class="string">'#sogou_vr_11002601_title_0'</span>)[<span class="number">0</span>][<span class="string">'href'</span>]</div><div class="line">    <span class="keyword">return</span> first_article_link</div></pre></td></tr></table></figure>
<h2 id="将文章转为html"><a href="#将文章转为html" class="headerlink" title="将文章转为html"></a>将文章转为html</h2><p>解析文章链接，将文章内容保存为html文件。这里需要注意的是，在解析文章的时候，如果文章中包含有图片的话，正常情况下是无法下载下来的，因为爬取的文章链接为临时链接，非永久链接，无法直接解析src里面的链接。但是，data-src这个属性的值还是可以解析出来的，所以只要把data-src替换为src就可以下载图片了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article_html</span><span class="params">(link)</span>:</span></div><div class="line">    <span class="comment"># 为了保险起见，这里使用不同的headers</span></div><div class="line">    User_Agent = <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/3.0 Safari/536.11'</span></div><div class="line">    article_headers = &#123;<span class="string">'User_Agent'</span>: User_Agent&#125;</div><div class="line">    article_obj = requests.get(link, headers=article_headers)</div><div class="line">    article_obj.encoding = <span class="string">'utf-8'</span></div><div class="line">    soup = BeautifulSoup(article_obj.content, <span class="string">'html5lib'</span>)</div><div class="line">    <span class="comment"># 以实际的文章名称为准</span></div><div class="line">    article_name = soup.select(<span class="string">'#activity-name'</span>)[<span class="number">0</span>].text.strip()</div><div class="line">    content = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'id'</span>: <span class="string">'page-content'</span>&#125;)</div><div class="line">    html = str(content)</div><div class="line">    <span class="comment"># 把属性data-src替换成src,前面无法将属性src解析出来，data-src，只是LAZY用的，</span></div><div class="line">    <span class="comment"># 延迟加载图片所以显示不出来，LAZYLOAD</span></div><div class="line">    src_compile = re.compile(<span class="string">'data-src'</span>)</div><div class="line">    html_new = re.sub(src_compile, <span class="string">'src'</span>, html)</div><div class="line">    <span class="comment"># 存储成html</span></div><div class="line">    <span class="keyword">with</span> open(<span class="string">'wechat_article.html'</span>, <span class="string">'w'</span>, encoding=<span class="string">'GB18030'</span>) <span class="keyword">as</span> f:</div><div class="line">        f.write(html_new)</div><div class="line">    <span class="keyword">return</span> article_name</div></pre></td></tr></table></figure>
<h2 id="html转pdf"><a href="#html转pdf" class="headerlink" title="html转pdf"></a>html转pdf</h2><p>html文件转pdf调用了pdfkit这个包，使用这个包需要安装<a href="http://wkhtmltopdf.org/downloads.html" target="_blank" rel="external">wkhtmltopdf</a>软件（pdfkit依赖于wkhtmltopdf，因此需要配置路径）。<br>在运行过程中，发现pdfkit在html转pdf时，生成的pdf文件名中如果包含有| / *这些特殊符号时会报错，因此如果以原文章名对pdf命名失败时，仅保留文章名的汉字、字母和数字进行命名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_to_pdf</span><span class="params">(query_article)</span>:</span></div><div class="line">    path_wk = <span class="string">r'D:\Program Files\wkhtmltopdf\bin\wkhtmltopdf.exe'</span></div><div class="line">    config = pdfkit.configuration(wkhtmltopdf=path_wk)</div><div class="line">    options = &#123;</div><div class="line">        <span class="string">'page-size'</span>: <span class="string">'Letter'</span>,</div><div class="line">        <span class="string">'encoding'</span>: <span class="string">"GB18030"</span>,</div><div class="line">        <span class="string">'custom-header'</span>: [</div><div class="line">            (<span class="string">'Accept-Encoding'</span>, <span class="string">'gzip'</span>)</div><div class="line">        ]</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">try</span> :</div><div class="line">        pdfkit.from_file(<span class="string">'wechat_article.html'</span>, <span class="string">'%s.pdf'</span> % query_article, configuration=config, options=options)</div><div class="line">    <span class="keyword">except</span>:</div><div class="line">        name_compile = re.compile(<span class="string">'[a-zA-Z\u4e00-\u9fa5][a-zA-Z0-9\u4e00-\u9fa5]+'</span>)</div><div class="line">        pdf_name = re.findall(name_compile,query_article)[<span class="number">0</span>]</div><div class="line">        pdfkit.from_file(<span class="string">'wechat_article.html'</span>, <span class="string">'%s.pdf'</span> % pdf_name, configuration=config, options=options)</div><div class="line">        print(<span class="string">'文件名已被修改为:%s'</span> %pdf_name)</div></pre></td></tr></table></figure>
<h1 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h1><p>最后附上文章爬取的完整代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding:utf-8</span></div><div class="line"><span class="comment">#author:linchart</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> pdfkit</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article_link</span><span class="params">(query)</span>:</span></div><div class="line">    base_url = <span class="string">r'http://weixin.sogou.com/weixin'</span></div><div class="line">    User_Agent = <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span></div><div class="line">    Host = <span class="string">'weixin.sogou.com'</span></div><div class="line">    Connection = <span class="string">'keep-alive'</span></div><div class="line">    headers = &#123;<span class="string">'User-Agent'</span>: User_Agent, <span class="string">'Host'</span>: Host, <span class="string">'Connection'</span>: Connection&#125;</div><div class="line">    params = &#123;<span class="string">'type'</span>: <span class="number">2</span>, <span class="string">'ie'</span>: <span class="string">'utf-8'</span>, <span class="string">'w'</span>: <span class="string">'01019900'</span>, <span class="string">'sut'</span>: <span class="string">'707'</span>,<span class="string">'query'</span>:query&#125;</div><div class="line">    request = requests.get(base_url, headers=headers, params=params)</div><div class="line">    request.encoding = <span class="string">'utf-8'</span></div><div class="line">    bsobj = BeautifulSoup(request.text, <span class="string">'lxml'</span>)</div><div class="line">    <span class="comment"># 仅提取列表中的第一篇文章</span></div><div class="line">    first_article_link = bsobj.select(<span class="string">'#sogou_vr_11002601_title_0'</span>)[<span class="number">0</span>][<span class="string">'href'</span>]</div><div class="line">    <span class="keyword">return</span> first_article_link</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article_html</span><span class="params">(link)</span>:</span></div><div class="line">    <span class="comment"># 需要不同的headers</span></div><div class="line">    User_Agent = <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/3.0 Safari/536.11'</span></div><div class="line">    article_headers = &#123;<span class="string">'User_Agent'</span>: User_Agent&#125;</div><div class="line">    article_obj = requests.get(link, headers=article_headers)</div><div class="line">    article_obj.encoding = <span class="string">'utf-8'</span></div><div class="line">    soup = BeautifulSoup(article_obj.content, <span class="string">'html5lib'</span>)</div><div class="line">    <span class="comment"># 以实际的文章名称为准</span></div><div class="line">    article_name = soup.select(<span class="string">'#activity-name'</span>)[<span class="number">0</span>].text.strip()</div><div class="line">    content = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'id'</span>: <span class="string">'page-content'</span>&#125;)</div><div class="line">    html = str(content)</div><div class="line">    <span class="comment"># 把属性data-src替换成src,前面无法将属性src解析出来，data-src，只是LAZY用的，</span></div><div class="line">    <span class="comment"># 延迟加载图片所以显示不出来，LAZYLOAD</span></div><div class="line">    src_compile = re.compile(<span class="string">'data-src'</span>)</div><div class="line">    html_new = re.sub(src_compile, <span class="string">'src'</span>, html)</div><div class="line">    <span class="comment"># 存储成html</span></div><div class="line">    <span class="keyword">with</span> open(<span class="string">'wechat_article.html'</span>, <span class="string">'w'</span>, encoding=<span class="string">'GB18030'</span>) <span class="keyword">as</span> f:</div><div class="line">        f.write(html_new)</div><div class="line">    <span class="keyword">return</span> article_name</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_to_pdf</span><span class="params">(query_article)</span>:</span></div><div class="line">    path_wk = <span class="string">r'D:\Program Files\wkhtmltopdf\bin\wkhtmltopdf.exe'</span></div><div class="line">    config = pdfkit.configuration(wkhtmltopdf=path_wk)</div><div class="line">    options = &#123;</div><div class="line">        <span class="string">'page-size'</span>: <span class="string">'Letter'</span>,</div><div class="line">        <span class="string">'encoding'</span>: <span class="string">"GB18030"</span>,</div><div class="line">        <span class="string">'custom-header'</span>: [</div><div class="line">            (<span class="string">'Accept-Encoding'</span>, <span class="string">'gzip'</span>)</div><div class="line">        ]</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">try</span> :</div><div class="line">        pdfkit.from_file(<span class="string">'wechat_article.html'</span>, <span class="string">'%s.pdf'</span> % query_article, configuration=config, options=options)</div><div class="line">    <span class="keyword">except</span>:</div><div class="line">        name_compile = re.compile(<span class="string">'[a-zA-Z\u4e00-\u9fa5][a-zA-Z0-9\u4e00-\u9fa5]+'</span>)</div><div class="line">        pdf_name = re.findall(name_compile,query_article)[<span class="number">0</span>]</div><div class="line">        pdfkit.from_file(<span class="string">'wechat_article.html'</span>, <span class="string">'%s.pdf'</span> % pdf_name, configuration=config, options=options)</div><div class="line">        print(<span class="string">'文件名已被修改为:%s'</span> %pdf_name)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">wechat_article</span><span class="params">(query=None,link=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> link :</div><div class="line">        <span class="keyword">try</span> :</div><div class="line">            article_name = get_article_html(link)</div><div class="line">            html_to_pdf(article_name)</div><div class="line">            print(<span class="string">'文章下载成功'</span>)</div><div class="line">        <span class="keyword">except</span> :</div><div class="line">            article_link = get_article_link(query)</div><div class="line">            get_article_html(article_link)</div><div class="line">            html_to_pdf(query)</div><div class="line">            print(<span class="string">'文章下载成功'</span>)</div><div class="line">    <span class="keyword">else</span> :</div><div class="line">        article_link = get_article_link(query)</div><div class="line">        get_article_html(article_link)</div><div class="line">        html_to_pdf(query)</div><div class="line">        print(<span class="string">'文章下载成功'</span>)</div><div class="line"></div><div class="line"><span class="comment"># PDF可以用中文命名，但是命名中不可以包含* \/|等特殊字符。</span></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    link = <span class="keyword">None</span></div><div class="line">    query = <span class="string">'文本分析|词频与余弦相似度'</span></div><div class="line">    wechat_article(query=query,link=link)</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;有时在微信公众号上面看到一些写的比较好的文章，但又没有时间细看，闲下来想找这些文章的时候又忘了是在哪个公众号看的了、文章名字也想不起来，因此想搞个爬虫把想看的文章爬下来，一来可以在闲时咀嚼一下，二来也可以收藏一些好文章，做些知识积累。&lt;br&gt;&lt;em&gt;只是想把自己平常做的一些东西记录下来，非教程&lt;/em&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>博客的搭建</title>
    <link href="http://yoursite.com/posts/2017/03/05/blog.html"/>
    <id>http://yoursite.com/posts/2017/03/05/blog.html</id>
    <published>2017-03-04T18:08:32.626Z</published>
    <updated>2017-03-14T07:58:02.018Z</updated>
    
    <content type="html"><![CDATA[<h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p>这个博客的搭建，完全得益于<a href="https://zhangslob.github.io/2017/02/28/%E6%95%99%E4%BD%A0%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%8CHexo-Github/" target="_blank" rel="external">教你免费搭建个人博客，Hexo&amp;Github</a>和<a href="http://www.jianshu.com/p/1cd86fac2585" target="_blank" rel="external">使用GitHub和Hexo搭建免费静态Blog</a>这两篇文章，非常详细地描述了基于hexo+github搭建个人博客的准备工作及安装和配置流程。<br><a id="more"></a></p>
<h1 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h1><p>虽然上面这两篇文章写得很详细，但是我在按着教程搭建的过程中还是遇到一些小问题，这里记录一下：   </p>
<h2 id="1、注意运行路径"><a href="#1、注意运行路径" class="headerlink" title="1、注意运行路径"></a>1、注意运行路径</h2><p>在浏览器中查看自带的hello world文章，需要执行 hexo generate 和hexo server两个命令，这里要注意一下这两个命令的执行路径，需要在hello world文章路径下执行。   </p>
<h2 id="2、MarkdownPad无法预览"><a href="#2、MarkdownPad无法预览" class="headerlink" title="2、MarkdownPad无法预览"></a>2、MarkdownPad无法预览</h2><p>win10下首次安装MarkdownPad会出现右侧浏览页面无法浏览的情况，<br><img src="http://i.imgur.com/O9hUr8v.png" alt=""><br>这种情况下需要安装<a href="http://markdownpad.com/download/awesomium_v1.6.6_sdk_win.exe" target="_blank" rel="external">Awesomium 1.6.6 SDK</a>，安装完成之后问题可解决。<br><img src="http://i.imgur.com/oous83g.png" alt=""></p>
<h2 id="3、yilia主题头像无法显示"><a href="#3、yilia主题头像无法显示" class="headerlink" title="3、yilia主题头像无法显示"></a>3、yilia主题头像无法显示</h2><p>若加载头像后，头像无法显示，需要将’themes/yilia/layout/_partial’路径下的文件left-col.ejs中的第6行修改为：</p>
<pre><code class="bash">&lt;img src=<span class="string">"&lt;%=theme.avatar%&gt;"</span> class=<span class="string">"js-avatar show"</span>&gt;
</code></pre>
<h2 id="4、部署上传"><a href="#4、部署上传" class="headerlink" title="4、部署上传"></a>4、部署上传</h2><p>部署上传时执行以下命令时  </p>
<pre><code class="bash">hexo d
</code></pre>
<p>报’ERROR Deployer not found: git’错误，可能是deployer-git插件未安装，在根目录下执行下面代码安装该插件即可。</p>
<pre><code class="bash">npm install hexo-deployer-git --save
</code></pre>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;参考文档&quot;&gt;&lt;a href=&quot;#参考文档&quot; class=&quot;headerlink&quot; title=&quot;参考文档&quot;&gt;&lt;/a&gt;参考文档&lt;/h1&gt;&lt;p&gt;这个博客的搭建，完全得益于&lt;a href=&quot;https://zhangslob.github.io/2017/02/28/%E6%95%99%E4%BD%A0%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%8CHexo-Github/&quot;&gt;教你免费搭建个人博客，Hexo&amp;amp;Github&lt;/a&gt;和&lt;a href=&quot;http://www.jianshu.com/p/1cd86fac2585&quot;&gt;使用GitHub和Hexo搭建免费静态Blog&lt;/a&gt;这两篇文章，非常详细地描述了基于hexo+github搭建个人博客的准备工作及安装和配置流程。&lt;br&gt;
    
    </summary>
    
      <category term="其他" scheme="http://yoursite.com/categories/%E5%85%B6%E4%BB%96/"/>
    
    
  </entry>
  
</feed>
