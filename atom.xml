<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>linchart</title>
  <subtitle>I&#39;m on the way to the future, where you are there.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-03-14T07:48:05.298Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>山久丰</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>爬取微信文章（一）</title>
    <link href="http://yoursite.com/2017/03/13/WeChat_Article1"/>
    <id>http://yoursite.com/2017/03/13/WeChat_Article1</id>
    <published>2017-03-13T09:59:18.349Z</published>
    <updated>2017-03-14T07:48:05.298Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;有时在微信公众号上面看到一些写的比较好的文章，但又没有时间细看，闲下来想找这些文章的时候又忘了是在哪个公众号看的了、文章名字也想不起来，因此想搞个爬虫把想看的文章爬下来，一来可以在闲时咀嚼一下，二来也可以收藏一些好文章，做些知识积累。<br><em>只是想把自己平常做的一些东西记录下来，非教程</em></p>
<a id="more"></a>   
<h1 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h1><ul>
<li>Python 3.5.1   </li>
</ul>
<h1 id="使用的库"><a href="#使用的库" class="headerlink" title="使用的库"></a>使用的库</h1><ul>
<li>re</li>
<li>pdfkit</li>
<li>requests</li>
<li>BeautifulSoup   </li>
</ul>
<h1 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h1><p>输入微信文章名称或者对应的文章链接，输出文章的pdf文件。</p>
<h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><ul>
<li>如果同时提供文章链接和文章名称，则优先通过文章链接爬取，如果文章链接爬取失败，则通过文章名称爬取；   </li>
<li>如果仅提供文章链接，则通过文章链接爬取；</li>
<li>如果仅提供文章名称，则通过搜狗微信接口搜索微信文章，找到对应文章链接，然后在通过文章链接爬取。</li>
</ul>
<h1 id="爬取流程"><a href="#爬取流程" class="headerlink" title="爬取流程"></a>爬取流程</h1><h2 id="获取文章链接"><a href="#获取文章链接" class="headerlink" title="获取文章链接"></a>获取文章链接</h2><p>将提供的文章名称传入<a href="http://weixin.sogou.com/weixin" target="_blank" rel="external">搜狗微信搜索引擎搜索</a>，将结果列表中的第一篇文章作为目标文章下载。下面代码返回目标文章链接。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article_link</span><span class="params">(query)</span>:</span></div><div class="line">    base_url = <span class="string">r'http://weixin.sogou.com/weixin'</span></div><div class="line">    User_Agent = <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span></div><div class="line">    Host = <span class="string">'weixin.sogou.com'</span></div><div class="line">    Connection = <span class="string">'keep-alive'</span></div><div class="line">    headers = &#123;<span class="string">'User-Agent'</span>: User_Agent, <span class="string">'Host'</span>: Host, <span class="string">'Connection'</span>: Connection&#125;</div><div class="line">    params = &#123;<span class="string">'type'</span>: <span class="number">2</span>, <span class="string">'ie'</span>: <span class="string">'utf-8'</span>, <span class="string">'w'</span>: <span class="string">'01019900'</span>, <span class="string">'sut'</span>: <span class="string">'707'</span>,<span class="string">'query'</span>:query&#125;</div><div class="line">    request = requests.get(base_url, headers=headers, params=params)</div><div class="line">    request.encoding = <span class="string">'utf-8'</span></div><div class="line">    bsobj = BeautifulSoup(request.text, <span class="string">'lxml'</span>)</div><div class="line">    <span class="comment"># 仅提取列表中的第一篇文章</span></div><div class="line">    first_article_link = bsobj.select(<span class="string">'#sogou_vr_11002601_title_0'</span>)[<span class="number">0</span>][<span class="string">'href'</span>]</div><div class="line">    <span class="keyword">return</span> first_article_link</div></pre></td></tr></table></figure>
<h2 id="将文章转为html"><a href="#将文章转为html" class="headerlink" title="将文章转为html"></a>将文章转为html</h2><p>解析文章链接，将文章内容保存为html文件。这里需要注意的是，在解析文章的时候，如果文章中包含有图片的话，正常情况下是无法下载下来的，因为爬取的文章链接为临时链接，非永久链接，无法直接解析src里面的链接。但是，data-src这个属性的值还是可以解析出来的，所以只要把data-src替换为src就可以下载图片了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article_html</span><span class="params">(link)</span>:</span></div><div class="line">    <span class="comment"># 为了保险起见，这里使用不同的headers</span></div><div class="line">    User_Agent = <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/3.0 Safari/536.11'</span></div><div class="line">    article_headers = &#123;<span class="string">'User_Agent'</span>: User_Agent&#125;</div><div class="line">    article_obj = requests.get(link, headers=article_headers)</div><div class="line">    article_obj.encoding = <span class="string">'utf-8'</span></div><div class="line">    soup = BeautifulSoup(article_obj.content, <span class="string">'html5lib'</span>)</div><div class="line">    <span class="comment"># 以实际的文章名称为准</span></div><div class="line">    article_name = soup.select(<span class="string">'#activity-name'</span>)[<span class="number">0</span>].text.strip()</div><div class="line">    content = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'id'</span>: <span class="string">'page-content'</span>&#125;)</div><div class="line">    html = str(content)</div><div class="line">    <span class="comment"># 把属性data-src替换成src,前面无法将属性src解析出来，data-src，只是LAZY用的，</span></div><div class="line">    <span class="comment"># 延迟加载图片所以显示不出来，LAZYLOAD</span></div><div class="line">    src_compile = re.compile(<span class="string">'data-src'</span>)</div><div class="line">    html_new = re.sub(src_compile, <span class="string">'src'</span>, html)</div><div class="line">    <span class="comment"># 存储成html</span></div><div class="line">    <span class="keyword">with</span> open(<span class="string">'wechat_article.html'</span>, <span class="string">'w'</span>, encoding=<span class="string">'GB18030'</span>) <span class="keyword">as</span> f:</div><div class="line">        f.write(html_new)</div><div class="line">    <span class="keyword">return</span> article_name</div></pre></td></tr></table></figure>
<h2 id="html转pdf"><a href="#html转pdf" class="headerlink" title="html转pdf"></a>html转pdf</h2><p>html文件转pdf调用了pdfkit这个包，使用这个包需要安装<a href="http://wkhtmltopdf.org/downloads.html" target="_blank" rel="external">wkhtmltopdf</a>软件（pdfkit依赖于wkhtmltopdf，因此需要配置路径）。<br>在运行过程中，发现pdfkit在html转pdf时，生成的pdf文件名中如果包含有| / *这些特殊符号时会报错，因此如果以原文章名对pdf命名失败时，仅保留文章名的汉字、字母和数字进行命名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_to_pdf</span><span class="params">(query_article)</span>:</span></div><div class="line">    path_wk = <span class="string">r'D:\Program Files\wkhtmltopdf\bin\wkhtmltopdf.exe'</span></div><div class="line">    config = pdfkit.configuration(wkhtmltopdf=path_wk)</div><div class="line">    options = &#123;</div><div class="line">        <span class="string">'page-size'</span>: <span class="string">'Letter'</span>,</div><div class="line">        <span class="string">'encoding'</span>: <span class="string">"GB18030"</span>,</div><div class="line">        <span class="string">'custom-header'</span>: [</div><div class="line">            (<span class="string">'Accept-Encoding'</span>, <span class="string">'gzip'</span>)</div><div class="line">        ]</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">try</span> :</div><div class="line">        pdfkit.from_file(<span class="string">'wechat_article.html'</span>, <span class="string">'%s.pdf'</span> % query_article, configuration=config, options=options)</div><div class="line">    <span class="keyword">except</span>:</div><div class="line">        name_compile = re.compile(<span class="string">'[a-zA-Z\u4e00-\u9fa5][a-zA-Z0-9\u4e00-\u9fa5]+'</span>)</div><div class="line">        pdf_name = re.findall(name_compile,query_article)[<span class="number">0</span>]</div><div class="line">        pdfkit.from_file(<span class="string">'wechat_article.html'</span>, <span class="string">'%s.pdf'</span> % pdf_name, configuration=config, options=options)</div><div class="line">        print(<span class="string">'文件名已被修改为:%s'</span> %pdf_name)</div></pre></td></tr></table></figure>
<h1 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h1><p>最后附上文章爬取的完整代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding:utf-8</span></div><div class="line"><span class="comment">#author:linchart</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> pdfkit</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article_link</span><span class="params">(query)</span>:</span></div><div class="line">    base_url = <span class="string">r'http://weixin.sogou.com/weixin'</span></div><div class="line">    User_Agent = <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span></div><div class="line">    Host = <span class="string">'weixin.sogou.com'</span></div><div class="line">    Connection = <span class="string">'keep-alive'</span></div><div class="line">    headers = &#123;<span class="string">'User-Agent'</span>: User_Agent, <span class="string">'Host'</span>: Host, <span class="string">'Connection'</span>: Connection&#125;</div><div class="line">    params = &#123;<span class="string">'type'</span>: <span class="number">2</span>, <span class="string">'ie'</span>: <span class="string">'utf-8'</span>, <span class="string">'w'</span>: <span class="string">'01019900'</span>, <span class="string">'sut'</span>: <span class="string">'707'</span>,<span class="string">'query'</span>:query&#125;</div><div class="line">    request = requests.get(base_url, headers=headers, params=params)</div><div class="line">    request.encoding = <span class="string">'utf-8'</span></div><div class="line">    bsobj = BeautifulSoup(request.text, <span class="string">'lxml'</span>)</div><div class="line">    <span class="comment"># 仅提取列表中的第一篇文章</span></div><div class="line">    first_article_link = bsobj.select(<span class="string">'#sogou_vr_11002601_title_0'</span>)[<span class="number">0</span>][<span class="string">'href'</span>]</div><div class="line">    <span class="keyword">return</span> first_article_link</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article_html</span><span class="params">(link)</span>:</span></div><div class="line">    <span class="comment"># 需要不同的headers</span></div><div class="line">    User_Agent = <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/3.0 Safari/536.11'</span></div><div class="line">    article_headers = &#123;<span class="string">'User_Agent'</span>: User_Agent&#125;</div><div class="line">    article_obj = requests.get(link, headers=article_headers)</div><div class="line">    article_obj.encoding = <span class="string">'utf-8'</span></div><div class="line">    soup = BeautifulSoup(article_obj.content, <span class="string">'html5lib'</span>)</div><div class="line">    <span class="comment"># 以实际的文章名称为准</span></div><div class="line">    article_name = soup.select(<span class="string">'#activity-name'</span>)[<span class="number">0</span>].text.strip()</div><div class="line">    content = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'id'</span>: <span class="string">'page-content'</span>&#125;)</div><div class="line">    html = str(content)</div><div class="line">    <span class="comment"># 把属性data-src替换成src,前面无法将属性src解析出来，data-src，只是LAZY用的，</span></div><div class="line">    <span class="comment"># 延迟加载图片所以显示不出来，LAZYLOAD</span></div><div class="line">    src_compile = re.compile(<span class="string">'data-src'</span>)</div><div class="line">    html_new = re.sub(src_compile, <span class="string">'src'</span>, html)</div><div class="line">    <span class="comment"># 存储成html</span></div><div class="line">    <span class="keyword">with</span> open(<span class="string">'wechat_article.html'</span>, <span class="string">'w'</span>, encoding=<span class="string">'GB18030'</span>) <span class="keyword">as</span> f:</div><div class="line">        f.write(html_new)</div><div class="line">    <span class="keyword">return</span> article_name</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_to_pdf</span><span class="params">(query_article)</span>:</span></div><div class="line">    path_wk = <span class="string">r'D:\Program Files\wkhtmltopdf\bin\wkhtmltopdf.exe'</span></div><div class="line">    config = pdfkit.configuration(wkhtmltopdf=path_wk)</div><div class="line">    options = &#123;</div><div class="line">        <span class="string">'page-size'</span>: <span class="string">'Letter'</span>,</div><div class="line">        <span class="string">'encoding'</span>: <span class="string">"GB18030"</span>,</div><div class="line">        <span class="string">'custom-header'</span>: [</div><div class="line">            (<span class="string">'Accept-Encoding'</span>, <span class="string">'gzip'</span>)</div><div class="line">        ]</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">try</span> :</div><div class="line">        pdfkit.from_file(<span class="string">'wechat_article.html'</span>, <span class="string">'%s.pdf'</span> % query_article, configuration=config, options=options)</div><div class="line">    <span class="keyword">except</span>:</div><div class="line">        name_compile = re.compile(<span class="string">'[a-zA-Z\u4e00-\u9fa5][a-zA-Z0-9\u4e00-\u9fa5]+'</span>)</div><div class="line">        pdf_name = re.findall(name_compile,query_article)[<span class="number">0</span>]</div><div class="line">        pdfkit.from_file(<span class="string">'wechat_article.html'</span>, <span class="string">'%s.pdf'</span> % pdf_name, configuration=config, options=options)</div><div class="line">        print(<span class="string">'文件名已被修改为:%s'</span> %pdf_name)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">wechat_article</span><span class="params">(query=None,link=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> link :</div><div class="line">        <span class="keyword">try</span> :</div><div class="line">            article_name = get_article_html(link)</div><div class="line">            html_to_pdf(article_name)</div><div class="line">            print(<span class="string">'文章下载成功'</span>)</div><div class="line">        <span class="keyword">except</span> :</div><div class="line">            article_link = get_article_link(query)</div><div class="line">            get_article_html(article_link)</div><div class="line">            html_to_pdf(query)</div><div class="line">            print(<span class="string">'文章下载成功'</span>)</div><div class="line">    <span class="keyword">else</span> :</div><div class="line">        article_link = get_article_link(query)</div><div class="line">        get_article_html(article_link)</div><div class="line">        html_to_pdf(query)</div><div class="line">        print(<span class="string">'文章下载成功'</span>)</div><div class="line"></div><div class="line"><span class="comment"># PDF可以用中文命名，但是命名中不可以包含* \/|等特殊字符。</span></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    link = <span class="keyword">None</span></div><div class="line">    query = <span class="string">'文本分析|词频与余弦相似度'</span></div><div class="line">    wechat_article(query=query,link=link)</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;有时在微信公众号上面看到一些写的比较好的文章，但又没有时间细看，闲下来想找这些文章的时候又忘了是在哪个公众号看的了、文章名字也想不起来，因此想搞个爬虫把想看的文章爬下来，一来可以在闲时咀嚼一下，二来也可以收藏一些好文章，做些知识积累。&lt;br&gt;&lt;em&gt;只是想把自己平常做的一些东西记录下来，非教程&lt;/em&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
  </entry>
  
  <entry>
    <title>博客的搭建</title>
    <link href="http://yoursite.com/2017/03/05/blog"/>
    <id>http://yoursite.com/2017/03/05/blog</id>
    <published>2017-03-04T18:08:32.626Z</published>
    <updated>2017-03-05T15:26:40.715Z</updated>
    
    <content type="html"><![CDATA[<h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p>这个博客的搭建，完全得益于<a href="https://zhangslob.github.io/2017/02/28/%E6%95%99%E4%BD%A0%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%8CHexo-Github/" target="_blank" rel="external">教你免费搭建个人博客，Hexo&amp;Github</a>和<a href="http://www.jianshu.com/p/1cd86fac2585" target="_blank" rel="external">使用GitHub和Hexo搭建免费静态Blog</a>这两篇文章，非常详细地描述了基于hexo+github搭建个人博客的准备工作及安装和配置流程。</p>
<h1 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h1><p>虽然上面这两篇文章写得很详细，但是我在按着教程搭建的过程中还是遇到一些小问题，这里记录一下：   </p>
<h2 id="1、注意运行路径"><a href="#1、注意运行路径" class="headerlink" title="1、注意运行路径"></a>1、注意运行路径</h2><p>在浏览器中查看自带的hello world文章，需要执行 hexo generate 和hexo server两个命令，这里要注意一下这两个命令的执行路径，需要在hello world文章路径下执行。   </p>
<h2 id="2、MarkdownPad无法预览"><a href="#2、MarkdownPad无法预览" class="headerlink" title="2、MarkdownPad无法预览"></a>2、MarkdownPad无法预览</h2><p>win10下首次安装MarkdownPad会出现右侧浏览页面无法浏览的情况，<br><img src="http://i.imgur.com/O9hUr8v.png" alt=""><br>这种情况下需要安装<a href="http://markdownpad.com/download/awesomium_v1.6.6_sdk_win.exe" target="_blank" rel="external">Awesomium 1.6.6 SDK</a>，安装完成之后问题可解决。<br><img src="http://i.imgur.com/oous83g.png" alt=""></p>
<h2 id="3、yilia主题头像无法显示"><a href="#3、yilia主题头像无法显示" class="headerlink" title="3、yilia主题头像无法显示"></a>3、yilia主题头像无法显示</h2><p>若加载头像后，头像无法显示，需要将’themes/yilia/layout/_partial’路径下的文件left-col.ejs中的第6行修改为：</p>
<pre><code class="bash">&lt;img src=<span class="string">"&lt;%=theme.avatar%&gt;"</span> class=<span class="string">"js-avatar show"</span>&gt;
</code></pre>
<h2 id="4、部署上传"><a href="#4、部署上传" class="headerlink" title="4、部署上传"></a>4、部署上传</h2><p>部署上传时执行以下命令时  </p>
<pre><code class="bash">hexo d
</code></pre>
<p>报’ERROR Deployer not found: git’错误，可能是deployer-git插件未安装，在根目录下执行下面代码安装该插件即可。</p>
<pre><code class="bash">npm install hexo-deployer-git --save
</code></pre>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;参考文档&quot;&gt;&lt;a href=&quot;#参考文档&quot; class=&quot;headerlink&quot; title=&quot;参考文档&quot;&gt;&lt;/a&gt;参考文档&lt;/h1&gt;&lt;p&gt;这个博客的搭建，完全得益于&lt;a href=&quot;https://zhangslob.github.io/2017/02/28/%
    
    </summary>
    
    
  </entry>
  
</feed>
